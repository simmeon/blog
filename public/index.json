[{"content":"\rThe Initial Value Problem In engineering, we often encounter systems that evolve over time, such as circuits, mechanical systems, or chemical reactions. These systems are best described using differential equations. For example, Newton\u0026rsquo;s law of cooling states:\n$$ \\dfrac{dT}{dt} = -k(T - T_{surr}) $$\nwhere T is the temperature of some point, k is a proportionality constant, and Tsurr is the temperature surrounding the point of interest.\nTo solve this equation is to find a solution for the temperature, T, over time. In this case, this is fairly straightforward and we can come up with an analytical solution of the form:\n$$ T(t) = T_{surr} + (T(0) - T_{surr})e^{-kt} $$\nNotice that the solution depends on the initial temperature, as Figure 1 shows. We need a point of reference to define the solution that is relevant to our system.\nFigure 1: Temperature of a point over time with different initial temperatures. Tsurr = 20 C, k = 1.\nFigure 1 code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 \u0026#39;\u0026#39;\u0026#39; Plotting the temperature of a point with different starting temperatures. Created by: simmeon Last Modified: 28/04/24 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np # Define system parameters T_surr = 20 # surrounding temperature of 20 C k = 1 # proportionality constant of 1 for simplicity # Define our cooling function def solve_T(t, T0): return T_surr + (T0 - T_surr)*np.exp(-k*t) # Create an array of times from 0-5 seconds t = np.arange(0, 5, 0.1) # Solve and plot the solutions for different initial values of T T0_choices = [30, 25, 20, 15, 10] plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) for T0 in T0_choices: T = solve_T(t, T0) plt.plot(t, T, label=\u0026#39;T0 = {} C\u0026#39;.format(T0)) plt.title(\u0026#39;Temperature over time\u0026#39;, fontsize=20) plt.xlabel(\u0026#39;Time [s]\u0026#39;, fontsize=20) plt.ylabel(\u0026#39;Temperature [C]\u0026#39;, fontsize=20) plt.xlim(0, 5) plt.ylim(10, 30) plt.grid(alpha=0.7) plt.legend() plt.show() The initial value problem deals with solving these ordinary differential equations where we have an initial state of the system, eg. the initial temperature is 25 C. However, often these systems are hard or impossible to solve analytically. To solve them, we use numerical methods to approximate the solution. Let\u0026rsquo;s look at how we might be able to do that\u0026hellip;\nNumerical Methods Let\u0026rsquo;s take the previous cooling equation,\n$$ \\dfrac{dT}{dt} = -k(T - T_{surr}) $$\nand assume we can\u0026rsquo;t find an analytical solution. Let\u0026rsquo;s consider the information we do have that could help us approximate the solution. We have an expression for the derivative that we can calculate at any time, t, assuming we know the current temperature at that time. We know the temperature at some initial time (t = 0 in this case), so we can calculate the derivative at that time. But how does this help us find the temperature at other times?\nThe Euler Method From calculus we know that we get the derivative of a function by taking two points on the function and approximating the derivative as if it were a striaght line. As the distance, h, between the two points gets closer to 0, the approximation gets better. Formally,\n$$ f^{\\prime}(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h} $$\nWe can rearrange this and assume a finite step size, h, to get\n$$ f(x+h) \\approx f(x) + h f^{\\prime}(x) $$\nwhich gives us an expression to approximate the next step of a function using only the current known function value and the function derivative. We can see what this looks like with different step sizes in Figure 2.\nFigure 2: Estimating the value of y = x2 with different step sizes.\nFigure 2 code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 \u0026#39;\u0026#39;\u0026#39; Visualising how the derivative can be used to estimate the value of a function. Created by: simmeon Last Modified: 28/04/24 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) # Define some function we want to find the value of def func(t): return t**2 # Define the derivative line of the function def dydt(t, t0, y0): m = 2 * t0 return m * (t-t0) + y0 # Create an array of time values to plot the function t_func = np.arange(0.5, 2, 0.1) # Estimation parameters t0 = 1.0 # known initial value y0 = func(t0) # known initial value h = [1, 0.5, 0.2] # step size options # Plotting # Find and plot function values y_func = func(t_func) plt.plot(t_func, y_func) plt.plot(t0, y0, \u0026#39;x\u0026#39;, color=\u0026#39;C0\u0026#39;) # Plot the derivative line for each step size colours = [\u0026#39;C1\u0026#39;, \u0026#39;C2\u0026#39;, \u0026#39;C3\u0026#39;] for i in range(len(h)): t_dydt = np.arange(t0, t0 + h[i], 0.1) y_dydt = dydt(t_dydt, t0, y0) plt.plot(t_dydt, y_dydt, color=colours[i], label=\u0026#39;Step = {:.1f}\u0026#39;.format(h[i])) plt.plot(t_dydt[-1], y_dydt[-1], \u0026#39;o\u0026#39;, color=colours[i]) # Show plot plt.title(\u0026#34;Estimating a function by using the derivative\u0026#34;, fontsize=20) plt.legend(loc=\u0026#39;lower right\u0026#39;, fontsize=10) plt.show() This is Euler\u0026rsquo;s method for solving the initial value problem. We might also write it as:\n$$ f_{k+1} \\approx f_{k} + h f^{\\prime}(x_{k}) $$\nfor some index, k.\nWe can iterate through time with this method, using the newly found fk+1 as our new fk and so on. In code, that would look like the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def euler(dydt, y0, t0, t_end, h=0.1): \u0026#39;\u0026#39;\u0026#39; Takes the derivative function, an initial condition, the time we want to integrate until, and a step size. Returns arrays of time and y values. \u0026#39;\u0026#39;\u0026#39; t = np.arange(t0, t_end+h, h) y = np.zeros(len(t)) y[0] = y0 for i in range(1,len(t)): y[i] = y[i-1] + h * dydt(y[i-1]) return t, y The highlighted line shows the Euler method itself where the next value of the function is estimated.\nLet\u0026rsquo;s try do that with Newton\u0026rsquo;s cooling law and see how the result compares to the analytical solution. Figure 3 shows what that would look like.\nFigure 3: Analytical and Euler method solutions to Newton\u0026rsquo;s cooling law. Tsurr = 20 C, k = 1.\nFigure 3 code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 \u0026#39;\u0026#39;\u0026#39; Performing Euler integration to solve the cooling equation. We will compare with the analytical solution. Created by: simmeon Last Modified: 28/04/24 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) # Define system T_surr = 20 # surrounding temperature of 20 C k = 1 # proportionality constant of 1 for simplicity # Define our cooling function def solve_T(t, T0): return T_surr + (T0 - T_surr)*np.exp(-k*t) # Define the derivative (only depends on the current temp, not time) def dTdt(T): return -k * (T - T_surr) # Define our Euler integration function def euler(dTdt, T0, t0, t_end, h=0.1): \u0026#39;\u0026#39;\u0026#39; Takes the derivative function, an initial condition, the time we want to integrate until, and a step size. Returns arrays of time and temperature values \u0026#39;\u0026#39;\u0026#39; t = np.arange(t0, t_end+h, h) T = np.zeros(len(t)) T[0] = T0 for i in range(1,len(t)): T[i] = T[i-1] + h * dTdt(T[i-1]) return t, T # Parameters t0 = 0 T0 = 30 t_end = 5 # Get analytical solution t_analytical = np.arange(t0, t_end+0.1, 0.1) T_analytical = solve_T(t_analytical, T0) # Get Euler numerical solution t_euler_05, T_euler_05 = euler(dTdt, T0, t0, t_end, 0.5) t_euler_01, T_euler_01 = euler(dTdt, T0, t0, t_end, 0.1) # Plotting plt.plot(t_analytical, T_analytical, label=\u0026#39;Analytical\u0026#39;, linestyle=\u0026#39;--\u0026#39;) plt.plot(t_euler_05, T_euler_05, label=\u0026#39;Euler method, h = 0.5\u0026#39;) plt.plot(t_euler_01, T_euler_01, label=\u0026#39;Euler method, h = 0.1\u0026#39;) plt.title(\u0026#34;Analytical and Euler method solutions to Newton\u0026#39;s cooling law\u0026#34;, fontsize=20) plt.xlabel(\u0026#39;Time [s]\u0026#39;, fontsize=20) plt.ylabel(\u0026#39;Temperature [C]\u0026#39;, fontsize=20) plt.legend(fontsize=15) plt.show() It is clear that the step size plays a big role in the accuracy of the solution. We could continue to reduce the step size, but that will quickly increase the time it takes to get a solution beyond a reasonable amount. Maybe we can think of a better way of doing this\u0026hellip;\nRunge-Kutta Methods A major problem with Euler\u0026rsquo;s method is that using the derivative of the point we\u0026rsquo;re at is not very good at estimating the next value of the function (for the kind of step sizes we want to use). The derivative is constantly changing and could be vastly different at the new value. We might get a more accurate step if we use a mix of the derivative at our start point and end point.\nThis is the basis for how Runge-Kutta methods work. We find derivatives between the start and end points of each step and use a weighted average of those as the actual derivative for the step. The simplest version of this would be to just use the derivative at the start of the step \u0026ndash; which is exactly the Euler method! The Euler method is a 1st order Runge-Kutta method.\nLet\u0026rsquo;s define the method more concretely.\nDerivation of Runge-Kutta Methods We will start by examining the initial problem again, being that we want to solve the following general equation for y:\n$$ \\dfrac{dy}{dt} = f(y, t) $$\nThis could be our cooling equation from before,\n$$ \\dfrac{dT}{dt} = -k(T - T_{surr}) $$\nor something more complex like a state space defining a spring, mass, damper system:\n$$ \\bold{\\dot{x}} = \\begin{bmatrix} 0 \u0026amp; 1 \\\\ \\frac{-k}{m} \u0026amp; \\frac{-c}{m} \\end{bmatrix} \\bold{x} + \\begin{bmatrix} 0 \\\\ \\frac{1}{m} \\end{bmatrix} u(t) $$\nIn both cases, we are given the derivative of the state we want to solve for (eg. temperature) and the derivative depends on the value of the state and time. In the case of the cooling equation, the derivative only depends on the state, T, and not time.\nIdeally, we could just integrate the derivative to find the value at \\( t + h \\):\n$$ y(t+h) = y(t) + \\int_{\\tau=t}^{\\tau=t+h}{\\dfrac{dy(\\tau)}{d\\tau}} d\\tau $$\nHowever, as we said earlier, this is often hard or impossible. We can instead approximate the integral with a weighted sum.\nWeighted Sum Approximations Let\u0026rsquo;s build this up slowly. Take the following function, for example:\n$$ f(t) = t^3 - 2t^2 - t + 3 $$\nHow would we evaluate\n$$ \\int_0^2{f(t)}dt $$\nWe can do this analytically, which is like summing up tiny vertical slices of the function to find the total area underneath it. But we could think about this a different way. We can get that same area by finding the average value of the function over the interval, then multiplying by the length of the interval. You can see this in Figure 4.\nFigure 4: Comparison of areas found by integration and weighted sum approximation.\nFigure 4 code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 \u0026#39;\u0026#39;\u0026#39; Making sense of weighted sums as integral approximations. Created by: simmeon Last Modified: 28/04/24 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) def f(t): return t**3 -2*t**2 - t + 3 # Perform a weighted sum approximation N = 10 # higher number of samples, N, gives a better approximation sum = 0 t0 = 0 h = 2 nodes = np.linspace(t0, t0+h, N) for node in nodes: sum += f(node) weighted_sum = sum / N print(\u0026#39;Weighted sum area: \u0026#39;, weighted_sum * h) # Plotting t = np.arange(t0, t0 + h, 0.01) y = f(t) # Integration result plt.subplot(1,2,1) plt.plot(t,y) plt.fill_between(t, y, alpha=0.3) plt.annotate(\u0026#39;Area = 2.67\u0026#39;, (1,2.5), fontsize=20) plt.title(\u0026#39;Area from integration\u0026#39;, fontsize=20) # Weighted sum result plt.subplot(1,2,2) plt.plot(t,y) plt.fill_between(t, weighted_sum, alpha=0.3) plt.annotate(\u0026#39;Area = {:.2f}\u0026#39;.format(weighted_sum * h), (1,2.5), fontsize=20) plt.title(\u0026#39;Area from weighted sum\u0026#39;, fontsize=20) plt.show() To put this idea into an equation, we can say:\n$$ \\int_0^2{f(t)}dt \\approx \\frac{2}{\\sum{w_i}} \\sum_{i=1}^N{w_i f(t_i)} $$\nWhere \\( N \\) is the number of points we sample from the function to find the average, \\( w_i \\) is the weighting we give to each point, and \\( t_i \\) is some point inside the integration bounds of \\( [0, 2] \\). We can simplify this by giving every point a weighting of 1:\n$$ \\int_0^2{f(t)}dt \\approx \\frac{2}{N} \\sum_{i=1}^N{f(t_i)} $$\nThe more points we sample, the closer the average will be to the actual average of the function and therefore the actual integral.\nWe can extend this concept to our derivative from earlier:\n$$ \\int_{\\tau=t}^{\\tau=t+h}{\\dfrac{dy(\\tau)}{d\\tau}} d\\tau \\approx \\frac{h}{\\sum{w_i}} \\sum_{i=1}^N{w_i y^{\\prime}(t+v_i h, y(t+v_i h))} $$\nWhat a mess.\nWe\u0026rsquo;re still using the same concept of a weighted sum to find the average value of the derivative function, but since the derivative depends on \\( t \\) and \\( y \\) it\u0026rsquo;s a bit more complicated. We introduced \\( v_i \\) to help define which values of the function we sample, also called nodes. This can range from 0 to 1 to cover the interval from \\( t \\) to \\( t+h \\). Of course, whatever time we evaluate our derivative at must be the same time we use to get the \\( y \\) values for the derivative.\nFor the sake of simplicity let\u0026rsquo;s say that all our weights will sum up to 1. We are not, however, going to assume that all our weights will be the same value as we did previously.\nConsidering all this, the equation we are trying to solve is now:\n$$ y(t+h) = y(t) + h \\sum_{i=1}^N{w_i y^{\\prime}(t+v_i h, y(t+v_i h))} $$\nThe Runge-Kutta Family You might have noticed there is a problem with the sum we just defined. In particular, we don\u0026rsquo;t know what \\( y(t+v_i h) \\) is. In fact, finding this is basically the point of the whole method. So how do we deal with this?\nSimply, we are going to make worse approximations of \\( y(t+v_i h) \\) so that we can make a much better approximation of \\( y(t + h) \\). Again, these \\( y(t+v_i h) \\) values are used to find values of the derivative that we will then average. Let\u0026rsquo;s define what we will do to find these approximations.\nWe will start by saying that \\( v_1 = 0 \\). This means the first term in the sum will be\n$$ w_1 y^{\\prime}(t, y(t)) $$\nIf this was the only term in our sum (so \\( w_1 = 1 \\) ), then the equation we would be solving would be:\n$$ y(t+h) \\approx y(t) + h y^{\\prime}(t, y(t)) $$\nThis should look familiar, it\u0026rsquo;s the Euler method! This is what we mean by the Euler method is a 1st order Runge-Kutta method \u0026ndash; because it uses one term in the weighted sum. Or in other words, it is a linear approximation.\nFor simplicity, we will write this first sum term as:\n$$ k_1 = y^{\\prime}(t, y(t)) h $$\n\\( k_1 \\) is a 1st order estimate of the change in \\( y \\) between \\( y(t) \\) and \\( y(t+h) \\).\nThe second term gets trickier as we have to somehow estimate \\( y(t+v_2 h) \\). Conveniently, we have just found an estimate for how \\( y \\) changes: \\( k_1 \\). So we can utilise some fraction of this change to create our estimate for the second weighted sum term where:\n$$ y^{\\prime}(t + \\alpha_2 h, y(t) + \\beta_{2,1} k_1) $$\nWe don\u0026rsquo;t know yet how much of \\( k_1 \\) we should add, we will figure that out later. Same with what time we should sample at.\nWe have changed notation slightly to help set up higher order Runge-Kutta methods. Instead of \\( v_i \\) we are now using \\( \\alpha_i \\) to tell us about what time we are sampling at. And since our \\( y \\) value is no longer defined in simple terms of time, we are going to use \\( \\beta_{i,j} \\) to describe how much of previous estimates we will add to the estimate for the new term.\nWe define\n$$ k_2 = y^{\\prime}(t + \\alpha_2 h, y(t) + \\beta_{2,1} k_1) h $$\nso that the equation we are solving is now:\n$$ y(t+h) \\approx y(t) + w_1 k_1 + w_2 k_2 $$\nWith that, we have defined the 2nd order Runge-Kutta Method!\nWe can continue the weighted sum with a third and fourth term, following similar logic of using the previous estimates to inform the new value of \\( y(t + v_i h) \\). This will give us:\n$$ k_3 = y^{\\prime}(t + \\alpha_3 h, y(t) + \\beta_{3,1} k_1 + \\beta_{3,2} k_2) h \\\\ k_4 = y^{\\prime}(t + \\alpha_4 h, y(t) + \\beta_{4,1} k_1 + \\beta_{4,2} k_2 + \\beta_{4,3} k_3) h $$\nThis gives the 4th order Runge-Kutta method:\n$$ y(t+h) \\approx y(t) + w_1 k_1 + w_2 k_2 + w_3 k_3 + w_4 k_4 $$\nNow to actually solve these higher order methods, we need to define these coefficients\u0026hellip;\nDefining Coefficients We need to define all the \\( \\alpha_i \\), \\( \\beta_{i,j} \\), and \\( w_i \\) coefficients to be able to use these methods. We can do this by comparing the Taylor series expansion of our approximation to the Taylor series expansion of \\( y(t+h) \\) and equating coefficients. Let\u0026rsquo;s do this for the 2nd order method:\n$$ y(t+h) \\approx y(t) + w_1 k_1 + w_2 k_2 $$\nAs this is a 2nd order method, we will need to find the 2nd order expansions of the left and right sides.\nLet\u0026rsquo;s start with the left. The 2nd order Taylor series expansion of \\( y(t+h) \\) about \\( t \\) is:\n$$ y(t+h) \\approx y(t) + h \\dfrac{dy}{dt} \\Big| _{t,y} + \\frac{h^2}{2} \\dfrac{d^2y}{dt^2} \\Big| _{t,y} + O(h^3) $$\nWe can write our derivative:\n$$ \\dfrac{dy}{dt} = f(t, y) \\\\ {} \\\\ \\dfrac{d^2y}{dt^2} = \\dfrac{df(t,y)}{dt} = \\dfrac{\\partial f}{\\partial t} + \\dfrac{\\partial f}{\\partial y} \\dfrac{dy}{dt} = \\dfrac{\\partial f}{\\partial t} + f \\dfrac{\\partial f}{\\partial y} $$\nThis makes our left hand side (using slightly different notation):\n$$ y_{n+1} \\approx y_n + h f(t_n, y_n) + \\frac{h^2}{2} \\bigg(\\dfrac{\\partial f}{\\partial t} + f \\dfrac{\\partial f}{\\partial y}\\bigg) \\bigg|_{t_n, y_n} + O(h^3) $$\nGreat! Let\u0026rsquo;s do the right hand side now. We can write the right hand same with the same notation as above:\n$$ y_n + w_1 k_{1,n} + w_2 k_{2,n} $$\n\\( k_2 \\) is a bit tricky to expand. Our \\( k_2 \\) term is made up mostly of a function with the form \\( f(t + \\Delta t, y + \\Delta y) \\), which will need to be expanded. In general, the 2nd order expansion looks like:\n$$ f(t + \\Delta t, y + \\Delta y) = f(t, y) + \\Delta t \\dfrac{\\partial f}{\\partial t}\\bigg| _{t, y} + \\Delta y \\dfrac{\\partial f}{\\partial y} \\bigg| _{t, y} + O(h^3) $$\nApplying this to \\( k_2 \\) gives:\n$$ k_{2,n} = h f(t + \\alpha_2 h, y_n + \\beta_{2,1} k_1) \\approx h \\bigg( f(t_n, y_n) + \\alpha _2 h \\dfrac{\\partial f}{\\partial t} \\bigg| _{t_n, y_n} + \\beta _{2,1} k_1 \\dfrac{\\partial f}{\\partial y} \\bigg| _{t_n, y_n} \\bigg) $$\nAll together, the right hand side is then\n$$ y_n + w_1 k_{1,n} + w_2 k_{2,n} \\approx y_n + w_1 h f(t_n, y_n) + w_2 h \\bigg( f(t_n, y_n) + \\alpha _2 h \\dfrac{\\partial f}{\\partial t} \\bigg| _{t_n, y_n} + \\beta _{2,1} k_1 \\dfrac{\\partial f}{\\partial y} \\bigg| _{t_n, y_n} \\bigg) + O(h^3) $$\nwhich we can rearrange to be in the same form (substituting in for \\( k_1 \\)) as the left hand side:\n$$ y_n + w_1 k_{1,n} + w_2 k_{2,n} \\approx y_n + (w_1 + w_2) h f(t_n, y_n) + \\frac{h^2}{2} \\bigg(2 w_2 \\alpha_2 \\dfrac{\\partial f}{\\partial t} + 2 w_2 \\beta _{2,1} f \\dfrac{\\partial f}{\\partial y} \\bigg) \\bigg| _{t_n, y_n} + O(h^3) $$\nFinally, we have both expansions and can equate the coefficients:\n$$ y(t+h) \\approx y(t) + w_1 k_1 + w_2 k_2 \\\\ {} \\\\ \\big\\downarrow \\\\ {} \\\\ y_n + h f(t_n, y_n) + \\frac{h^2}{2} \\bigg(\\dfrac{\\partial f}{\\partial t} + f \\dfrac{\\partial f}{\\partial y}\\bigg) \\bigg|_{t_n, y_n} + O(h^3) \\\\ {} \\\\ = \\\\ {} \\\\ y_n + (w_1 + w_2) h f(t_n, y_n) + \\frac{h^2}{2} \\bigg(2 w_2 \\alpha_2 \\dfrac{\\partial f}{\\partial t} + 2 w_2 \\beta _{2,1} f \\dfrac{\\partial f}{\\partial y} \\bigg) \\bigg| _{t_n, y_n} + O(h^3) $$\nFrom this, we can see that our coefficients must satisfy the following equations:\n$$ w_1 + w_2 = 1 \\\\ {} \\\\ w_2 \\alpha_2= \\frac{1}{2} \\\\ {} \\\\ w_2 \\beta_{2,1} = \\frac{1}{2} $$\nWith 3 equations and 4 unknows, there are infinitely many solutions. However, the standard choices are:\n$$ \\alpha_2 = \\beta_{2,1} = 1 \\\\ {} \\\\ w_1 = w_2 = \\frac{1}{2} $$\nThis brings us finally to the complete 2nd order Runge-Kutta Method:\n$$ k_1 = h y^\\prime (t_n, y_n) \\\\ {} \\\\ k_2 = h y^\\prime (t_n + h, y_n + k_1) \\\\ {} \\\\ y_{n+1} = y_n + \\frac{1}{2} k_1 + \\frac{1}{2} k_2 $$\nA similar (messier) method of expansions can be used to for higher order methods. The standard terms for the 4th order Runge-Kutta method are:\n$$ k_1 = h y^\\prime (t_n, y_n) \\\\ {} \\\\ k_2 = h y^\\prime (t_n + \\frac{h}{2}, y_n + \\frac{k_1}{2}) \\\\ {} \\\\ k_3 = h y^\\prime (t_n + \\frac{h}{2}, y_n + \\frac{k_2}{2}) \\\\ {} \\\\ k_4 = h y^\\prime (t_n + h, y_n + k_3) \\\\ {} \\\\ y_{n+1} = y_n + \\frac{1}{6} (k_1 + 2 k_2 + 2 k_3 + k_4) $$\nKeep in mind that there are infinitely many choices of these coefficients and lots of research has gone into figuring out which ones work best. We will stick to these simple standard ones for now.\nImplementing Runge-Kutta Now that we have finally derived the Runge-Kutta methods, let\u0026rsquo;s implement them in code. We will do both the 2nd order and 4th order methods and compare their accuracy.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def rk2(dydt, t0, y0, t_end, h): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the 2nd order Runge-Kutta method Args: dydt: The derivative function to integrate t0: Initial value of time y0: Initial value of y t_end: Final time for integration h: Step size Returns: An array of y values for each time step \u0026#34;\u0026#34;\u0026#34; t = np.arange(t0, t_end+h, h) # Array of time points y = np.zeros_like(t) y[0] = y0 for i in range(1, len(t)): k1 = h * dydt(t[i - 1], y[i - 1]) k2 = h * dydt(t[i - 1] + h, y[i - 1] + k1) y[i] = y[i - 1] + (k1 + k2) / 2 return y We can see how, at each time step, we calculate the \\( k_i \\) values and use these to estimate the next value of the function.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def rk4(dydt, t0, y0, t_end, h): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the 4th order Runge-Kutta method Args: dydt: The derivative function to integrate t0: Initial value of time y0: Initial value of y t_end: Final time for integration h: Step size Returns: An array of y values for each time step \u0026#34;\u0026#34;\u0026#34; t = np.arange(t0, t_end+h, h) y = np.zeros_like(t) y[0] = y0 for i in range(1, len(t)): k1 = h * dydt(t[i - 1], y[i - 1]) k2 = h * dydt(t[i - 1] + h / 2, y[i - 1] + k1 / 2) k3 = h * dydt(t[i - 1] + h / 2, y[i - 1] + k2 / 2) k4 = h * dydt(t[i], y[i - 1] + k3) y[i] = y[i - 1] + (k1 + 2 * k2 + 2 * k3 + k4) / 6 return y After all that derivation, the actual method is remarkably clean and simple to implement. Figure 5 gives an idea of how these higher order functions perform compared to our original Euler method.\nFigure 5: Comparing the accuracy of different order ODE solvers, h = 0.1\nAll the solvers are using the same step size here. As we can see, the 4th order method is better than the 2nd order method. They are both much more accurate than the 1st order Euler method.\nAs a final comparison, let\u0026rsquo;s look at Newton\u0026rsquo;s law of cooling one last time. Figure 6 shows how our 4th order solver compares to our previous test with the Euler method.\nFigure 6: Comparing the accuracy of the RK4 and Euler methods on the cooling equation. Tsurr = 20 C, k = 1, h = 0.5\nFigure 5 and 6 code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 \u0026#39;\u0026#39;\u0026#39; Implementing and comparing the Runge-Kutta 2nd and 4th order methods. We can also compare to the Euler method (1st order). Created by: simmeon Last Modified: 29/04/24 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) # Define some functions to test our solvers with def dydt(t, y): return 3 * t**2 + 20 * np.cos(10 * t) def analytical_solution(t): return t**3 + 2 * np.sin(10 * t) # Analytical solution to the ODE # # Define our cooling function # def solve_T(t, T0): # return T_surr + (T0 - T_surr)*np.exp(-k*t) # # Define the derivative (only depends on the current temp, not time) # def dTdt(T): # return -k * (T - T_surr) def euler_solver(dydt, t0, y0, t_end, h): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the Euler method Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y t_end: Final time for integration h: Step size Returns: An array of y values for each time step \u0026#34;\u0026#34;\u0026#34; t = np.arange(t0, t_end+h, h) y = np.zeros_like(t) y[0] = y0 for i in range(1, len(t)): y[i] = y[i - 1] + h * dydt(t[i - 1], y[i - 1]) return y def rk2(dydt, t0, y0, t_end, h): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the 2nd order Runge-Kutta method Args: dydt: The derivative function to integrate t0: Initial value of time y0: Initial value of y t_end: Final time for integration h: Step size Returns: An array of y values for each time step \u0026#34;\u0026#34;\u0026#34; t = np.arange(t0, t_end+h, h) # Array of time points y = np.zeros_like(t) y[0] = y0 for i in range(1, len(t)): k1 = h * dydt(t[i - 1], y[i - 1]) k2 = h * dydt(t[i - 1] + h, y[i - 1] + k1) y[i] = y[i - 1] + (k1 + k2) / 2 return y def rk4(dydt, t0, y0, t_end, h): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the 4th order Runge-Kutta method Args: dydt: The derivative function to integrate t0: Initial value of time y0: Initial value of y t_end: Final time for integration h: Step size Returns: An array of y values for each time step \u0026#34;\u0026#34;\u0026#34; t = np.arange(t0, t_end+h, h) y = np.zeros_like(t) y[0] = y0 for i in range(1, len(t)): k1 = dydt(t[i - 1], y[i - 1]) k2 = dydt(t[i - 1] + h / 2, y[i - 1] + k1 / 2) k3 = dydt(t[i - 1] + h / 2, y[i - 1] + k2 / 2) k4 = dydt(t[i], y[i - 1] + k3) y[i] = y[i - 1] + (k1 + 2 * k2 + 2 * k3 + k4) / 6 * h return y # Define simulation parameters t0 = 0 y0 = 0 t_end = 5 h = 0.1 # Solve using both methods y_euler = euler_solver(dydt, t0, y0, t_end, h) y_rk2 = rk2(dydt, t0, y0, t_end, h) y_rk4 = rk4(dydt, t0, y0, t_end, h) # Get analytical solution t = np.arange(t0, t_end+h, h) # Array of time points y_analytical = analytical_solution(t) # Calculate errors error_euler = np.abs(y_euler - y_analytical) error_rk2 = np.abs(y_rk2 - y_analytical) error_rk4 = np.abs(y_rk4 - y_analytical) # Plot the numerical solutions plt.subplot(1,2,1) plt.plot(t, y_euler, label=\u0026#39;Euler\u0026#39;) plt.plot(t, y_rk2, label=\u0026#39;RK2\u0026#39;) plt.plot(t, y_rk4, label=\u0026#39;RK4\u0026#39;) plt.plot(t, y_analytical, label=\u0026#39;Analytical\u0026#39;, linestyle=\u0026#39;--\u0026#39;) plt.xlabel(\u0026#39;Time (t)\u0026#39;, fontsize=20) plt.ylabel(\u0026#39;y(t)\u0026#39;, fontsize=20) plt.title(\u0026#39;Numerical Solutions\u0026#39;, fontsize=20) plt.legend(fontsize=15) # Plot the errors plt.subplot(1,2,2) plt.plot(t, error_euler, label=\u0026#39;Euler\u0026#39;) plt.plot(t, error_rk2, label=\u0026#39;RK2\u0026#39;) plt.plot(t, error_rk4, label=\u0026#39;RK4\u0026#39;) plt.xlabel(\u0026#39;Time (t)\u0026#39;, fontsize=20) plt.ylabel(\u0026#39;Abs Error\u0026#39;, fontsize=20) plt.title(\u0026#39;Absolute Errors of the Numerical Solutions\u0026#39;, fontsize=20) plt.legend(fontsize=15) plt.show() Even with a relatively large step size, the 4th order method is still much better than the Euler method. In fact, we could make the step size even bigger and still have a solution that fell within some tiny tolerance.\nBut how can we choose a good value to make this step size? If we want to be as efficient as possible, each step would as large as it can be while staying within the tolerance we want\u0026hellip;\nStep Sizing Constant Step Size Issues Take the following function,\n$$ y = \\sin{(t^5)} \\\\ {} \\\\ \\dfrac{dy}{dt} = 5t^4 \\cos{(t^5)} $$\nWe can use the derivative to solve for \\( y \\) using one of our numerical methods from earlier. Figure 7 shows us doing this using the Euler method, along with the error in our solution.\nFigure 7: Euler method solution and error using a constant step size, h = 0.05.\nFigure 7 code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 \u0026#39;\u0026#39;\u0026#39; Seeing how a constant step size numerical solver can have wildly varying error on diffrent steps. Created by: simmeon Last Modified: 18/05/24 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) # Define function and its derivative def dydt(t, y): return 5*t**4 * np.cos(t**5) def analytical_solution(t): return np.sin(t**5) # Our Euler numerical solver def euler_solver(dydt, t0, y0, t_end, h): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the Euler method Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y t_end: Final time for integration h: Step size Returns: An array of y values for each time step \u0026#34;\u0026#34;\u0026#34; t = np.arange(t0, t_end+h, h) y = np.zeros_like(t) y[0] = y0 for i in range(1, len(t)): y[i] = y[i - 1] + h * dydt(t[i - 1], y[i - 1]) return t, y # Define simulation parameters t0 = 0 y0 = 0 t_end = 2 h = 0.05 # we are using a constant step size here # Numerically integrate t_euler, y_euler = euler_solver(dydt, t0, y0, t_end, h) # Get analytical solution t_analytical = np.arange(t0, t_end+h, 0.001) y_analytical = analytical_solution(t_analytical) # Get error y_analytical_sampled = y_analytical[0:-1:50] error = np.abs(y_euler - y_analytical_sampled) # Plotting plt.subplot(2,1,1) plt.plot(t_analytical, y_analytical, label=\u0026#39;Analytical\u0026#39;, linestyle=\u0026#39;-\u0026#39;) plt.plot(t_euler, y_euler, label=\u0026#39;Euler method\u0026#39;) plt.title(\u0026#34;Error when integrating with a constant step size\u0026#34;, fontsize=20) plt.ylabel(\u0026#39;y\u0026#39;, fontsize=20) plt.legend(fontsize=15) plt.subplot(2,1,2) plt.plot(t_euler, error) plt.ylabel(\u0026#34;absolute error\u0026#34;, fontsize=20) plt.xlabel(\u0026#39;t\u0026#39;, fontsize=20) plt.show() The method works well initially when the function is changing slowly. However, when the function is changing more rapidly, our Euler method solver error gets large. You can try increasing the time to integrate over and see how the error becomes even bigger.\nThis is happening because our step size (\\( h = 0.05 \\)) becomes too large to properly capture how the function is changing. We could make our step size smaller, but we don\u0026rsquo;t need that additional resolution for the start of the function, it\u0026rsquo;s already pretty accurate.\nUsing Multiple Step Sizes Instead of decreasing the step size for the entire integration period, it would be much more efficient if we could just decrease it where we need to.\nIn this example, we could try switching to a smaller step size (say, \\( h = 0.01 \\)) after \\( t = 1 \\) when our solution starts getting less accurate. Doing this gives the solution shown in Figure 8.\nFigure 8: Euler method solution and error using two step sizes: t \u0026lt;= 1, h = 0.05; t \u0026gt; 1, h = 0.01\nFigure 8 code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 \u0026#39;\u0026#39;\u0026#39; Seeing how we can improve our solution and efficiency by using two different step sizes at different times. Created by: simmeon Last Modified: 18/05/24 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) # Define function and its derivative def dydt(t, y): return 5*t**4 * np.cos(t**5) def analytical_solution(t): return np.sin(t**5) # Our Euler numerical solver def euler_solver(dydt, t0, y0, t_end, h): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the Euler method Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y t_end: Final time for integration h: Step size Returns: An array of y values for each time step \u0026#34;\u0026#34;\u0026#34; t = np.arange(t0, t_end+h, h) y = np.zeros_like(t) y[0] = y0 for i in range(1, len(t)): y[i] = y[i - 1] + h * dydt(t[i - 1], y[i - 1]) return t, y # Define simulation parameters t0 = 0 y0 = 0 t_end1 = 1 t_end2 = 2 h1 = 0.05 # first, bigger step size h2 = 0.01 # second. smaller step size # Numerically integrate t_euler1, y_euler1 = euler_solver(dydt, t0, y0, t_end1, h1) t_euler2, y_euler2 = euler_solver(dydt, t_end1, y_euler1[-1], t_end2, h2) y_euler = np.concatenate((y_euler1, y_euler2)) t_euler = np.concatenate((t_euler1, t_euler2)) # Get analytical solution t_analytical = np.arange(t0, t_end2+h2, 0.001) y_analytical = analytical_solution(t_analytical) # Get error y_analytical_compare = analytical_solution(t_euler) error = np.abs(y_euler - y_analytical_compare) # Plotting plt.subplot(2,1,1) plt.plot(t_analytical, y_analytical, label=\u0026#39;Analytical\u0026#39;, linestyle=\u0026#39;-\u0026#39;) plt.plot(t_euler, y_euler, label=\u0026#39;Euler method\u0026#39;) plt.vlines(t_end1, -1, 1, colors=\u0026#39;white\u0026#39;, alpha=0.5) plt.title(\u0026#34;Error when integrating with two different step sizes\u0026#34;, fontsize=20) plt.ylabel(\u0026#39;y\u0026#39;, fontsize=20) plt.legend(fontsize=15) plt.subplot(2,1,2) plt.plot(t_euler, error) plt.vlines(t_end1, 0, 0.5, colors=\u0026#39;white\u0026#39;, alpha=0.5) plt.ylabel(\u0026#34;absolute error\u0026#34;, fontsize=20) plt.xlabel(\u0026#39;t\u0026#39;, fontsize=20) plt.show() This has made our solution around an order of magnitude more accurate. But how much computational complexity have we saved compared to just solving the entire time interval with the smaller step size? We can summarize this in Table 1.\nTable 1\nStep size Number of iteration steps \\( h = 0.05 \\) 41 \\( t \\leq 1, h = 0.05 \\\\ t\u0026gt;1, h = 0.01\\) 122 \\( h = 0.01 \\) 201 So we are saving a significant amount of computation compared to making the whole interval have a smaller step size.\nChoosing to make the step size smaller at \\( t = 1 \\) was very arbitrary. We could also change the step size in more places \u0026ndash; maybe making it even bigger initially and even smaller for \\( t \u0026gt; 1.5 \\). We should come up with a smart way of choosing when and how to change the step size.\nAdaptive Step Sizing Since we want our solution to be accurate, it would be smart to decrease our step size when our error is getting too big. Also, to keep things efficient, we could make our step size bigger when the error is very small.\nThe issue is we don\u0026rsquo;t have the actual solution, so how can we know what our error is?\nWe need to create some sort of \u0026ldquo;true solution\u0026rdquo; to compare our estimate against. One way we could do this is to use a higher order method, since we know that these should be more accurate and be closer to the actual solution. For example, if we want to use a 1st order Euler solver, we could also calculate a 2nd order Runge-Kutta solution and use that as the \u0026ldquo;true solution\u0026rdquo;.\nSo for every step, we will calculate the next step with two methods: a lower and higher order one. Then, we will use difference in these as our approximation of the error. From that, we can decide whether we should change the step size for the step or if it was ok.\nThe last thing to decide is how much we should change the step size. For now, a simple approach could be to halve the step size if the error was too big, and double it if it was too small.\nMore formally, we will:\nTake a step with the current step size using a 1st and 2nd order method. Compare the solutions to approximate our error. If the \\( error \u0026gt; tol \\), reject the step, halve the step size, and go back to Step 1. If the \\( error \u0026lt; tol \\), accept the step and double the step size for the next step. In code this would look like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def euler_solver(dydt, t0, y0, t_end, tol): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the Euler method with adaptive step sizing. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y t_end: Final time for integration tol: Error tolerance Returns: Arrays of y and t values for each time step \u0026#34;\u0026#34;\u0026#34; h = 0.1 # set some initial step size t = [t0] y_euler = [y0] y_rk2 = [y0] i = 1 while t[-1] \u0026lt; t_end: # Take a Euler step y_euler.append(euler_step(dydt, t[i-1], y_euler[i-1], h)) t.append(t[i-1] + h) # Take a RK2 step, starting from the previous Euler step value y_rk2.append(rk2_step(dydt, t[i-1], y_euler[i-1], h)) isStepGood = False error = np.abs(y_rk2[i] - y_euler[i]) if error \u0026lt; tol: # accept step, make step size bigger for next step isStepGood = True h = h * 2 while not isStepGood: # Take a step with both methods y_euler[i] = euler_step(dydt, t[i-1], y_euler[i-1], h) t[i] = t[i-1] + h y_rk2[i] = rk2_step(dydt, t[i-1], y_euler[i-1], h) # Calculate the error between the methods error = np.abs(y_rk2[i] - y_euler[i]) # Check error to accept or reject the step if error \u0026gt; tol: # reject step, halve step size h = h / 2 else: # accept step, make step size bigger for next step isStepGood = True h = h * 2 i += 1 return t, y_euler If we use this to solve the same function as before, we get the solution shown in Figure 9.\nFigure 9: Euler method solution and error with an adaptive step size, tol = 0.01.\nFigure 9 code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 \u0026#39;\u0026#39;\u0026#39; Implementing an adaptive step size by doubling or halving the step size depending on the error. Created by: simmeon Last Modified: 18/05/24 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) # Define function and its derivative def dydt(t, y): return 5*t**4 * np.cos(t**5) def analytical_solution(t): return np.sin(t**5) def euler_step(dydt, t0, y0, h): \u0026#34;\u0026#34;\u0026#34; Takes a single 1st order integration step and returns the y value. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y h: Step size Returns: The y value after the step. \u0026#34;\u0026#34;\u0026#34; y = y0 + h * dydt(t0, y0) return y def rk2_step(dydt, t0, y0, h): \u0026#34;\u0026#34;\u0026#34; Takes a single 2nd order integration step and returns the y value. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y h: Step size Returns: The y value after the step. \u0026#34;\u0026#34;\u0026#34; k1 = h * dydt(t0, y0) k2 = h * dydt(t0 + h, y0 + k1) y = y0 + (k1 + k2) / 2 return y # Our adaptive Euler numerical solver def euler_solver(dydt, t0, y0, t_end, tol): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the Euler method with adaptive step sizing. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y t_end: Final time for integration tol: Error tolerance Returns: Arrays of y, t and local error values for each time step \u0026#34;\u0026#34;\u0026#34; h = 0.1 # set some initial step size t = [t0] y_euler = [y0] y_rk2 = [y0] error = [0] i = 1 while t[-1] \u0026lt; t_end: # Take a Euler step y_euler.append(euler_step(dydt, t[i-1], y_euler[i-1], h)) t.append(t[i-1] + h) # Take a RK2 step, starting from the previous Euler step value y_rk2.append(rk2_step(dydt, t[i-1], y_euler[i-1], h)) isStepGood = False error.append(np.abs(y_rk2[i] - y_euler[i])) if error[i] \u0026lt; tol: # accept step, make step size bigger for next step isStepGood = True h = h * 2 while not isStepGood: # Take a step with both methods y_euler[i] = euler_step(dydt, t[i-1], y_euler[i-1], h) t[i] = t[i-1] + h y_rk2[i] = rk2_step(dydt, t[i-1], y_euler[i-1], h) # Calculate the error between the methods error[i] = np.abs(y_rk2[i] - y_euler[i]) # Check error to accept or reject the step if error[i] \u0026gt; tol: # reject step, halve step size h = h / 2 else: # accept step, make step size bigger for next step isStepGood = True h = h * 2 i += 1 return t, y_euler, error # Define simulation parameters t0 = 0 y0 = 0 t_end = 2 tol = 1e-2 # Numerically integrate t_euler, y_euler, local_error = euler_solver(dydt, t0, y0, t_end, tol) t_euler = np.array(t_euler) y_euler = np.array(y_euler) # Get analytical solution t_analytical = np.arange(t0, t_end, 0.001) y_analytical = analytical_solution(t_analytical) # Get error y_analytical_compare = analytical_solution(t_euler) error = np.abs(y_euler - y_analytical_compare) # Plotting plt.subplot(3,1,1) plt.plot(t_analytical, y_analytical, label=\u0026#39;Analytical\u0026#39;, linestyle=\u0026#39;-\u0026#39;) plt.plot(t_euler, y_euler, label=\u0026#39;Euler method\u0026#39;) plt.scatter(t_euler, y_euler, s=30, marker=\u0026#39;o\u0026#39;, facecolors=\u0026#39;none\u0026#39;, color=\u0026#39;C1\u0026#39;) plt.title(\u0026#34;Integrating with a adaptive step size, tol = 0.01\u0026#34;, fontsize=20) plt.ylabel(\u0026#39;y\u0026#39;, fontsize=20) plt.legend(fontsize=15) plt.text(0, -0.75, f\u0026#34;Number of steps: {len(t_euler)}\u0026#34;, fontsize=15) plt.subplot(3,1,2) plt.plot(t_euler, error) plt.ylabel(\u0026#34;absolute error\u0026#34;, fontsize=20) plt.xlabel(\u0026#39;t\u0026#39;, fontsize=20) plt.subplot(3,1,3) plt.plot(t_euler, local_error) plt.ylabel(\u0026#34;local step error\u0026#34;, fontsize=20) plt.xlabel(\u0026#39;t\u0026#39;, fontsize=20) plt.show() Each dot represents where a step was taken. We can see how the step size is bigger when the function is changing slowly and smaller when the function changes quickly.\nThe error is interesting. We set a tolerance of \\( 0.01 \\) but the absolute error gets up to nearly \\( 0.1 \\). This is because the tolerance is used to set the allowable error for each step, not the overall (or global) error. Another reason is that our error is an estimate but here we are plotting against the actual, analytical error. Howveer, looking at the local error for each step we can see that it never gets above our tolerance.\nChoosing Better Step Size Changes We used a very simple method of halving the step size if the error was too big or doubling the step size if the error was too small. For the 255 steps we ended up with, the method calculated the next step 746 times. This means that, on average for each step, we changed the step size 2.9 times before finding a value that worked. There is a lot of computation that is wasted there.\nLet\u0026rsquo;s try and come up with a better way of choosing the step size to hopefully make our method more efficient.\nThis will depend on what order methods we are using.\nWe will start by doing this for the 1st order Euler method. We write this as:\n$$ y_{n+1} = y_{n} + h \\dfrac{dy}{dt} \\bigg| _{t_n, y_n} + O(h^2) $$\nThe error in our estimate is proportional to \\( h^2 \\). If we assume that this error is constant over time, we can say\n$$ error = \\Delta = c h^2 $$\nwhere \\( c \\) is some constant value. Then, for some step size, \\( h_1 \\), we would get\n$$ \\Delta _1 = c h_1^2 $$\nWe could also write an equation for what the step size would have to be to get an error equal to our tolerance. Let\u0026rsquo;s call this error \\( \\Delta _0 \\).\n$$ \\Delta _0 = c h_0^2 $$\nThis \\( h_0 \\) is the value that we would want to use for the current step size to. In other words, it will give the largest step that stays within our tolerance.\nWe can use these two equations together through the constant \\( c \\) to get the relation:\n$$ \\frac{\\Delta_1}{h_1^2} = \\frac{\\Delta_0}{h_0^2} $$\nThis can then be arranged to solve for what our step size \\( h_0 \\) should be\n$$ h_0 = h_1 \\bigg(\\frac{\\Delta _0}{\\Delta _1} \\bigg) ^{0.5} $$\nor in more familiar notation\n$$ h_{new} = h_{current} \\bigg(\\frac{tol}{error} \\bigg) ^{0.5} $$\nIn theory, using this formula should mean that we get the correct step size in one iteration instead of two or three. However, in practice, our error is only an approximation and so we should put in a safety factor to make the new step size slightly smaller that the theoretical value:\n$$ h_{new} = 0.9 h_{current} \\bigg(\\frac{tol}{error} \\bigg) ^{0.5} $$\nLet\u0026rsquo;s use this instead of our doubling and halving approach and see how many calculations we use. Note that formula has the correct behaviour for errors that are both larger and smaller than the tolerance. We can see the result in Figure 10.\nFigure 10: Euler method solution and error with a better adaptive step size, tol = 0.01.\nFigure 10 code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 \u0026#39;\u0026#39;\u0026#39; Implementing an adaptive step size with better step sizing. Created by: simmeon Last Modified: 19/05/24 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) # Define function and its derivative def dydt(t, y): return 5*t**4 * np.cos(t**5) def analytical_solution(t): return np.sin(t**5) def euler_step(dydt, t0, y0, h): \u0026#34;\u0026#34;\u0026#34; Takes a single 1st order integration step and returns the y value. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y h: Step size Returns: The y value after the step. \u0026#34;\u0026#34;\u0026#34; y = y0 + h * dydt(t0, y0) return y def rk2_step(dydt, t0, y0, h): \u0026#34;\u0026#34;\u0026#34; Takes a single 2nd order integration step and returns the y value. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y h: Step size Returns: The y value after the step. \u0026#34;\u0026#34;\u0026#34; k1 = h * dydt(t0, y0) k2 = h * dydt(t0 + h, y0 + k1) y = y0 + (k1 + k2) / 2 return y # Our adaptive Euler numerical solver def euler_solver(dydt, t0, y0, t_end, tol): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the Euler method with adaptive step sizing. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y t_end: Final time for integration tol: Error tolerance Returns: Arrays of y, t and local error values for each time step \u0026#34;\u0026#34;\u0026#34; h = 0.1 # set some initial step size t = [t0] y_euler = [y0] y_rk2 = [y0] error = [0] i = 1 count = 0 while t[-1] \u0026lt; t_end: count += 1 # Take a Euler step y_euler.append(euler_step(dydt, t[i-1], y_euler[i-1], h)) t.append(t[i-1] + h) # Take a RK2 step, starting from the previous Euler step value y_rk2.append(rk2_step(dydt, t[i-1], y_euler[i-1], h)) isStepGood = False error.append(np.abs(y_rk2[i] - y_euler[i])) if error[i] \u0026lt; tol: # accept step, make step size bigger for next step isStepGood = True h = 0.9 * h * (tol / error[i]) ** 0.5 while not isStepGood: count += 1 # Take a step with both methods y_euler[i] = euler_step(dydt, t[i-1], y_euler[i-1], h) t[i] = t[i-1] + h y_rk2[i] = rk2_step(dydt, t[i-1], y_euler[i-1], h) # Calculate the error between the methods error[i] = np.abs(y_rk2[i] - y_euler[i]) # Check error to accept or reject the step if error[i] \u0026gt; tol: # reject step, halve step size h = 0.9 * h * (tol / error[i]) ** 0.5 else: # accept step, make step size bigger for next step isStepGood = True h = 0.9 * h * (tol / error[i]) ** 0.5 i += 1 print(\u0026#34;Count: \u0026#34;, count) return t, y_euler, error # Define simulation parameters t0 = 0 y0 = 0 t_end = 2 tol = 1e-2 # Numerically integrate t_euler, y_euler, local_error = euler_solver(dydt, t0, y0, t_end, tol) t_euler = np.array(t_euler) y_euler = np.array(y_euler) # Get analytical solution t_analytical = np.arange(t0, t_end, 0.001) y_analytical = analytical_solution(t_analytical) # Get error y_analytical_compare = analytical_solution(t_euler) error = np.abs(y_euler - y_analytical_compare) # Plotting plt.subplot(3,1,1) plt.plot(t_analytical, y_analytical, label=\u0026#39;Analytical\u0026#39;, linestyle=\u0026#39;-\u0026#39;) plt.plot(t_euler, y_euler, label=\u0026#39;Euler method\u0026#39;) plt.scatter(t_euler, y_euler, s=30, marker=\u0026#39;o\u0026#39;, facecolors=\u0026#39;none\u0026#39;, color=\u0026#39;C1\u0026#39;) plt.title(\u0026#34;Integrating with a adaptive step size, tol = 0.01\u0026#34;, fontsize=20) plt.ylabel(\u0026#39;y\u0026#39;, fontsize=20) plt.legend(fontsize=15) plt.text(0, -0.75, f\u0026#34;Number of steps: {len(t_euler)}\u0026#34;, fontsize=15) plt.subplot(3,1,2) plt.plot(t_euler, error) plt.ylabel(\u0026#34;absolute error\u0026#34;, fontsize=20) plt.subplot(3,1,3) plt.plot(t_euler, local_error) plt.ylabel(\u0026#34;local step error\u0026#34;, fontsize=20) plt.xlabel(\u0026#39;t\u0026#39;, fontsize=20) plt.show() This change used 216 steps with a total of 323 step calulations, meaning we changed the step size 1.5 times per step. This is much closer to the minimum of 1 change per step. It also ended up requiring less steps than previously, being even more efficient.\nThis step change formula we created can be applied to higher order methods, for example if we were using a 4th order method with 5th order errors we would use:\n$$ h_{new} = 0.9 h_{current} \\bigg(\\frac{tol}{error} \\bigg) ^{\\frac{1}{5}} $$\nA Full RK45 Method Finally, we have all the parts to create a working 4th order Runge-Kutta numerical solver with adaptive step sizing based on 5th order errors.\nWe will start by writing a function that takes a single 5th order step. We are going to use the 5th order step to approximate our \u0026ldquo;true solution\u0026rdquo; and compare it to the 4th order step to find our error. The function will then return the 4th order step and the error. One reason this method is so good is because calculating the 5th step reuses a lot of the RK4 step calculations, making it very efficient.\nTo do this we will be using the Dormand-Prince coefficients. These are much messier than the standard coefficients we used before. However, they are very good at minimising the error in the 5th order approximation, which is exactly what we want \u0026ndash; we want that 5th order approximation to be as close to the real solution as possible.\nThe intermediate approximations are calculated as follows:\n$$ \\begin{aligned} \u0026amp;k_1 = hf(t_n, y_n) \\\\ {} \\\\ \u0026amp;k_2 = hf \\Big(t_n + \\frac{1}{5}h, y_n + \\frac{1}{5} k_1 \\Big) \\\\ {} \\\\ \u0026amp;k_3 = hf \\Big(t_n + \\frac{3}{10}h, y_n + \\frac{3}{40} k_1 + \\frac{9}{40} k_2 \\Big) \\\\ {} \\\\ \u0026amp;k_4 = hf \\Big(t_n + \\frac{4}{5}h, y_n + \\frac{44}{45} k_1 - \\frac{56}{15} k_2 + \\frac{32}{9} k_3 \\Big) \\\\ {} \\\\ \u0026amp;k_5 = hf \\Big(t_n + \\frac{8}{9}h, y_n + \\frac{19372}{6561} k_1 - \\frac{25360}{2187} k_2 + \\frac{64448}{6561} k_3 - \\frac{212}{729} k_4 \\Big) \\\\ {} \\\\ \u0026amp;k_6 = hf \\Big(t_n + h, y_n + \\frac{9017}{3168} k_1 - \\frac{355}{33} k_2 - \\frac{46732}{5247} k_3 + \\frac{49}{176} k_4 - \\frac{5103}{18656} k_5 \\Big) \\\\ {} \\\\ \u0026amp;k_7 = hf \\Big(t_n + h, y_n + \\frac{35}{848} k_1 + \\frac{500}{1113} k_3 + \\frac{125}{192} k_4 - \\frac{2187}{6784} k_5 + \\frac{11}{84} k_6 \\Big) \\end{aligned} $$\nThen, the next 4th order step is calculated as\n$$ y_{n+1} = y_n + \\frac{35}{384} k_1 + \\frac{500}{1113} k_3 + \\frac{125}{192} k_4 - \\frac{2187}{6784} k_5 + \\frac{11}{84} k_6 $$\nNote that these are the same coefficients that we use for \\( k_7 \\), which is useful. We can rewrite \\( k_7 \\) as\n$$ k_7 = hf \\Big( t_n + h, y_{n+1} \\Big) $$\nThis value is then the same as \\( k_1 \\) will be in the next step (except for \\( h \\) changing). This means that, except for the first step, we only have to calculate 6 derivatives per step.\nThen, the 5th order step, \\( z_{n+1} \\), is calculated as\n$$ z_{n+1} = y_n + \\frac{5179}{57600} k_1 + \\frac{7571}{16695} k_3 + \\frac{393}{640} k_4 - \\frac{92097}{339200} k_5 + \\frac{187}{2100} k_6 + \\frac{1}{40} k_7 $$\nThis gives the error\n$$ error = \\Big| z_{n+1} - y_{n+1} \\Big| = \\Big| -\\frac{71}{57600} k_1 + \\frac{71}{16695} k_3 - \\frac{71}{1920} k_4 + \\frac{17253}{339200} k_5 - \\frac{22}{525} k_6 + \\frac{1}{40} k_7 \\Big| $$\nand from earlier we know that our step size change will be\n$$ h_{new} = 0.9 h_{current} \\bigg(\\frac{tol}{error} \\bigg) ^{0.2} $$\nLet\u0026rsquo;s write this step function now, along with the function that adaptively steps to return the solution.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 # ----- Dormand-Prince coefficients ----- # # Alpha in notation, coefficients describe what percentage of the full # step we evaluate each derivative at A = np.array([0, 1/5, 3/10, 4/5, 8/9, 1, 1]) # Beta in notation, coefficients describe how much of each previous change in y # we add to the current estimate B = np.array([ [0, 0, 0, 0, 0, 0], [1/5, 0, 0, 0, 0, 0], [3/40, 9/40, 0, 0, 0, 0], [44/45, -56/15, 32/9, 0, 0, 0], [19372/6561, -25360/2187, 64448/6561, -212/729, 0, 0], [9017/3168, -355/33, 46732/5247, 49/176, -5103/18656, 0], [35/384, 0, 500/1113, 125/192, -2187/6784, 11/84] ]) # Weighting for each derivative in the approximation, w in notation W = np.array([35/384, 0, 500/1113, 125/192, -2187/6784, 11/84, 0]) #W = np.array([5179/57600, 0, 7571/16695, 393/640, -92097/339200, 187/2100, 1/40]) # Coefficients for error calculation E = np.array([-71/57600, 0, 71/16695, -71/1920, 17253/339200, -22/525, 1/40]) # ------------------------------------------------------------------ # # Define a function to take a single RK45 step def rk45_step(dydt, t0, y0, h): \u0026#34;\u0026#34;\u0026#34; Takes a single 5th order integration step and returns the 4th order step along with the 5th order error. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y h: Step size Returns: The the 4th order y value along with the 5th order error. \u0026#34;\u0026#34;\u0026#34; k1 = h * dydt(t0, y0) k2 = h * dydt(t0 + A[1] * h, y0 + B[1][0]*k1) k3 = h * dydt(t0 + A[2] * h, y0 + B[2][0]*k1 + B[2][1]*k2) k4 = h * dydt(t0 + A[3] * h, y0 + B[3][0]*k1 + B[3][1]*k2 + B[3][2]*k3) k5 = h * dydt(t0 + A[4] * h, y0 + B[4][0]*k1 + B[4][1]*k2 + B[4][2]*k3 + B[4][3]*k4) k6 = h * dydt(t0 + A[5] * h, y0 + B[5][0]*k1 + B[5][1]*k2 + B[5][2]*k3 + B[5][3]*k4 + B[5][4]*k5) y = y0 + W[0]*k1 + W[1]*k2 + W[2]*k3 + W[3]*k4 + W[4]*k5 + W[5]*k6 k7 = h * dydt(t0 + A[6] * h, y) error = np.abs( E[0]*k1 + E[1]*k2 + E[2]*k3 + E[3]*k4 + E[4]*k5 + E[5]*k6 + E[6]*k7 ) return y, error # Define the full RK45 solver function def rk45_solver(dydt, t0, y0, t_end, tol): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the RK45 method. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y t_end: Final time for integration tol: Error tolerance Returns: Arrays of y, t and local error values for each time step \u0026#34;\u0026#34;\u0026#34; h = 1e-3 # set some initial step size t = [t0] y = [y0] error = [0] i = 1 while t[-1] \u0026lt; t_end: # Take a step t.append(t[i-1] + h) y_step, error_step = rk45_step(dydt, t[i-1], y[i-1], h) y.append(y_step) error.append(error_step) # Update step size after step h = 0.9 * h * (tol / error[i]) ** 0.2 isStepGood = False if error[i] \u0026lt; tol: # accept step isStepGood = True while not isStepGood: # If there was too much error... # Take a step t[i] = t[i-1] + h # update our time value with the new step size y_step, error_step = rk45_step(dydt, t[i-1], y[i-1], h) y[i] = y_step error[i] = error_step # Update step size after step h = 0.9 * h * (tol / error[i]) ** 0.2 # Check error to accept or reject the step if error[i] \u0026lt; tol: isStepGood = True i += 1 return t, y, error Using this step function along with updating the step size gives the result in Figure 11.\nFigure 11: RK45 Solver with error.\nFigure 11 code (full solver code + graphing)\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 \u0026#39;\u0026#39;\u0026#39; Implementing a complete RK45 algorithm. Created by: simmeon Last Modified: 19/05/24 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) # Define function (for analytical solution) and its derivative def dydt(t, y): return 5*t**4 * np.cos(t**5) def analytical_solution(t): return np.sin(t**5) # ----- Dormand-Prince coefficients ----- # # Alpha in notation, coefficients describe what percentage of the full # step we evaluate each derivative at A = np.array([0, 1/5, 3/10, 4/5, 8/9, 1, 1]) # Beta in notation, coefficients describe how much of each previous change in y # we add to the current estimate B = np.array([ [0, 0, 0, 0, 0, 0], [1/5, 0, 0, 0, 0, 0], [3/40, 9/40, 0, 0, 0, 0], [44/45, -56/15, 32/9, 0, 0, 0], [19372/6561, -25360/2187, 64448/6561, -212/729, 0, 0], [9017/3168, -355/33, 46732/5247, 49/176, -5103/18656, 0], [35/384, 0, 500/1113, 125/192, -2187/6784, 11/84] ]) # Weighting for each derivative in the approximation, w in notation W = np.array([35/384, 0, 500/1113, 125/192, -2187/6784, 11/84, 0]) #W = np.array([5179/57600, 0, 7571/16695, 393/640, -92097/339200, 187/2100, 1/40]) # Coefficients for error calculation E = np.array([-71/57600, 0, 71/16695, -71/1920, 17253/339200, -22/525, 1/40]) # ------------------------------------------------------------------ # # Define a function to take a single RK45 step # To improve efficiency, k7 could be reused as k1 in the following step def rk45_step(dydt, t0, y0, h): \u0026#34;\u0026#34;\u0026#34; Takes a single 5th order integration step and returns the 4th order step along with the 5th order error. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y h: Step size Returns: The the 4th order y value along with the 5th order error. \u0026#34;\u0026#34;\u0026#34; k1 = h * dydt(t0, y0) k2 = h * dydt(t0 + A[1] * h, y0 + B[1][0]*k1) k3 = h * dydt(t0 + A[2] * h, y0 + B[2][0]*k1 + B[2][1]*k2) k4 = h * dydt(t0 + A[3] * h, y0 + B[3][0]*k1 + B[3][1]*k2 + B[3][2]*k3) k5 = h * dydt(t0 + A[4] * h, y0 + B[4][0]*k1 + B[4][1]*k2 + B[4][2]*k3 + B[4][3]*k4) k6 = h * dydt(t0 + A[5] * h, y0 + B[5][0]*k1 + B[5][1]*k2 + B[5][2]*k3 + B[5][3]*k4 + B[5][4]*k5) y = y0 + W[0]*k1 + W[1]*k2 + W[2]*k3 + W[3]*k4 + W[4]*k5 + W[5]*k6 k7 = h * dydt(t0 + A[6] * h, y) error = np.abs( E[0]*k1 + E[1]*k2 + E[2]*k3 + E[3]*k4 + E[4]*k5 + E[5]*k6 + E[6]*k7 ) return y, error # Define the full RK45 solver function def rk45_solver(dydt, t0, y0, t_end, tol): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the RK45 method. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y t_end: Final time for integration tol: Error tolerance Returns: Arrays of y, t and local error values for each time step \u0026#34;\u0026#34;\u0026#34; h = 1e-3 # set some initial step size t = [t0] y = [y0] error = [0] i = 1 while t[-1] \u0026lt; t_end: # Take a step t.append(t[i-1] + h) y_step, error_step = rk45_step(dydt, t[i-1], y[i-1], h) y.append(y_step) error.append(error_step) # Update step size after step h = 0.9 * h * (tol / error[i]) ** 0.2 isStepGood = False if error[i] \u0026lt; tol: # accept step isStepGood = True while not isStepGood: # If there was too much error... # Take a step t[i] = t[i-1] + h # update our time value with the new step size y_step, error_step = rk45_step(dydt, t[i-1], y[i-1], h) y[i] = y_step error[i] = error_step # Update step size after step h = 0.9 * h * (tol / error[i]) ** 0.2 # Check error to accept or reject the step if error[i] \u0026lt; tol: isStepGood = True i += 1 return t, y, error # Define simulation parameters t0 = 0 y0 = 0 t_end = 2 tol = 1e-6 # Numerically integrate t_rk45, y_rk45, local_error = rk45_solver(dydt, t0, y0, t_end, tol) t_rk45 = np.array(t_rk45) y_rk45 = np.array(y_rk45) # Get analytical solution t_analytical = np.arange(t0, t_end, 0.001) y_analytical = analytical_solution(t_analytical) # Get error y_analytical_compare = analytical_solution(t_rk45) error = np.abs(y_rk45 - y_analytical_compare) # Plotting plt.subplot(3,1,1) plt.plot(t_analytical, y_analytical, label=\u0026#39;Analytical\u0026#39;, linestyle=\u0026#39;-\u0026#39;) plt.plot(t_rk45, y_rk45, label=\u0026#39;RK45 method\u0026#39;) #plt.scatter(t_rk45, y_rk45, s=30, marker=\u0026#39;o\u0026#39;, facecolors=\u0026#39;none\u0026#39;, color=\u0026#39;C1\u0026#39;) plt.title(f\u0026#34;RK45 Solver, tol = {tol}\u0026#34;, fontsize=20) plt.ylabel(\u0026#39;y\u0026#39;, fontsize=20) plt.legend(fontsize=15) plt.text(0, -0.75, f\u0026#34;Number of steps: {len(t_rk45)}\u0026#34;, fontsize=15) plt.subplot(3,1,2) plt.plot(t_rk45, error) plt.ylabel(\u0026#34;absolute error\u0026#34;, fontsize=20) plt.subplot(3,1,3) plt.plot(t_rk45, local_error) plt.ylabel(\u0026#34;local step error\u0026#34;, fontsize=20) plt.xlabel(\u0026#39;t\u0026#39;, fontsize=20) plt.show() As we can see, the method is incredibly efficient and accurate. In comparison, our 1st order adaptive method used over 200 steps for a tolerance of only 1e-03. This combination of efficiency and accuracy makes the RK45 method an incredibly good general solver.\nA Final Application So far we have used mostly trivial or contrived examples to demonstrate why the RK45 solver is good. To finish off, we will use a real-world example where the adaptability, efficiency, and accuracy of the RK45 solver is very useful.\nWe will be using the solver to propagate a satellite orbit around the earth. We will skip some details of deriving the relevant physics, but the important equation is:\n$$ F = \\frac{G m_{earth} m_{sat}}{r^2} $$\nLet\u0026rsquo;s define the gravitational parameter, \\( \\mu \\)\n$$ \\mu = G(m_{earth} + m_{sat}) $$\nBy assuming an inertial reference frame centred on the Earth, we get\n$$ \\ddot{\\vec{r}} = - \\frac{G(m_{earth} + m_{sat})}{r^3}\\vec{r} \\\\ {} \\\\ \\ddot{\\vec{r}} = - \\frac{\\mu}{r^3}\\vec{r} $$\nwhere \\( \\vec{r} \\) is the position vector of the satellite we want to solve for.\nThis is currently a second order ODE. We can use a state space to transform it into being first order:\n$$ \\bold{x} = \\begin{bmatrix} r_x \\\\ r_y \\\\ \\dot{r}_x \\\\ \\dot{r}_y \\end{bmatrix} \\\\ {} \\\\ {} \\\\ \\bold{\\dot{x}} = \\begin{bmatrix} \\dot{r}_x \\\\ {} \\\\ \\dot{r}_y \\\\ {} \\\\ \\ddot{r}_x \\\\ {} \\\\ \\ddot{r}_y \\\\ \\end{bmatrix} = \\begin{bmatrix} \\dot{r}_x \\\\ {} \\\\ \\dot{r}_y \\\\ {} \\\\ -\\Large \\frac{\\mu r_x}{|r|^3} \\\\ {} \\\\ -\\Large \\frac{\\mu r_y}{|r|^3} \\\\ \\end{bmatrix} \\\\ {} \\\\ {} \\\\ \\bold{\\dot{x}} = \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ -\\Large \\frac{\\mu}{|r|^3} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; -\\Large \\frac{\\mu}{|r|^3} \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} \\bold{x} $$\nThis is now a system of first order ODEs where we are solving for \\( \\bold{x} \\). With some small code changes to allow for vector inputs, Figure 12 shows the resulting satellite position and velocity for some initial state. To deal with multiple ODE systems with arrays of solutions instead of just a single value, we use the maximum error as our test for whether the step was good or not.\nFigure 12: Propagating an orbit with RK45.\nFigure 12 code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 \u0026#39;\u0026#39;\u0026#39; An application of the RK45 ODE solver. We will solve for the position of a satellite around Earth given an initial position and velocity. Created by: simmeon Last Modified: 19/05/24 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) # Define derivative def dxdt(t, x): r_mag = np.sqrt(x[0]**2 + x[1]**2) mu = 398600.4415; # [km^3 / (kg*s^2)] return np.array([x[2], x[3], -mu * x[0] / (r_mag**3), -mu * x[1] / (r_mag**3)]) # ----- Dormand-Prince coefficients ----- # # Alpha in notation, coefficients describe what percentage of the full # step we evaluate each derivative at A = np.array([0, 1/5, 3/10, 4/5, 8/9, 1, 1]) # Beta in notation, coefficients describe how much of each previous change in y # we add to the current estimate B = np.array([ [0, 0, 0, 0, 0, 0], [1/5, 0, 0, 0, 0, 0], [3/40, 9/40, 0, 0, 0, 0], [44/45, -56/15, 32/9, 0, 0, 0], [19372/6561, -25360/2187, 64448/6561, -212/729, 0, 0], [9017/3168, -355/33, 46732/5247, 49/176, -5103/18656, 0], [35/384, 0, 500/1113, 125/192, -2187/6784, 11/84] ]) # Weighting for each derivative in the approximation, w in notation W = np.array([35/384, 0, 500/1113, 125/192, -2187/6784, 11/84, 0]) #W = np.array([5179/57600, 0, 7571/16695, 393/640, -92097/339200, 187/2100, 1/40]) # Coefficients for error calculation E = np.array([-71/57600, 0, 71/16695, -71/1920, 17253/339200, -22/525, 1/40]) # ------------------------------------------------------------------ # # Define a function to take a single RK45 step # To improve efficiency, k7 can be reused as k1 in the following step def rk45_step(dydt, t0, y0, h): \u0026#34;\u0026#34;\u0026#34; Takes a single 5th order integration step and returns the 4th order step along with the 5th order error. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y h: Step size Returns: The the 4th order y value along with the 5th order error. \u0026#34;\u0026#34;\u0026#34; k1 = h * dydt(t0, y0) k2 = h * dydt(t0 + A[1] * h, y0 + B[1][0]*k1) k3 = h * dydt(t0 + A[2] * h, y0 + B[2][0]*k1 + B[2][1]*k2) k4 = h * dydt(t0 + A[3] * h, y0 + B[3][0]*k1 + B[3][1]*k2 + B[3][2]*k3) k5 = h * dydt(t0 + A[4] * h, y0 + B[4][0]*k1 + B[4][1]*k2 + B[4][2]*k3 + B[4][3]*k4) k6 = h * dydt(t0 + A[5] * h, y0 + B[5][0]*k1 + B[5][1]*k2 + B[5][2]*k3 + B[5][3]*k4 + B[5][4]*k5) y = y0 + W[0]*k1 + W[1]*k2 + W[2]*k3 + W[3]*k4 + W[4]*k5 + W[5]*k6 k7 = h * dydt(t0 + A[6] * h, y) error = np.abs( E[0]*k1 + E[1]*k2 + E[2]*k3 + E[3]*k4 + E[4]*k5 + E[5]*k6 + E[6]*k7 ) return y, error # Define the full RK45 solver function def rk45_solver(dydt, t0, y0, t_end, tol): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the RK45 method. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y t_end: Final time for integration tol: Error tolerance Returns: Arrays of y, t and local error values for each time step \u0026#34;\u0026#34;\u0026#34; h = 1e-3 # set some initial step size t = [t0] y = np.array([y0]) error = [0] i = 1 while t[-1] \u0026lt; t_end: # Take a step t.append(t[i-1] + h) y_step, error_step = rk45_step(dydt, t[i-1], y[i-1], h) y = np.append(y, np.array([y_step]), axis=0) error.append(error_step) # Update step size after step h = 0.9 * h * (tol / max(error[i])) ** 0.2 isStepGood = False if max(error[i]) \u0026lt; tol: # accept step isStepGood = True while not isStepGood: # If there was too much error... # Take a step t[i] = t[i-1] + h # update our time value with the new step size y_step, error_step = rk45_step(dydt, t[i-1], y[i-1], h) y[i] = y_step error[i] = error_step # Update step size after step h = 0.9 * h * (tol / max(error[i])) ** 0.2 # Check error to accept or reject the step if max(error[i]) \u0026lt; tol: isStepGood = True i += 1 return t, y, error # Define orbit parameters mu = 398600.4415 rp = 6678 e = 0.9 a = rp/(1-e) T = 2*np.pi*np.sqrt(a**3/mu) # Define simulation parameters t0 = 0 x0 = np.array([rp, 0, 0, np.sqrt(2*mu/rp - mu/a)]) t_end = T tol = 1e-12 # Numerically integrate t_rk45, y_rk45, local_error = rk45_solver(dxdt, t0, x0, t_end, tol) y_rk45 = np.array(y_rk45) # Plotting plt.subplot(2,1,1) plt.plot(y_rk45[:, 0], y_rk45[:, 1]) plt.axis(\u0026#39;equal\u0026#39;) plt.scatter(0, 0, s=50) # dot for Earth plt.title(\u0026#34;Satellite Orbit Around Earth\u0026#34;, fontsize=20) plt.ylabel(\u0026#39;y [km]\u0026#39;, fontsize=20) plt.xlabel(\u0026#39;x [km]\u0026#39;, fontsize=20) plt.subplot(2,1,2) plt.plot(t_rk45, np.sqrt(y_rk45[:, 2]**2 + y_rk45[:, 3]**2), color=\u0026#39;C1\u0026#39;) plt.title(\u0026#34;Satellite Velocity\u0026#34;, fontsize=20) plt.ylabel(\u0026#39;velocity [km/s]\u0026#39;, fontsize=20) plt.xlabel(\u0026#39;t [s]\u0026#39;, fontsize=20) plt.show() The adaptive step size is able to deal with velocity changing by orders of magnitude. With a constant step size, we would be wasting a huge amount of computation on the far side of the orbit. The accuracy of the RK45 method also allows us to use very small tolerances (1e-12) and still have the computation time be relatively quick.\nConclusion This guide should serve as a single comprehensive reference for what the RK45 method is, how it is derived, and how and why we use it to solve engineering problems. There is still room to improve on the ideas and code that have been shown here (the code lacks a lot of polish and dealing with edge cases for example).\nHowever, for those like me who wanted to understand where this method comes from and why it is so good, I hope this has been a useful reference. The goal was to make these quite dense mathematical concepts as clear as possible for other engineers like me.\nReferences [1] Roson J. S., \u0026ldquo;The Runge-Kutta Equations by Quadrature Methods\u0026rdquo;, 1967, NASA.\n[2] \u0026ldquo;Runge-Kutta Methods\u0026rdquo;, 10.001: Numerical Solution of Ordinary Differential Equations\n[3] Explanation and proof of the 4th order Runge-Kutta method\n[4] Numerical Recipes in C: The Art of Scientific Computing\n[5] S. Brorson, \u0026ldquo;Numerically Solving Ordinary Differential Equations\u0026rdquo;\n[6] Adaptive Runge-Kutta Methods | Lecture 54 | Numerical Methods for Engineers\n[7] Dormand, J. R. and P. J. Prince, A family of embedded Runge-Kutta formulae, J. Comp. Appl. Math., Vol. 6, 1980, pp. 1926.\n[8] Dormand-Prince Method\n","permalink":"http://localhost:1313/blog/posts/rk45/rk45/","summary":"The Initial Value Problem In engineering, we often encounter systems that evolve over time, such as circuits, mechanical systems, or chemical reactions. These systems are best described using differential equations. For example, Newton\u0026rsquo;s law of cooling states:\n$$ \\dfrac{dT}{dt} = -k(T - T_{surr}) $$\nwhere T is the temperature of some point, k is a proportionality constant, and Tsurr is the temperature surrounding the point of interest.\nTo solve this equation is to find a solution for the temperature, T, over time.","title":"An Engineer's Guide to the Runge-Kutta (RK45) Method"}]