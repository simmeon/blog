[{"content":"\rThe Initial Value Problem In engineering, we often encounter systems that evolve over time, such as circuits, mechanical systems, or chemical reactions. These systems are best described using differential equations. For example, Newton\u0026rsquo;s law of cooling states:\n$$ \\dfrac{dT}{dt} = -k(T - T_{surr}) $$\nwhere T is the temperature of some point, k is a proportionality constant, and Tsurr is the temperature surrounding the point of interest.\nTo solve this equation is to find a solution for the temperature, T, over time. In this case, this is fairly straightforward and we can come up with an analytical solution of the form:\n$$ T(t) = T_{surr} + (T(0) - T_{surr})e^{-kt} $$\nNotice that the solution depends on the initial temperature, as Figure 1 shows. We need a point of reference to define the solution that is relevant to our system.\nFigure 1: Temperature of a point over time with different initial temperatures. Tsurr = 20 C, k = 1.\nFigure 1 code\r1\u0026#39;\u0026#39;\u0026#39; 2Plotting the temperature of a point with different starting temperatures. 3 4Created by: simmeon 5Last Modified: 28/04/24 6License: MIT 7 8\u0026#39;\u0026#39;\u0026#39; 9 10import matplotlib.pyplot as plt 11import numpy as np 12 13# Define system parameters 14T_surr = 20 # surrounding temperature of 20 C 15k = 1 # proportionality constant of 1 for simplicity 16 17# Define our cooling function 18def solve_T(t, T0): 19 return T_surr + (T0 - T_surr)*np.exp(-k*t) 20 21# Create an array of times from 0-5 seconds 22t = np.arange(0, 5, 0.1) 23 24# Solve and plot the solutions for different initial values of T 25T0_choices = [30, 25, 20, 15, 10] 26 27plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) 28 29for T0 in T0_choices: 30 T = solve_T(t, T0) 31 plt.plot(t, T, label=\u0026#39;T0 = {} C\u0026#39;.format(T0)) 32 33plt.title(\u0026#39;Temperature over time\u0026#39;, fontsize=20) 34plt.xlabel(\u0026#39;Time [s]\u0026#39;, fontsize=20) 35plt.ylabel(\u0026#39;Temperature [C]\u0026#39;, fontsize=20) 36plt.xlim(0, 5) 37plt.ylim(10, 30) 38plt.grid(alpha=0.7) 39plt.legend() 40plt.show() The initial value problem deals with solving these ordinary differential equations where we have an initial state of the system, eg. the initial temperature is 25 C. However, often these systems are hard or impossible to solve analytically. To solve them, we use numerical methods to approximate the solution. Let\u0026rsquo;s look at how we might be able to do that\u0026hellip;\nNumerical Methods Let\u0026rsquo;s take the previous cooling equation,\n$$ \\dfrac{dT}{dt} = -k(T - T_{surr}) $$\nand assume we can\u0026rsquo;t find an analytical solution. Let\u0026rsquo;s consider the information we do have that could help us approximate the solution. We have an expression for the derivative that we can calculate at any time, t, assuming we know the current temperature at that time. We know the temperature at some initial time (t = 0 in this case), so we can calculate the derivative at that time. But how does this help us find the temperature at other times?\nThe Euler Method From calculus we know that we get the derivative of a function by taking two points on the function and approximating the derivative as if it were a striaght line. As the distance, h, between the two points gets closer to 0, the approximation gets better. Formally,\n$$ f^{\\prime}(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h} $$\nWe can rearrange this and assume a finite step size, h, to get\n$$ f(x+h) \\approx f(x) + h f^{\\prime}(x) $$\nwhich gives us an expression to approximate the next step of a function using only the current known function value and the function derivative. We can see what this looks like with different step sizes in Figure 2.\nFigure 2: Estimating the value of y = x2 with different step sizes.\nFigure 2 code\r1\u0026#39;\u0026#39;\u0026#39; 2Visualising how the derivative can be used to estimate the value of a function. 3 4Created by: simmeon 5Last Modified: 28/04/24 6License: MIT 7 8\u0026#39;\u0026#39;\u0026#39; 9 10import matplotlib.pyplot as plt 11import numpy as np 12 13plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) 14 15# Define some function we want to find the value of 16def func(t): 17 return t**2 18 19# Define the derivative line of the function 20def dydt(t, t0, y0): 21 m = 2 * t0 22 return m * (t-t0) + y0 23 24# Create an array of time values to plot the function 25t_func = np.arange(0.5, 2, 0.1) 26 27# Estimation parameters 28t0 = 1.0 # known initial value 29y0 = func(t0) # known initial value 30h = [1, 0.5, 0.2] # step size options 31 32# Plotting 33# Find and plot function values 34y_func = func(t_func) 35plt.plot(t_func, y_func) 36plt.plot(t0, y0, \u0026#39;x\u0026#39;, color=\u0026#39;C0\u0026#39;) 37 38# Plot the derivative line for each step size 39colours = [\u0026#39;C1\u0026#39;, \u0026#39;C2\u0026#39;, \u0026#39;C3\u0026#39;] 40for i in range(len(h)): 41 t_dydt = np.arange(t0, t0 + h[i], 0.1) 42 y_dydt = dydt(t_dydt, t0, y0) 43 plt.plot(t_dydt, y_dydt, color=colours[i], label=\u0026#39;Step = {:.1f}\u0026#39;.format(h[i])) 44 plt.plot(t_dydt[-1], y_dydt[-1], \u0026#39;o\u0026#39;, color=colours[i]) 45 46# Show plot 47plt.title(\u0026#34;Estimating a function by using the derivative\u0026#34;, fontsize=20) 48plt.legend(loc=\u0026#39;lower right\u0026#39;, fontsize=10) 49plt.show() This is Euler\u0026rsquo;s method for solving the initial value problem. We might also write it as:\n$$ f_{k+1} \\approx f_{k} + h f^{\\prime}(x_{k}) $$\nfor some index, k.\nWe can iterate through time with this method, using the newly found fk+1 as our new fk and so on. In code, that would look like the following:\n1def euler(dydt, y0, t0, t_end, h=0.1): 2 \u0026#39;\u0026#39;\u0026#39; 3 Takes the derivative function, an initial condition, 4 the time we want to integrate until, and a step size. 5 6 Returns arrays of time and y values. 7 \u0026#39;\u0026#39;\u0026#39; 8 t = np.arange(t0, t_end+h, h) 9 y = np.zeros(len(t)) 10 11 y[0] = y0 12 13 for i in range(1,len(t)): 14 y[i] = y[i-1] + h * dydt(y[i-1]) 15 16 return t, y The highlighted line shows the Euler method itself where the next value of the function is estimated.\nLet\u0026rsquo;s try do that with Newton\u0026rsquo;s cooling law and see how the result compares to the analytical solution. Figure 3 shows what that would look like.\nFigure 3: Analytical and Euler method solutions to Newton\u0026rsquo;s cooling law. Tsurr = 20 C, k = 1.\nFigure 3 code\r1\u0026#39;\u0026#39;\u0026#39; 2Performing Euler integration to solve the cooling equation. We will compare with the analytical solution. 3 4Created by: simmeon 5Last Modified: 28/04/24 6License: MIT 7 8\u0026#39;\u0026#39;\u0026#39; 9 10import matplotlib.pyplot as plt 11import numpy as np 12 13plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) 14 15# Define system 16T_surr = 20 # surrounding temperature of 20 C 17k = 1 # proportionality constant of 1 for simplicity 18 19# Define our cooling function 20def solve_T(t, T0): 21 return T_surr + (T0 - T_surr)*np.exp(-k*t) 22 23# Define the derivative (only depends on the current temp, not time) 24def dTdt(T): 25 return -k * (T - T_surr) 26 27# Define our Euler integration function 28def euler(dTdt, T0, t0, t_end, h=0.1): 29 \u0026#39;\u0026#39;\u0026#39; 30 Takes the derivative function, an initial condition, the time we want to integrate until, 31 and a step size. 32 33 Returns arrays of time and temperature values 34 \u0026#39;\u0026#39;\u0026#39; 35 t = np.arange(t0, t_end+h, h) 36 T = np.zeros(len(t)) 37 38 T[0] = T0 39 40 for i in range(1,len(t)): 41 T[i] = T[i-1] + h * dTdt(T[i-1]) 42 43 return t, T 44 45 46# Parameters 47t0 = 0 48T0 = 30 49t_end = 5 50 51# Get analytical solution 52t_analytical = np.arange(t0, t_end+0.1, 0.1) 53T_analytical = solve_T(t_analytical, T0) 54 55# Get Euler numerical solution 56t_euler_05, T_euler_05 = euler(dTdt, T0, t0, t_end, 0.5) 57t_euler_01, T_euler_01 = euler(dTdt, T0, t0, t_end, 0.1) 58 59# Plotting 60plt.plot(t_analytical, T_analytical, label=\u0026#39;Analytical\u0026#39;, linestyle=\u0026#39;--\u0026#39;) 61plt.plot(t_euler_05, T_euler_05, label=\u0026#39;Euler method, h = 0.5\u0026#39;) 62plt.plot(t_euler_01, T_euler_01, label=\u0026#39;Euler method, h = 0.1\u0026#39;) 63 64plt.title(\u0026#34;Analytical and Euler method solutions to Newton\u0026#39;s cooling law\u0026#34;, fontsize=20) 65plt.xlabel(\u0026#39;Time [s]\u0026#39;, fontsize=20) 66plt.ylabel(\u0026#39;Temperature [C]\u0026#39;, fontsize=20) 67 68plt.legend(fontsize=15) 69plt.show() It is clear that the step size plays a big role in the accuracy of the solution. We could continue to reduce the step size, but that will quickly increase the time it takes to get a solution beyond a reasonable amount. Maybe we can think of a better way of doing this\u0026hellip;\nRunge-Kutta Methods A major problem with Euler\u0026rsquo;s method is that using the derivative of the point we\u0026rsquo;re at is not very good at estimating the next value of the function (for the kind of step sizes we want to use). The derivative is constantly changing and could be vastly different at the new value. We might get a more accurate step if we use a mix of the derivative at our start point and end point.\nThis is the basis for how Runge-Kutta methods work. We find derivatives between the start and end points of each step and use a weighted average of those as the actual derivative for the step. The simplest version of this would be to just use the derivative at the start of the step \u0026ndash; which is exactly the Euler method! The Euler method is a 1st order Runge-Kutta method.\nLet\u0026rsquo;s define the method more concretely.\nDerivation of Runge-Kutta Methods We will start by examining the initial problem again, being that we want to solve the following general equation for y:\n$$ \\dfrac{dy}{dt} = f(y, t) $$\nThis could be our cooling equation from before,\n$$ \\dfrac{dT}{dt} = -k(T - T_{surr}) $$\nor something more complex like a state space defining a spring, mass, damper system:\n$$ \\bold{\\dot{x}} = \\begin{bmatrix} 0 \u0026amp; 1 \\\\ \\frac{-k}{m} \u0026amp; \\frac{-c}{m} \\end{bmatrix} \\bold{x} + \\begin{bmatrix} 0 \\\\ \\frac{1}{m} \\end{bmatrix} u(t) $$\nIn both cases, we are given the derivative of the state we want to solve for (eg. temperature) and the derivative depends on the value of the state and time. In the case of the cooling equation, the derivative only depends on the state, T, and not time.\nIdeally, we could just integrate the derivative to find the value at \\( t + h \\):\n$$ y(t+h) = y(t) + \\int_{\\tau=t}^{\\tau=t+h}{\\dfrac{dy(\\tau)}{d\\tau}} d\\tau $$\nHowever, as we said earlier, this is often hard or impossible. We can instead approximate the integral with a weighted sum.\nWeighted Sum Approximations Let\u0026rsquo;s build this up slowly. Take the following function, for example:\n$$ f(t) = t^3 - 2t^2 - t + 3 $$\nHow would we evaluate\n$$ \\int_0^2{f(t)}dt $$\nWe can do this analytically, which is like summing up tiny vertical slices of the function to find the total area underneath it. But we could think about this a different way. We can get that same area by finding the average value of the function over the interval, then multiplying by the length of the interval. You can see this in Figure 4.\nFigure 4: Comparison of areas found by integration and weighted sum approximation.\nFigure 4 code\r1\u0026#39;\u0026#39;\u0026#39; 2Making sense of weighted sums as integral approximations. 3 4Created by: simmeon 5Last Modified: 28/04/24 6License: MIT 7 8\u0026#39;\u0026#39;\u0026#39; 9 10import matplotlib.pyplot as plt 11import numpy as np 12 13plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) 14 15def f(t): 16 return t**3 -2*t**2 - t + 3 17 18# Perform a weighted sum approximation 19N = 10 # higher number of samples, N, gives a better approximation 20sum = 0 21t0 = 0 22h = 2 23nodes = np.linspace(t0, t0+h, N) 24 25for node in nodes: 26 sum += f(node) 27weighted_sum = sum / N 28 29print(\u0026#39;Weighted sum area: \u0026#39;, weighted_sum * h) 30 31# Plotting 32t = np.arange(t0, t0 + h, 0.01) 33y = f(t) 34 35# Integration result 36plt.subplot(1,2,1) 37plt.plot(t,y) 38plt.fill_between(t, y, alpha=0.3) 39plt.annotate(\u0026#39;Area = 2.67\u0026#39;, (1,2.5), fontsize=20) 40plt.title(\u0026#39;Area from integration\u0026#39;, fontsize=20) 41 42# Weighted sum result 43plt.subplot(1,2,2) 44plt.plot(t,y) 45plt.fill_between(t, weighted_sum, alpha=0.3) 46plt.annotate(\u0026#39;Area = {:.2f}\u0026#39;.format(weighted_sum * h), (1,2.5), fontsize=20) 47plt.title(\u0026#39;Area from weighted sum\u0026#39;, fontsize=20) 48 49plt.show() To put this idea into an equation, we can say:\n$$ \\int_0^2{f(t)}dt \\approx \\frac{2}{\\sum{w_i}} \\sum_{i=1}^N{w_i f(t_i)} $$\nWhere \\( N \\) is the number of points we sample from the function to find the average, \\( w_i \\) is the weighting we give to each point, and \\( t_i \\) is some point inside the integration bounds of \\( [0, 2] \\). We can simplify this by giving every point a weighting of 1:\n$$ \\int_0^2{f(t)}dt \\approx \\frac{2}{N} \\sum_{i=1}^N{f(t_i)} $$\nThe more points we sample, the closer the average will be to the actual average of the function and therefore the actual integral.\nWe can extend this concept to our derivative from earlier:\n$$ \\int_{\\tau=t}^{\\tau=t+h}{\\dfrac{dy(\\tau)}{d\\tau}} d\\tau \\approx \\frac{h}{\\sum{w_i}} \\sum_{i=1}^N{w_i y^{\\prime}(t+v_i h, y(t+v_i h))} $$\nWhat a mess.\nWe\u0026rsquo;re still using the same concept of a weighted sum to find the average value of the derivative function, but since the derivative depends on \\( t \\) and \\( y \\) it\u0026rsquo;s a bit more complicated. We introduced \\( v_i \\) to help define which values of the function we sample, also called nodes. This can range from 0 to 1 to cover the interval from \\( t \\) to \\( t+h \\). Of course, whatever time we evaluate our derivative at must be the same time we use to get the \\( y \\) values for the derivative.\nFor the sake of simplicity let\u0026rsquo;s say that all our weights will sum up to 1. We are not, however, going to assume that all our weights will be the same value as we did previously.\nConsidering all this, the equation we are trying to solve is now:\n$$ y(t+h) = y(t) + h \\sum_{i=1}^N{w_i y^{\\prime}(t+v_i h, y(t+v_i h))} $$\nThe Runge-Kutta Family You might have noticed there is a problem with the sum we just defined. In particular, we don\u0026rsquo;t know what \\( y(t+v_i h) \\) is. In fact, finding this is basically the point of the whole method. So how do we deal with this?\nSimply, we are going to make worse approximations of \\( y(t+v_i h) \\) so that we can make a much better approximation of \\( y(t + h) \\). Again, these \\( y(t+v_i h) \\) values are used to find values of the derivative that we will then average. Let\u0026rsquo;s define what we will do to find these approximations.\nWe will start by saying that \\( v_1 = 0 \\). This means the first term in the sum will be\n$$ w_1 y^{\\prime}(t, y(t)) $$\nIf this was the only term in our sum (so \\( w_1 = 1 \\) ), then the equation we would be solving would be:\n$$ y(t+h) \\approx y(t) + h y^{\\prime}(t, y(t)) $$\nThis should look familiar, it\u0026rsquo;s the Euler method! This is what we mean by the Euler method is a 1st order Runge-Kutta method \u0026ndash; because it uses one term in the weighted sum. Or in other words, it is a linear approximation.\nFor simplicity, we will write this first sum term as:\n$$ k_1 = y^{\\prime}(t, y(t)) h $$\n\\( k_1 \\) is a 1st order estimate of the change in \\( y \\) between \\( y(t) \\) and \\( y(t+h) \\).\nThe second term gets trickier as we have to somehow estimate \\( y(t+v_2 h) \\). Conveniently, we have just found an estimate for how \\( y \\) changes: \\( k_1 \\). So we can utilise some fraction of this change to create our estimate for the second weighted sum term where:\n$$ y^{\\prime}(t + \\alpha_2 h, y(t) + \\beta_{2,1} k_1) $$\nWe don\u0026rsquo;t know yet how much of \\( k_1 \\) we should add, we will figure that out later. Same with what time we should sample at.\nWe have changed notation slightly to help set up higher order Runge-Kutta methods. Instead of \\( v_i \\) we are now using \\( \\alpha_i \\) to tell us about what time we are sampling at. And since our \\( y \\) value is no longer defined in simple terms of time, we are going to use \\( \\beta_{i,j} \\) to describe how much of previous estimates we will add to the estimate for the new term.\nWe define\n$$ k_2 = y^{\\prime}(t + \\alpha_2 h, y(t) + \\beta_{2,1} k_1) h $$\nso that the equation we are solving is now:\n$$ y(t+h) \\approx y(t) + w_1 k_1 + w_2 k_2 $$\nWith that, we have defined the 2nd order Runge-Kutta Method!\nWe can continue the weighted sum with a third and fourth term, following similar logic of using the previous estimates to inform the new value of \\( y(t + v_i h) \\). This will give us:\n$$ k_3 = y^{\\prime}(t + \\alpha_3 h, y(t) + \\beta_{3,1} k_1 + \\beta_{3,2} k_2) h \\\\ k_4 = y^{\\prime}(t + \\alpha_4 h, y(t) + \\beta_{4,1} k_1 + \\beta_{4,2} k_2 + \\beta_{4,3} k_3) h $$\nThis gives the 4th order Runge-Kutta method:\n$$ y(t+h) \\approx y(t) + w_1 k_1 + w_2 k_2 + w_3 k_3 + w_4 k_4 $$\nNow to actually solve these higher order methods, we need to define these coefficients\u0026hellip;\nDefining Coefficients We need to define all the \\( \\alpha_i \\), \\( \\beta_{i,j} \\), and \\( w_i \\) coefficients to be able to use these methods. We can do this by comparing the Taylor series expansion of our approximation to the Taylor series expansion of \\( y(t+h) \\) and equating coefficients. Let\u0026rsquo;s do this for the 2nd order method:\n$$ y(t+h) \\approx y(t) + w_1 k_1 + w_2 k_2 $$\nAs this is a 2nd order method, we will need to find the 2nd order expansions of the left and right sides.\nLet\u0026rsquo;s start with the left. The 2nd order Taylor series expansion of \\( y(t+h) \\) about \\( t \\) is:\n$$ y(t+h) \\approx y(t) + h \\dfrac{dy}{dt} \\Big| _{t,y} + \\frac{h^2}{2} \\dfrac{d^2y}{dt^2} \\Big| _{t,y} + O(h^3) $$\nWe can write our derivative:\n$$ \\dfrac{dy}{dt} = f(t, y) \\\\ {} \\\\ \\dfrac{d^2y}{dt^2} = \\dfrac{df(t,y)}{dt} = \\dfrac{\\partial f}{\\partial t} + \\dfrac{\\partial f}{\\partial y} \\dfrac{dy}{dt} = \\dfrac{\\partial f}{\\partial t} + f \\dfrac{\\partial f}{\\partial y} $$\nThis makes our left hand side (using slightly different notation):\n$$ y_{n+1} \\approx y_n + h f(t_n, y_n) + \\frac{h^2}{2} \\bigg(\\dfrac{\\partial f}{\\partial t} + f \\dfrac{\\partial f}{\\partial y}\\bigg) \\bigg|_{t_n, y_n} + O(h^3) $$\nGreat! Let\u0026rsquo;s do the right hand side now. We can write the right hand same with the same notation as above:\n$$ y_n + w_1 k_{1,n} + w_2 k_{2,n} $$\n\\( k_2 \\) is a bit tricky to expand. Our \\( k_2 \\) term is made up mostly of a function with the form \\( f(t + \\Delta t, y + \\Delta y) \\), which will need to be expanded. In general, the 2nd order expansion looks like:\n$$ f(t + \\Delta t, y + \\Delta y) = f(t, y) + \\Delta t \\dfrac{\\partial f}{\\partial t}\\bigg| _{t, y} + \\Delta y \\dfrac{\\partial f}{\\partial y} \\bigg| _{t, y} + O(h^3) $$\nApplying this to \\( k_2 \\) gives:\n$$ k_{2,n} = h f(t + \\alpha_2 h, y_n + \\beta_{2,1} k_1) \\approx h \\bigg( f(t_n, y_n) + \\alpha _2 h \\dfrac{\\partial f}{\\partial t} \\bigg| _{t_n, y_n} + \\beta _{2,1} k_1 \\dfrac{\\partial f}{\\partial y} \\bigg| _{t_n, y_n} \\bigg) $$\nAll together, the right hand side is then\n$$ y_n + w_1 k_{1,n} + w_2 k_{2,n} \\approx y_n + w_1 h f(t_n, y_n) + w_2 h \\bigg( f(t_n, y_n) + \\alpha _2 h \\dfrac{\\partial f}{\\partial t} \\bigg| _{t_n, y_n} + \\beta _{2,1} k_1 \\dfrac{\\partial f}{\\partial y} \\bigg| _{t_n, y_n} \\bigg) + O(h^3) $$\nwhich we can rearrange to be in the same form (substituting in for \\( k_1 \\)) as the left hand side:\n$$ y_n + w_1 k_{1,n} + w_2 k_{2,n} \\approx y_n + (w_1 + w_2) h f(t_n, y_n) + \\frac{h^2}{2} \\bigg(2 w_2 \\alpha_2 \\dfrac{\\partial f}{\\partial t} + 2 w_2 \\beta _{2,1} f \\dfrac{\\partial f}{\\partial y} \\bigg) \\bigg| _{t_n, y_n} + O(h^3) $$\nFinally, we have both expansions and can equate the coefficients:\n$$ y(t+h) \\approx y(t) + w_1 k_1 + w_2 k_2 \\\\ {} \\\\ \\big\\downarrow \\\\ {} \\\\ y_n + h f(t_n, y_n) + \\frac{h^2}{2} \\bigg(\\dfrac{\\partial f}{\\partial t} + f \\dfrac{\\partial f}{\\partial y}\\bigg) \\bigg|_{t_n, y_n} + O(h^3) \\\\ {} \\\\ = \\\\ {} \\\\ y_n + (w_1 + w_2) h f(t_n, y_n) + \\frac{h^2}{2} \\bigg(2 w_2 \\alpha_2 \\dfrac{\\partial f}{\\partial t} + 2 w_2 \\beta _{2,1} f \\dfrac{\\partial f}{\\partial y} \\bigg) \\bigg| _{t_n, y_n} + O(h^3) $$\nFrom this, we can see that our coefficients must satisfy the following equations:\n$$ w_1 + w_2 = 1 \\\\ {} \\\\ w_2 \\alpha_2= \\frac{1}{2} \\\\ {} \\\\ w_2 \\beta_{2,1} = \\frac{1}{2} $$\nWith 3 equations and 4 unknows, there are infinitely many solutions. However, the standard choices are:\n$$ \\alpha_2 = \\beta_{2,1} = 1 \\\\ {} \\\\ w_1 = w_2 = \\frac{1}{2} $$\nThis brings us finally to the complete 2nd order Runge-Kutta Method:\n$$ k_1 = h y^\\prime (t_n, y_n) \\\\ {} \\\\ k_2 = h y^\\prime (t_n + h, y_n + k_1) \\\\ {} \\\\ y_{n+1} = y_n + \\frac{1}{2} k_1 + \\frac{1}{2} k_2 $$\nA similar (messier) method of expansions can be used to for higher order methods. The standard terms for the 4th order Runge-Kutta method are:\n$$ k_1 = h y^\\prime (t_n, y_n) \\\\ {} \\\\ k_2 = h y^\\prime (t_n + \\frac{h}{2}, y_n + \\frac{k_1}{2}) \\\\ {} \\\\ k_3 = h y^\\prime (t_n + \\frac{h}{2}, y_n + \\frac{k_2}{2}) \\\\ {} \\\\ k_4 = h y^\\prime (t_n + h, y_n + k_3) \\\\ {} \\\\ y_{n+1} = y_n + \\frac{1}{6} (k_1 + 2 k_2 + 2 k_3 + k_4) $$\nKeep in mind that there are infinitely many choices of these coefficients and lots of research has gone into figuring out which ones work best. We will stick to these simple standard ones for now.\nImplementing Runge-Kutta Now that we have finally derived the Runge-Kutta methods, let\u0026rsquo;s implement them in code. We will do both the 2nd order and 4th order methods and compare their accuracy.\n1def rk2(dydt, t0, y0, t_end, h): 2 \u0026#34;\u0026#34;\u0026#34; 3 Solves a first-order ODE using the 2nd order Runge-Kutta method 4 5 Args: 6 dydt: The derivative function to integrate 7 t0: Initial value of time 8 y0: Initial value of y 9 t_end: Final time for integration 10 h: Step size 11 12 Returns: 13 An array of y values for each time step 14 \u0026#34;\u0026#34;\u0026#34; 15 t = np.arange(t0, t_end+h, h) # Array of time points 16 y = np.zeros_like(t) 17 y[0] = y0 18 19 for i in range(1, len(t)): 20 k1 = h * dydt(t[i - 1], y[i - 1]) 21 k2 = h * dydt(t[i - 1] + h, y[i - 1] + k1) 22 23 y[i] = y[i - 1] + (k1 + k2) / 2 24 25 return y We can see how, at each time step, we calculate the \\( k_i \\) values and use these to estimate the next value of the function.\n1def rk4(dydt, t0, y0, t_end, h): 2 \u0026#34;\u0026#34;\u0026#34; 3 Solves a first-order ODE using the 4th order Runge-Kutta method 4 5 Args: 6 dydt: The derivative function to integrate 7 t0: Initial value of time 8 y0: Initial value of y 9 t_end: Final time for integration 10 h: Step size 11 12 Returns: 13 An array of y values for each time step 14 \u0026#34;\u0026#34;\u0026#34; 15 t = np.arange(t0, t_end+h, h) 16 y = np.zeros_like(t) 17 y[0] = y0 18 19 for i in range(1, len(t)): 20 k1 = h * dydt(t[i - 1], y[i - 1]) 21 k2 = h * dydt(t[i - 1] + h / 2, y[i - 1] + k1 / 2) 22 k3 = h * dydt(t[i - 1] + h / 2, y[i - 1] + k2 / 2) 23 k4 = h * dydt(t[i], y[i - 1] + k3) 24 25 y[i] = y[i - 1] + (k1 + 2 * k2 + 2 * k3 + k4) / 6 26 27 return y After all that derivation, the actual method is remarkably clean and simple to implement. Figure 5 gives an idea of how these higher order functions perform compared to our original Euler method.\nFigure 5: Comparing the accuracy of different order ODE solvers, h = 0.1\nAll the solvers are using the same step size here. As we can see, the 4th order method is better than the 2nd order method. They are both much more accurate than the 1st order Euler method.\nAs a final comparison, let\u0026rsquo;s look at Newton\u0026rsquo;s law of cooling one last time. Figure 6 shows how our 4th order solver compares to our previous test with the Euler method.\nFigure 6: Comparing the accuracy of the RK4 and Euler methods on the cooling equation. Tsurr = 20 C, k = 1, h = 0.5\nFigure 5 and 6 code\r1\u0026#39;\u0026#39;\u0026#39; 2Implementing and comparing the Runge-Kutta 2nd and 4th order methods. 3We can also compare to the Euler method (1st order). 4 5Created by: simmeon 6Last Modified: 29/04/24 7License: MIT 8 9\u0026#39;\u0026#39;\u0026#39; 10 11import matplotlib.pyplot as plt 12import numpy as np 13 14plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) 15 16# Define some functions to test our solvers with 17def dydt(t, y): 18 return 3 * t**2 + 20 * np.cos(10 * t) 19 20def analytical_solution(t): 21 return t**3 + 2 * np.sin(10 * t) # Analytical solution to the ODE 22 23# # Define our cooling function 24# def solve_T(t, T0): 25# return T_surr + (T0 - T_surr)*np.exp(-k*t) 26 27# # Define the derivative (only depends on the current temp, not time) 28# def dTdt(T): 29# return -k * (T - T_surr) 30 31def euler_solver(dydt, t0, y0, t_end, h): 32 \u0026#34;\u0026#34;\u0026#34; 33 Solves a first-order ODE using the Euler method 34 35 Args: 36 dydt: The differential equation (dy/dt) as a Python function 37 t0: Initial value of time 38 y0: Initial value of y 39 t_end: Final time for integration 40 h: Step size 41 42 Returns: 43 An array of y values for each time step 44 \u0026#34;\u0026#34;\u0026#34; 45 t = np.arange(t0, t_end+h, h) 46 y = np.zeros_like(t) 47 y[0] = y0 48 49 for i in range(1, len(t)): 50 y[i] = y[i - 1] + h * dydt(t[i - 1], y[i - 1]) 51 52 return y 53 54def rk2(dydt, t0, y0, t_end, h): 55 \u0026#34;\u0026#34;\u0026#34; 56 Solves a first-order ODE using the 2nd order Runge-Kutta method 57 58 Args: 59 dydt: The derivative function to integrate 60 t0: Initial value of time 61 y0: Initial value of y 62 t_end: Final time for integration 63 h: Step size 64 65 Returns: 66 An array of y values for each time step 67 \u0026#34;\u0026#34;\u0026#34; 68 t = np.arange(t0, t_end+h, h) # Array of time points 69 y = np.zeros_like(t) 70 y[0] = y0 71 72 for i in range(1, len(t)): 73 k1 = h * dydt(t[i - 1], y[i - 1]) 74 k2 = h * dydt(t[i - 1] + h, y[i - 1] + k1) 75 y[i] = y[i - 1] + (k1 + k2) / 2 76 77 return y 78 79def rk4(dydt, t0, y0, t_end, h): 80 \u0026#34;\u0026#34;\u0026#34; 81 Solves a first-order ODE using the 4th order Runge-Kutta method 82 83 Args: 84 dydt: The derivative function to integrate 85 t0: Initial value of time 86 y0: Initial value of y 87 t_end: Final time for integration 88 h: Step size 89 90 Returns: 91 An array of y values for each time step 92 \u0026#34;\u0026#34;\u0026#34; 93 t = np.arange(t0, t_end+h, h) 94 y = np.zeros_like(t) 95 y[0] = y0 96 97 for i in range(1, len(t)): 98 k1 = dydt(t[i - 1], y[i - 1]) 99 k2 = dydt(t[i - 1] + h / 2, y[i - 1] + k1 / 2) 100 k3 = dydt(t[i - 1] + h / 2, y[i - 1] + k2 / 2) 101 k4 = dydt(t[i], y[i - 1] + k3) 102 103 y[i] = y[i - 1] + (k1 + 2 * k2 + 2 * k3 + k4) / 6 * h 104 105 return y 106 107# Define simulation parameters 108t0 = 0 109y0 = 0 110t_end = 5 111h = 0.1 112 113# Solve using both methods 114y_euler = euler_solver(dydt, t0, y0, t_end, h) 115y_rk2 = rk2(dydt, t0, y0, t_end, h) 116y_rk4 = rk4(dydt, t0, y0, t_end, h) 117 118# Get analytical solution 119t = np.arange(t0, t_end+h, h) # Array of time points 120y_analytical = analytical_solution(t) 121 122# Calculate errors 123error_euler = np.abs(y_euler - y_analytical) 124error_rk2 = np.abs(y_rk2 - y_analytical) 125error_rk4 = np.abs(y_rk4 - y_analytical) 126 127# Plot the numerical solutions 128plt.subplot(1,2,1) 129plt.plot(t, y_euler, label=\u0026#39;Euler\u0026#39;) 130plt.plot(t, y_rk2, label=\u0026#39;RK2\u0026#39;) 131plt.plot(t, y_rk4, label=\u0026#39;RK4\u0026#39;) 132plt.plot(t, y_analytical, label=\u0026#39;Analytical\u0026#39;, linestyle=\u0026#39;--\u0026#39;) 133plt.xlabel(\u0026#39;Time (t)\u0026#39;, fontsize=20) 134plt.ylabel(\u0026#39;y(t)\u0026#39;, fontsize=20) 135plt.title(\u0026#39;Numerical Solutions\u0026#39;, fontsize=20) 136plt.legend(fontsize=15) 137 138# Plot the errors 139plt.subplot(1,2,2) 140plt.plot(t, error_euler, label=\u0026#39;Euler\u0026#39;) 141plt.plot(t, error_rk2, label=\u0026#39;RK2\u0026#39;) 142plt.plot(t, error_rk4, label=\u0026#39;RK4\u0026#39;) 143plt.xlabel(\u0026#39;Time (t)\u0026#39;, fontsize=20) 144plt.ylabel(\u0026#39;Abs Error\u0026#39;, fontsize=20) 145plt.title(\u0026#39;Absolute Errors of the Numerical Solutions\u0026#39;, fontsize=20) 146plt.legend(fontsize=15) 147 148plt.show() Even with a relatively large step size, the 4th order method is still much better than the Euler method. In fact, we could make the step size even bigger and still have a solution that fell within some tiny tolerance.\nBut how can we choose a good value to make this step size? And better yet, could we even possibly change the step size during our integration to make sure our result stayed within some tolerance?\nAdaptive Step Sizing \u0026hellip;to be continued\u0026hellip;\nReferences [1] The Runge-Kutta Equations by Quadrature Methods. Roson J. S., 1967, NASA.\n[2] Runge-Kutta Methods. 10.001: Numerical Solution of Ordinary Differential Equations, MIT\n[3] Explanation and proof of the 4th order Runge-Kutta method\n","permalink":"http://localhost:1313/blog/posts/rk45/rk45/","summary":"The Initial Value Problem In engineering, we often encounter systems that evolve over time, such as circuits, mechanical systems, or chemical reactions. These systems are best described using differential equations. For example, Newton\u0026rsquo;s law of cooling states:\n$$ \\dfrac{dT}{dt} = -k(T - T_{surr}) $$\nwhere T is the temperature of some point, k is a proportionality constant, and Tsurr is the temperature surrounding the point of interest.\nTo solve this equation is to find a solution for the temperature, T, over time.","title":"An Engineer's Guide to the Runge-Kutta (RK45) Method"}]