[{"content":"\rThe Initial Value Problem In engineering, we often encounter systems that evolve over time, such as circuits, mechanical systems, or chemical reactions. These systems are best described using differential equations. For example, Newton\u0026rsquo;s law of cooling states:\n$$ \\dfrac{dT}{dt} = -k(T - T_{surr}) $$\nwhere T is the temperature of some point, k is a proportionality constant, and Tsurr is the temperature surrounding the point of interest.\nTo solve this equation is to find a solution for the temperature, T, over time. In this case, this is fairly straightforward and we can come up with an analytical solution of the form:\n$$ T(t) = T_{surr} + (T(0) - T_{surr})e^{-kt} $$\nNotice that the solution depends on the initial temperature, as Figure 1 shows. We need a point of reference to define the solution that is relevant to our system.\nFigure 1: Temperature of a point over time with different initial temperatures. Tsurr = 20 C, k = 1.\nFigure 1 code\r1\u0026#39;\u0026#39;\u0026#39; 2Plotting the temperature of a point with different starting temperatures. 3 4Created by: simmeon 5Last Modified: 28/04/24 6License: MIT 7 8\u0026#39;\u0026#39;\u0026#39; 9 10import matplotlib.pyplot as plt 11import numpy as np 12 13# Define system parameters 14T_surr = 20 # surrounding temperature of 20 C 15k = 1 # proportionality constant of 1 for simplicity 16 17# Define our cooling function 18def solve_T(t, T0): 19 return T_surr + (T0 - T_surr)*np.exp(-k*t) 20 21# Create an array of times from 0-5 seconds 22t = np.arange(0, 5, 0.1) 23 24# Solve and plot the solutions for different initial values of T 25T0_choices = [30, 25, 20, 15, 10] 26 27plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) 28 29for T0 in T0_choices: 30 T = solve_T(t, T0) 31 plt.plot(t, T, label=\u0026#39;T0 = {} C\u0026#39;.format(T0)) 32 33plt.title(\u0026#39;Temperature over time\u0026#39;, fontsize=20) 34plt.xlabel(\u0026#39;Time [s]\u0026#39;, fontsize=20) 35plt.ylabel(\u0026#39;Temperature [C]\u0026#39;, fontsize=20) 36plt.xlim(0, 5) 37plt.ylim(10, 30) 38plt.grid(alpha=0.7) 39plt.legend() 40plt.show() The initial value problem deals with solving these ordinary differential equations where we have an initial state of the system, eg. the initial temperature is 25 C. However, often these systems are hard or impossible to solve analytically. To solve them, we use numerical methods to approximate the solution. Let\u0026rsquo;s look at how we might be able to do that\u0026hellip;\nNumerical Methods Let\u0026rsquo;s take the previous cooling equation,\n$$ \\dfrac{dT}{dt} = -k(T - T_{surr}) $$\nand assume we can\u0026rsquo;t find an analytical solution. Let\u0026rsquo;s consider the information we do have that could help us approximate the solution. We have an expression for the derivative that we can calculate at any time, t, assuming we know the current temperature at that time. We know the temperature at some initial time (t = 0 in this case), so we can calculate the derivative at that time. But how does this help us find the temperature at other times?\nThe Euler Method From calculus we know that we get the derivative of a function by taking two points on the function and approximating the derivative as if it were a striaght line. As the distance, h, between the two points gets closer to 0, the approximation gets better. Formally,\n$$ f^{\\prime}(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h} $$\nWe can rearrange this and assume a finite step size, h, to get\n$$ f(x+h) \\approx f(x) + h f^{\\prime}(x) $$\nwhich gives us an expression to approximate the next step of a function using only the current known function value and the function derivative. We can see what this looks like with different step sizes in Figure 2.\nFigure 2: Estimating the value of y = x2 with different step sizes.\nFigure 2 code\r1\u0026#39;\u0026#39;\u0026#39; 2Visualising how the derivative can be used to estimate the value of a function. 3 4Created by: simmeon 5Last Modified: 28/04/24 6License: MIT 7 8\u0026#39;\u0026#39;\u0026#39; 9 10import matplotlib.pyplot as plt 11import numpy as np 12 13plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) 14 15# Define some function we want to find the value of 16def func(t): 17 return t**2 18 19# Define the derivative line of the function 20def dydt(t, t0, y0): 21 m = 2 * t0 22 return m * (t-t0) + y0 23 24# Create an array of time values to plot the function 25t_func = np.arange(0.5, 2, 0.1) 26 27# Estimation parameters 28t0 = 1.0 # known initial value 29y0 = func(t0) # known initial value 30h = [1, 0.5, 0.2] # step size options 31 32# Plotting 33# Find and plot function values 34y_func = func(t_func) 35plt.plot(t_func, y_func) 36plt.plot(t0, y0, \u0026#39;x\u0026#39;, color=\u0026#39;C0\u0026#39;) 37 38# Plot the derivative line for each step size 39colours = [\u0026#39;C1\u0026#39;, \u0026#39;C2\u0026#39;, \u0026#39;C3\u0026#39;] 40for i in range(len(h)): 41 t_dydt = np.arange(t0, t0 + h[i], 0.1) 42 y_dydt = dydt(t_dydt, t0, y0) 43 plt.plot(t_dydt, y_dydt, color=colours[i], label=\u0026#39;Step = {:.1f}\u0026#39;.format(h[i])) 44 plt.plot(t_dydt[-1], y_dydt[-1], \u0026#39;o\u0026#39;, color=colours[i]) 45 46# Show plot 47plt.title(\u0026#34;Estimating a function by using the derivative\u0026#34;, fontsize=20) 48plt.legend(loc=\u0026#39;lower right\u0026#39;, fontsize=10) 49plt.show() This is Euler\u0026rsquo;s method for solving the initial value problem. We might also write it as:\n$$ f_{k+1} \\approx f_{k} + h f^{\\prime}(x_{k}) $$\nfor some index, k.\nWe can iterate through time with this method, using the newly found fk+1 as our new fk and so on. In code, that would look like the following:\n1def euler(dydt, y0, t0, t_end, h=0.1): 2 \u0026#39;\u0026#39;\u0026#39; 3 Takes the derivative function, an initial condition, 4 the time we want to integrate until, and a step size. 5 6 Returns arrays of time and y values. 7 \u0026#39;\u0026#39;\u0026#39; 8 t = np.arange(t0, t_end+h, h) 9 y = np.zeros(len(t)) 10 11 y[0] = y0 12 13 for i in range(1,len(t)): 14 y[i] = y[i-1] + h * dydt(y[i-1]) 15 16 return t, y The highlighted line shows the Euler method itself where the next value of the function is estimated.\nLet\u0026rsquo;s try do that with Newton\u0026rsquo;s cooling law and see how the result compares to the analytical solution. Figure 3 shows what that would look like.\nFigure 3: Analytical and Euler method solutions to Newton\u0026rsquo;s cooling law. Tsurr = 20 C, k = 1.\nFigure 3 code\r1\u0026#39;\u0026#39;\u0026#39; 2Performing Euler integration to solve the cooling equation. We will compare with the analytical solution. 3 4Created by: simmeon 5Last Modified: 28/04/24 6License: MIT 7 8\u0026#39;\u0026#39;\u0026#39; 9 10import matplotlib.pyplot as plt 11import numpy as np 12 13plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) 14 15# Define system 16T_surr = 20 # surrounding temperature of 20 C 17k = 1 # proportionality constant of 1 for simplicity 18 19# Define our cooling function 20def solve_T(t, T0): 21 return T_surr + (T0 - T_surr)*np.exp(-k*t) 22 23# Define the derivative (only depends on the current temp, not time) 24def dTdt(T): 25 return -k * (T - T_surr) 26 27# Define our Euler integration function 28def euler(dTdt, T0, t0, t_end, h=0.1): 29 \u0026#39;\u0026#39;\u0026#39; 30 Takes the derivative function, an initial condition, the time we want to integrate until, 31 and a step size. 32 33 Returns arrays of time and temperature values 34 \u0026#39;\u0026#39;\u0026#39; 35 t = np.arange(t0, t_end+h, h) 36 T = np.zeros(len(t)) 37 38 T[0] = T0 39 40 for i in range(1,len(t)): 41 T[i] = T[i-1] + h * dTdt(T[i-1]) 42 43 return t, T 44 45 46# Parameters 47t0 = 0 48T0 = 30 49t_end = 5 50 51# Get analytical solution 52t_analytical = np.arange(t0, t_end+0.1, 0.1) 53T_analytical = solve_T(t_analytical, T0) 54 55# Get Euler numerical solution 56t_euler_05, T_euler_05 = euler(dTdt, T0, t0, t_end, 0.5) 57t_euler_01, T_euler_01 = euler(dTdt, T0, t0, t_end, 0.1) 58 59# Plotting 60plt.plot(t_analytical, T_analytical, label=\u0026#39;Analytical\u0026#39;, linestyle=\u0026#39;--\u0026#39;) 61plt.plot(t_euler_05, T_euler_05, label=\u0026#39;Euler method, h = 0.5\u0026#39;) 62plt.plot(t_euler_01, T_euler_01, label=\u0026#39;Euler method, h = 0.1\u0026#39;) 63 64plt.title(\u0026#34;Analytical and Euler method solutions to Newton\u0026#39;s cooling law\u0026#34;, fontsize=20) 65plt.xlabel(\u0026#39;Time [s]\u0026#39;, fontsize=20) 66plt.ylabel(\u0026#39;Temperature [C]\u0026#39;, fontsize=20) 67 68plt.legend(fontsize=15) 69plt.show() It is clear that the step size plays a big role in the accuracy of the solution. We could continue to reduce the step size, but that will quickly increase the time it takes to get a solution beyond a reasonable amount. Maybe we can think of a better way of doing this\u0026hellip;\nRunge-Kutta Methods A major problem with Euler\u0026rsquo;s method is that using the derivative of the point we\u0026rsquo;re at is not very good at estimating the next value of the function (for the kind of step sizes we want to use). The derivative is constantly changing and could be vastly different at the new value. We might get a more accurate step if we use a mix of the derivative at our start point and end point.\nThis is the basis for how Runge-Kutta methods work. We find derivatives between the start and end points of each step and use a weighted average of those as the actual derivative for the step. The simplest version of this would be to just use the derivative at the start of the step \u0026ndash; which is exactly the Euler method! The Euler method is a 1st order Runge-Kutta method.\nLet\u0026rsquo;s define the method more concretely.\nDerivation of Runge-Kutta Methods We will start by examining the initial problem again, being that we want to solve the following general equation for y:\n$$ \\dfrac{dy}{dt} = f(y, t) $$\nThis could be our cooling equation from before,\n$$ \\dfrac{dT}{dt} = -k(T - T_{surr}) $$\nor something more complex like a state space defining a spring, mass, damper system:\n$$ \\bold{\\dot{x}} = \\begin{bmatrix} 0 \u0026amp; 1 \\\\ \\frac{-k}{m} \u0026amp; \\frac{-c}{m} \\end{bmatrix} \\bold{x} + \\begin{bmatrix} 0 \\\\ \\frac{1}{m} \\end{bmatrix} u(t) $$\nIn both cases, we are given the derivative of the state we want to solve for (eg. temperature) and the derivative depends on the value of the state and time. In the case of the cooling equation, the derivative only depends on the state, T, and not time.\nIdeally, we could just integrate the derivative to find the value at \\( t + h \\):\n$$ y(t+h) = y(t) + \\int_{\\tau=t}^{\\tau=t+h}{\\dfrac{dy(\\tau)}{d\\tau}} d\\tau $$\nHowever, as we said earlier, this is often hard or impossible. We can instead approximate the integral with a weighted sum.\nWeighted Sum Approximations Let\u0026rsquo;s build this up slowly. Take the following function, for example:\n$$ f(t) = t^3 - 2t^2 - t + 3 $$\nHow would we evaluate\n$$ \\int_0^2{f(t)}dt $$\nWe can do this analytically, which is like summing up tiny vertical slices of the function to find the total area underneath it. But we could think about this a different way. We can get that same area by finding the average value of the function over the interval, then multiplying by the length of the interval. You can see this in Figure 4.\nFigure 4: Comparison of areas found by integration and weighted sum approximation.\nFigure 4 code\r1\u0026#39;\u0026#39;\u0026#39; 2Making sense of weighted sums as integral approximations. 3 4Created by: simmeon 5Last Modified: 28/04/24 6License: MIT 7 8\u0026#39;\u0026#39;\u0026#39; 9 10import matplotlib.pyplot as plt 11import numpy as np 12 13plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) 14 15def f(t): 16 return t**3 -2*t**2 - t + 3 17 18# Perform a weighted sum approximation 19N = 10 # higher number of samples, N, gives a better approximation 20sum = 0 21t0 = 0 22h = 2 23nodes = np.linspace(t0, t0+h, N) 24 25for node in nodes: 26 sum += f(node) 27weighted_sum = sum / N 28 29print(\u0026#39;Weighted sum area: \u0026#39;, weighted_sum * h) 30 31# Plotting 32t = np.arange(t0, t0 + h, 0.01) 33y = f(t) 34 35# Integration result 36plt.subplot(1,2,1) 37plt.plot(t,y) 38plt.fill_between(t, y, alpha=0.3) 39plt.annotate(\u0026#39;Area = 2.67\u0026#39;, (1,2.5), fontsize=20) 40plt.title(\u0026#39;Area from integration\u0026#39;, fontsize=20) 41 42# Weighted sum result 43plt.subplot(1,2,2) 44plt.plot(t,y) 45plt.fill_between(t, weighted_sum, alpha=0.3) 46plt.annotate(\u0026#39;Area = {:.2f}\u0026#39;.format(weighted_sum * h), (1,2.5), fontsize=20) 47plt.title(\u0026#39;Area from weighted sum\u0026#39;, fontsize=20) 48 49plt.show() To put this idea into an equation, we can say:\n$$ \\int_0^2{f(t)}dt \\approx \\frac{2}{\\sum{w_i}} \\sum_{i=1}^N{w_i f(t_i)} $$\nWhere \\( N \\) is the number of points we sample from the function to find the average, \\( w_i \\) is the weighting we give to each point, and \\( t_i \\) is some point inside the integration bounds of \\( [0, 2] \\). We can simplify this by giving every point a weighting of 1:\n$$ \\int_0^2{f(t)}dt \\approx \\frac{2}{N} \\sum_{i=1}^N{f(t_i)} $$\nThe more points we sample, the closer the average will be to the actual average of the function and therefore the actual integral.\nWe can extend this concept to our derivative from earlier:\n$$ \\int_{\\tau=t}^{\\tau=t+h}{\\dfrac{dy(\\tau)}{d\\tau}} d\\tau \\approx \\frac{h}{\\sum{w_i}} \\sum_{i=1}^N{w_i y^{\\prime}(t+v_i h, y(t+v_i h))} $$\nWhat a mess.\nWe\u0026rsquo;re still using the same concept of a weighted sum to find the average value of the derivative function, but since the derivative depends on \\( t \\) and \\( y \\) it\u0026rsquo;s a bit more complicated. We introduced \\( v_i \\) to help define which values of the function we sample, also called nodes. This can range from 0 to 1 to cover the interval from \\( t \\) to \\( t+h \\). Of course, whatever time we evaluate our derivative at must be the same time we use to get the \\( y \\) values for the derivative.\nFor the sake of simplicity let\u0026rsquo;s say that all our weights will sum up to 1. We are not, however, going to assume that all our weights will be the same value as we did previously.\nConsidering all this, the equation we are trying to solve is now:\n$$ y(t+h) = y(t) + h \\sum_{i=1}^N{w_i y^{\\prime}(t+v_i h, y(t+v_i h))} $$\nThe Runge-Kutta Family You might have noticed there is a problem with the sum we just defined. In particular, we don\u0026rsquo;t know what \\( y(t+v_i h) \\) is. In fact, finding this is basically the point of the whole method. So how do we deal with this?\nSimply, we are going to make worse approximations of \\( y(t+v_i h) \\) so that we can make a much better approximation of \\( y(t + h) \\). Again, these \\( y(t+v_i h) \\) values are used to find values of the derivative that we will then average. Let\u0026rsquo;s define what we will do to find these approximations.\nWe will start by saying that \\( v_1 = 0 \\). This means the first term in the sum will be\n$$ w_1 y^{\\prime}(t, y(t)) $$\nIf this was the only term in our sum (so \\( w_1 = 1 \\) ), then the equation we would be solving would be:\n$$ y(t+h) \\approx y(t) + h y^{\\prime}(t, y(t)) $$\nThis should look familiar, it\u0026rsquo;s the Euler method! This is what we mean by the Euler method is a 1st order Runge-Kutta method \u0026ndash; because it uses one term in the weighted sum. Or in other words, it is a linear approximation.\nFor simplicity, we will write this first sum term as:\n$$ k_1 = y^{\\prime}(t, y(t)) h $$\n\\( k_1 \\) is a 1st order estimate of the change in \\( y \\) between \\( y(t) \\) and \\( y(t+h) \\).\nThe second term gets trickier as we have to somehow estimate \\( y(t+v_2 h) \\). Conveniently, we have just found an estimate for how \\( y \\) changes: \\( k_1 \\). So we can utilise some fraction of this change to create our estimate for the second weighted sum term where:\n$$ y^{\\prime}(t + \\alpha_2 h, y(t) + \\beta_{2,1} k_1) $$\nWe don\u0026rsquo;t know yet how much of \\( k_1 \\) we should add, we will figure that out later. Same with what time we should sample at.\nWe have changed notation slightly to help set up higher order Runge-Kutta methods. Instead of \\( v_i \\) we are now using \\( \\alpha_i \\) to tell us about what time we are sampling at. And since our \\( y \\) value is no longer defined in simple terms of time, we are going to use \\( \\beta_{i,j} \\) to describe how much of previous estimates we will add to the estimate for the new term.\nWe define\n$$ k_2 = y^{\\prime}(t + \\alpha_2 h, y(t) + \\beta_{2,1} k_1) h $$\nso that the equation we are solving is now:\n$$ y(t+h) \\approx y(t) + w_1 k_1 + w_2 k_2 $$\nWith that, we have defined the 2nd order Runge-Kutta Method!\nWe can continue the weighted sum with a third and fourth term, following similar logic of using the previous estimates to inform the new value of \\( y(t + v_i h) \\). This will give us:\n$$ k_3 = y^{\\prime}(t + \\alpha_3 h, y(t) + \\beta_{3,1} k_1 + \\beta_{3,2} k_2) h \\\\ k_4 = y^{\\prime}(t + \\alpha_4 h, y(t) + \\beta_{4,1} k_1 + \\beta_{4,2} k_2 + \\beta_{4,3} k_3) h $$\nThis gives the 4th order Runge-Kutta method:\n$$ y(t+h) \\approx y(t) + w_1 k_1 + w_2 k_2 + w_3 k_3 + w_4 k_4 $$\nNow to actually solve these higher order methods, we need to define these coefficients\u0026hellip;\nDefining Coefficients We need to define all the \\( \\alpha_i \\), \\( \\beta_{i,j} \\), and \\( w_i \\) coefficients to be able to use these methods. We can do this by comparing the Taylor series expansion of our approximation to the Taylor series expansion of \\( y(t+h) \\) and equating coefficients. Let\u0026rsquo;s do this for the 2nd order method:\n$$ y(t+h) \\approx y(t) + w_1 k_1 + w_2 k_2 $$\nAs this is a 2nd order method, we will need to find the 2nd order expansions of the left and right sides.\nLet\u0026rsquo;s start with the left. The 2nd order Taylor series expansion of \\( y(t+h) \\) about \\( t \\) is:\n$$ y(t+h) \\approx y(t) + h \\dfrac{dy}{dt} \\Big| _{t,y} + \\frac{h^2}{2} \\dfrac{d^2y}{dt^2} \\Big| _{t,y} + O(h^3) $$\nWe can write our derivative:\n$$ \\dfrac{dy}{dt} = f(t, y) \\\\ {} \\\\ \\dfrac{d^2y}{dt^2} = \\dfrac{df(t,y)}{dt} = \\dfrac{\\partial f}{\\partial t} + \\dfrac{\\partial f}{\\partial y} \\dfrac{dy}{dt} = \\dfrac{\\partial f}{\\partial t} + f \\dfrac{\\partial f}{\\partial y} $$\nThis makes our left hand side (using slightly different notation):\n$$ y_{n+1} \\approx y_n + h f(t_n, y_n) + \\frac{h^2}{2} \\bigg(\\dfrac{\\partial f}{\\partial t} + f \\dfrac{\\partial f}{\\partial y}\\bigg) \\bigg|_{t_n, y_n} + O(h^3) $$\nGreat! Let\u0026rsquo;s do the right hand side now. We can write the right hand same with the same notation as above:\n$$ y_n + w_1 k_{1,n} + w_2 k_{2,n} $$\n\\( k_2 \\) is a bit tricky to expand. Our \\( k_2 \\) term is made up mostly of a function with the form \\( f(t + \\Delta t, y + \\Delta y) \\), which will need to be expanded. In general, the 2nd order expansion looks like:\n$$ f(t + \\Delta t, y + \\Delta y) = f(t, y) + \\Delta t \\dfrac{\\partial f}{\\partial t}\\bigg| _{t, y} + \\Delta y \\dfrac{\\partial f}{\\partial y} \\bigg| _{t, y} + O(h^3) $$\nApplying this to \\( k_2 \\) gives:\n$$ k_{2,n} = h f(t + \\alpha_2 h, y_n + \\beta_{2,1} k_1) \\approx h \\bigg( f(t_n, y_n) + \\alpha _2 h \\dfrac{\\partial f}{\\partial t} \\bigg| _{t_n, y_n} + \\beta _{2,1} k_1 \\dfrac{\\partial f}{\\partial y} \\bigg| _{t_n, y_n} \\bigg) $$\nAll together, the right hand side is then\n$$ y_n + w_1 k_{1,n} + w_2 k_{2,n} \\approx y_n + w_1 h f(t_n, y_n) + w_2 h \\bigg( f(t_n, y_n) + \\alpha _2 h \\dfrac{\\partial f}{\\partial t} \\bigg| _{t_n, y_n} + \\beta _{2,1} k_1 \\dfrac{\\partial f}{\\partial y} \\bigg| _{t_n, y_n} \\bigg) + O(h^3) $$\nwhich we can rearrange to be in the same form (substituting in for \\( k_1 \\)) as the left hand side:\n$$ y_n + w_1 k_{1,n} + w_2 k_{2,n} \\approx y_n + (w_1 + w_2) h f(t_n, y_n) + \\frac{h^2}{2} \\bigg(2 w_2 \\alpha_2 \\dfrac{\\partial f}{\\partial t} + 2 w_2 \\beta _{2,1} f \\dfrac{\\partial f}{\\partial y} \\bigg) \\bigg| _{t_n, y_n} + O(h^3) $$\nFinally, we have both expansions and can equate the coefficients:\n$$ y(t+h) \\approx y(t) + w_1 k_1 + w_2 k_2 \\\\ {} \\\\ \\big\\downarrow \\\\ {} \\\\ y_n + h f(t_n, y_n) + \\frac{h^2}{2} \\bigg(\\dfrac{\\partial f}{\\partial t} + f \\dfrac{\\partial f}{\\partial y}\\bigg) \\bigg|_{t_n, y_n} + O(h^3) \\\\ {} \\\\ = \\\\ {} \\\\ y_n + (w_1 + w_2) h f(t_n, y_n) + \\frac{h^2}{2} \\bigg(2 w_2 \\alpha_2 \\dfrac{\\partial f}{\\partial t} + 2 w_2 \\beta _{2,1} f \\dfrac{\\partial f}{\\partial y} \\bigg) \\bigg| _{t_n, y_n} + O(h^3) $$\nFrom this, we can see that our coefficients must satisfy the following equations:\n$$ w_1 + w_2 = 1 \\\\ {} \\\\ w_2 \\alpha_2= \\frac{1}{2} \\\\ {} \\\\ w_2 \\beta_{2,1} = \\frac{1}{2} $$\nWith 3 equations and 4 unknows, there are infinitely many solutions. However, the standard choices are:\n$$ \\alpha_2 = \\beta_{2,1} = 1 \\\\ {} \\\\ w_1 = w_2 = \\frac{1}{2} $$\nThis brings us finally to the complete 2nd order Runge-Kutta Method:\n$$ k_1 = h y^\\prime (t_n, y_n) \\\\ {} \\\\ k_2 = h y^\\prime (t_n + h, y_n + k_1) \\\\ {} \\\\ y_{n+1} = y_n + \\frac{1}{2} k_1 + \\frac{1}{2} k_2 $$\nA similar (messier) method of expansions can be used to for higher order methods. The standard terms for the 4th order Runge-Kutta method are:\n$$ k_1 = h y^\\prime (t_n, y_n) \\\\ {} \\\\ k_2 = h y^\\prime (t_n + \\frac{h}{2}, y_n + \\frac{k_1}{2}) \\\\ {} \\\\ k_3 = h y^\\prime (t_n + \\frac{h}{2}, y_n + \\frac{k_2}{2}) \\\\ {} \\\\ k_4 = h y^\\prime (t_n + h, y_n + k_3) \\\\ {} \\\\ y_{n+1} = y_n + \\frac{1}{6} (k_1 + 2 k_2 + 2 k_3 + k_4) $$\nKeep in mind that there are infinitely many choices of these coefficients and lots of research has gone into figuring out which ones work best. We will stick to these simple standard ones for now.\nImplementing Runge-Kutta Now that we have finally derived the Runge-Kutta methods, let\u0026rsquo;s implement them in code. We will do both the 2nd order and 4th order methods and compare their accuracy.\n1def rk2(dydt, t0, y0, t_end, h): 2 \u0026#34;\u0026#34;\u0026#34; 3 Solves a first-order ODE using the 2nd order Runge-Kutta method 4 5 Args: 6 dydt: The derivative function to integrate 7 t0: Initial value of time 8 y0: Initial value of y 9 t_end: Final time for integration 10 h: Step size 11 12 Returns: 13 An array of y values for each time step 14 \u0026#34;\u0026#34;\u0026#34; 15 t = np.arange(t0, t_end+h, h) # Array of time points 16 y = np.zeros_like(t) 17 y[0] = y0 18 19 for i in range(1, len(t)): 20 k1 = h * dydt(t[i - 1], y[i - 1]) 21 k2 = h * dydt(t[i - 1] + h, y[i - 1] + k1) 22 23 y[i] = y[i - 1] + (k1 + k2) / 2 24 25 return y We can see how, at each time step, we calculate the \\( k_i \\) values and use these to estimate the next value of the function.\n1def rk4(dydt, t0, y0, t_end, h): 2 \u0026#34;\u0026#34;\u0026#34; 3 Solves a first-order ODE using the 4th order Runge-Kutta method 4 5 Args: 6 dydt: The derivative function to integrate 7 t0: Initial value of time 8 y0: Initial value of y 9 t_end: Final time for integration 10 h: Step size 11 12 Returns: 13 An array of y values for each time step 14 \u0026#34;\u0026#34;\u0026#34; 15 t = np.arange(t0, t_end+h, h) 16 y = np.zeros_like(t) 17 y[0] = y0 18 19 for i in range(1, len(t)): 20 k1 = h * dydt(t[i - 1], y[i - 1]) 21 k2 = h * dydt(t[i - 1] + h / 2, y[i - 1] + k1 / 2) 22 k3 = h * dydt(t[i - 1] + h / 2, y[i - 1] + k2 / 2) 23 k4 = h * dydt(t[i], y[i - 1] + k3) 24 25 y[i] = y[i - 1] + (k1 + 2 * k2 + 2 * k3 + k4) / 6 26 27 return y After all that derivation, the actual method is remarkably clean and simple to implement. Figure 5 gives an idea of how these higher order functions perform compared to our original Euler method.\nFigure 5: Comparing the accuracy of different order ODE solvers, h = 0.1\nAll the solvers are using the same step size here. As we can see, the 4th order method is better than the 2nd order method. They are both much more accurate than the 1st order Euler method.\nAs a final comparison, let\u0026rsquo;s look at Newton\u0026rsquo;s law of cooling one last time. Figure 6 shows how our 4th order solver compares to our previous test with the Euler method.\nFigure 6: Comparing the accuracy of the RK4 and Euler methods on the cooling equation. Tsurr = 20 C, k = 1, h = 0.5\nFigure 5 and 6 code\r1\u0026#39;\u0026#39;\u0026#39; 2Implementing and comparing the Runge-Kutta 2nd and 4th order methods. 3We can also compare to the Euler method (1st order). 4 5Created by: simmeon 6Last Modified: 29/04/24 7License: MIT 8 9\u0026#39;\u0026#39;\u0026#39; 10 11import matplotlib.pyplot as plt 12import numpy as np 13 14plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) 15 16# Define some functions to test our solvers with 17def dydt(t, y): 18 return 3 * t**2 + 20 * np.cos(10 * t) 19 20def analytical_solution(t): 21 return t**3 + 2 * np.sin(10 * t) # Analytical solution to the ODE 22 23# # Define our cooling function 24# def solve_T(t, T0): 25# return T_surr + (T0 - T_surr)*np.exp(-k*t) 26 27# # Define the derivative (only depends on the current temp, not time) 28# def dTdt(T): 29# return -k * (T - T_surr) 30 31def euler_solver(dydt, t0, y0, t_end, h): 32 \u0026#34;\u0026#34;\u0026#34; 33 Solves a first-order ODE using the Euler method 34 35 Args: 36 dydt: The differential equation (dy/dt) as a Python function 37 t0: Initial value of time 38 y0: Initial value of y 39 t_end: Final time for integration 40 h: Step size 41 42 Returns: 43 An array of y values for each time step 44 \u0026#34;\u0026#34;\u0026#34; 45 t = np.arange(t0, t_end+h, h) 46 y = np.zeros_like(t) 47 y[0] = y0 48 49 for i in range(1, len(t)): 50 y[i] = y[i - 1] + h * dydt(t[i - 1], y[i - 1]) 51 52 return y 53 54def rk2(dydt, t0, y0, t_end, h): 55 \u0026#34;\u0026#34;\u0026#34; 56 Solves a first-order ODE using the 2nd order Runge-Kutta method 57 58 Args: 59 dydt: The derivative function to integrate 60 t0: Initial value of time 61 y0: Initial value of y 62 t_end: Final time for integration 63 h: Step size 64 65 Returns: 66 An array of y values for each time step 67 \u0026#34;\u0026#34;\u0026#34; 68 t = np.arange(t0, t_end+h, h) # Array of time points 69 y = np.zeros_like(t) 70 y[0] = y0 71 72 for i in range(1, len(t)): 73 k1 = h * dydt(t[i - 1], y[i - 1]) 74 k2 = h * dydt(t[i - 1] + h, y[i - 1] + k1) 75 y[i] = y[i - 1] + (k1 + k2) / 2 76 77 return y 78 79def rk4(dydt, t0, y0, t_end, h): 80 \u0026#34;\u0026#34;\u0026#34; 81 Solves a first-order ODE using the 4th order Runge-Kutta method 82 83 Args: 84 dydt: The derivative function to integrate 85 t0: Initial value of time 86 y0: Initial value of y 87 t_end: Final time for integration 88 h: Step size 89 90 Returns: 91 An array of y values for each time step 92 \u0026#34;\u0026#34;\u0026#34; 93 t = np.arange(t0, t_end+h, h) 94 y = np.zeros_like(t) 95 y[0] = y0 96 97 for i in range(1, len(t)): 98 k1 = dydt(t[i - 1], y[i - 1]) 99 k2 = dydt(t[i - 1] + h / 2, y[i - 1] + k1 / 2) 100 k3 = dydt(t[i - 1] + h / 2, y[i - 1] + k2 / 2) 101 k4 = dydt(t[i], y[i - 1] + k3) 102 103 y[i] = y[i - 1] + (k1 + 2 * k2 + 2 * k3 + k4) / 6 * h 104 105 return y 106 107# Define simulation parameters 108t0 = 0 109y0 = 0 110t_end = 5 111h = 0.1 112 113# Solve using both methods 114y_euler = euler_solver(dydt, t0, y0, t_end, h) 115y_rk2 = rk2(dydt, t0, y0, t_end, h) 116y_rk4 = rk4(dydt, t0, y0, t_end, h) 117 118# Get analytical solution 119t = np.arange(t0, t_end+h, h) # Array of time points 120y_analytical = analytical_solution(t) 121 122# Calculate errors 123error_euler = np.abs(y_euler - y_analytical) 124error_rk2 = np.abs(y_rk2 - y_analytical) 125error_rk4 = np.abs(y_rk4 - y_analytical) 126 127# Plot the numerical solutions 128plt.subplot(1,2,1) 129plt.plot(t, y_euler, label=\u0026#39;Euler\u0026#39;) 130plt.plot(t, y_rk2, label=\u0026#39;RK2\u0026#39;) 131plt.plot(t, y_rk4, label=\u0026#39;RK4\u0026#39;) 132plt.plot(t, y_analytical, label=\u0026#39;Analytical\u0026#39;, linestyle=\u0026#39;--\u0026#39;) 133plt.xlabel(\u0026#39;Time (t)\u0026#39;, fontsize=20) 134plt.ylabel(\u0026#39;y(t)\u0026#39;, fontsize=20) 135plt.title(\u0026#39;Numerical Solutions\u0026#39;, fontsize=20) 136plt.legend(fontsize=15) 137 138# Plot the errors 139plt.subplot(1,2,2) 140plt.plot(t, error_euler, label=\u0026#39;Euler\u0026#39;) 141plt.plot(t, error_rk2, label=\u0026#39;RK2\u0026#39;) 142plt.plot(t, error_rk4, label=\u0026#39;RK4\u0026#39;) 143plt.xlabel(\u0026#39;Time (t)\u0026#39;, fontsize=20) 144plt.ylabel(\u0026#39;Abs Error\u0026#39;, fontsize=20) 145plt.title(\u0026#39;Absolute Errors of the Numerical Solutions\u0026#39;, fontsize=20) 146plt.legend(fontsize=15) 147 148plt.show() Even with a relatively large step size, the 4th order method is still much better than the Euler method. In fact, we could make the step size even bigger and still have a solution that fell within some tiny tolerance.\nBut how can we choose a good value to make this step size? If we want to be as efficient as possible, each step would as large as it can be while staying within the tolerance we want\u0026hellip;\nStep Sizing Constant Step Size Issues Take the following function,\n$$ y = \\sin{(t^5)} \\\\ {} \\\\ \\dfrac{dy}{dt} = 5t^4 \\cos{(t^5)} $$\nWe can use the derivative to solve for \\( y \\) using one of our numerical methods from earlier. Figure 7 shows us doing this using the Euler method, along with the error in our solution.\nFigure 7: Euler method solution and error using a constant step size, h = 0.05.\nFigure 7 code\r1\u0026#39;\u0026#39;\u0026#39; 2Seeing how a constant step size numerical solver can have wildly varying 3error on diffrent steps. 4 5Created by: simmeon 6Last Modified: 18/05/24 7License: MIT 8 9\u0026#39;\u0026#39;\u0026#39; 10 11import matplotlib.pyplot as plt 12import numpy as np 13 14plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) 15 16# Define function and its derivative 17def dydt(t, y): 18 return 5*t**4 * np.cos(t**5) 19 20def analytical_solution(t): 21 return np.sin(t**5) 22 23# Our Euler numerical solver 24def euler_solver(dydt, t0, y0, t_end, h): 25 \u0026#34;\u0026#34;\u0026#34; 26 Solves a first-order ODE using the Euler method 27 28 Args: 29 dydt: The differential equation (dy/dt) as a Python function 30 t0: Initial value of time 31 y0: Initial value of y 32 t_end: Final time for integration 33 h: Step size 34 35 Returns: 36 An array of y values for each time step 37 \u0026#34;\u0026#34;\u0026#34; 38 t = np.arange(t0, t_end+h, h) 39 y = np.zeros_like(t) 40 y[0] = y0 41 42 for i in range(1, len(t)): 43 y[i] = y[i - 1] + h * dydt(t[i - 1], y[i - 1]) 44 45 return t, y 46 47 48# Define simulation parameters 49t0 = 0 50y0 = 0 51t_end = 2 52 53h = 0.05 # we are using a constant step size here 54 55# Numerically integrate 56t_euler, y_euler = euler_solver(dydt, t0, y0, t_end, h) 57 58# Get analytical solution 59t_analytical = np.arange(t0, t_end+h, 0.001) 60y_analytical = analytical_solution(t_analytical) 61 62# Get error 63y_analytical_sampled = y_analytical[0:-1:50] 64error = np.abs(y_euler - y_analytical_sampled) 65 66 67# Plotting 68plt.subplot(2,1,1) 69plt.plot(t_analytical, y_analytical, label=\u0026#39;Analytical\u0026#39;, linestyle=\u0026#39;-\u0026#39;) 70plt.plot(t_euler, y_euler, label=\u0026#39;Euler method\u0026#39;) 71 72plt.title(\u0026#34;Error when integrating with a constant step size\u0026#34;, fontsize=20) 73plt.ylabel(\u0026#39;y\u0026#39;, fontsize=20) 74plt.legend(fontsize=15) 75 76plt.subplot(2,1,2) 77plt.plot(t_euler, error) 78plt.ylabel(\u0026#34;absolute error\u0026#34;, fontsize=20) 79plt.xlabel(\u0026#39;t\u0026#39;, fontsize=20) 80 81plt.show() The method works well initially when the function is changing slowly. However, when the function is changing more rapidly, our Euler method solver error gets large. You can try increasing the time to integrate over and see how the error becomes even bigger.\nThis is happening because our step size (\\( h = 0.05 \\)) becomes too large to properly capture how the function is changing. We could make our step size smaller, but we don\u0026rsquo;t need that additional resolution for the start of the function, it\u0026rsquo;s already pretty accurate.\nUsing Multiple Step Sizes Instead of decreasing the step size for the entire integration period, it would be much more efficient if we could just decrease it where we need to.\nIn this example, we could try switching to a smaller step size (say, \\( h = 0.01 \\)) after \\( t = 1 \\) when our solution starts getting less accurate. Doing this gives the solution shown in Figure 8.\nFigure 8: Euler method solution and error using two step sizes: t \u0026lt;= 1, h = 0.05; t \u0026gt; 1, h = 0.01\nFigure 8 code\r1\u0026#39;\u0026#39;\u0026#39; 2Seeing how we can improve our solution and efficiency by using two different 3step sizes at different times. 4 5Created by: simmeon 6Last Modified: 18/05/24 7License: MIT 8 9\u0026#39;\u0026#39;\u0026#39; 10 11import matplotlib.pyplot as plt 12import numpy as np 13 14plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) 15 16# Define function and its derivative 17def dydt(t, y): 18 return 5*t**4 * np.cos(t**5) 19 20def analytical_solution(t): 21 return np.sin(t**5) 22 23# Our Euler numerical solver 24def euler_solver(dydt, t0, y0, t_end, h): 25 \u0026#34;\u0026#34;\u0026#34; 26 Solves a first-order ODE using the Euler method 27 28 Args: 29 dydt: The differential equation (dy/dt) as a Python function 30 t0: Initial value of time 31 y0: Initial value of y 32 t_end: Final time for integration 33 h: Step size 34 35 Returns: 36 An array of y values for each time step 37 \u0026#34;\u0026#34;\u0026#34; 38 t = np.arange(t0, t_end+h, h) 39 y = np.zeros_like(t) 40 y[0] = y0 41 42 for i in range(1, len(t)): 43 y[i] = y[i - 1] + h * dydt(t[i - 1], y[i - 1]) 44 45 return t, y 46 47 48# Define simulation parameters 49t0 = 0 50y0 = 0 51t_end1 = 1 52t_end2 = 2 53 54h1 = 0.05 # first, bigger step size 55h2 = 0.01 # second. smaller step size 56 57# Numerically integrate 58t_euler1, y_euler1 = euler_solver(dydt, t0, y0, t_end1, h1) 59t_euler2, y_euler2 = euler_solver(dydt, t_end1, y_euler1[-1], t_end2, h2) 60 61y_euler = np.concatenate((y_euler1, y_euler2)) 62t_euler = np.concatenate((t_euler1, t_euler2)) 63 64# Get analytical solution 65t_analytical = np.arange(t0, t_end2+h2, 0.001) 66y_analytical = analytical_solution(t_analytical) 67 68# Get error 69y_analytical_compare = analytical_solution(t_euler) 70error = np.abs(y_euler - y_analytical_compare) 71 72 73# Plotting 74plt.subplot(2,1,1) 75plt.plot(t_analytical, y_analytical, label=\u0026#39;Analytical\u0026#39;, linestyle=\u0026#39;-\u0026#39;) 76plt.plot(t_euler, y_euler, label=\u0026#39;Euler method\u0026#39;) 77plt.vlines(t_end1, -1, 1, colors=\u0026#39;white\u0026#39;, alpha=0.5) 78 79plt.title(\u0026#34;Error when integrating with two different step sizes\u0026#34;, fontsize=20) 80plt.ylabel(\u0026#39;y\u0026#39;, fontsize=20) 81plt.legend(fontsize=15) 82 83plt.subplot(2,1,2) 84plt.plot(t_euler, error) 85plt.vlines(t_end1, 0, 0.5, colors=\u0026#39;white\u0026#39;, alpha=0.5) 86plt.ylabel(\u0026#34;absolute error\u0026#34;, fontsize=20) 87plt.xlabel(\u0026#39;t\u0026#39;, fontsize=20) 88 89plt.show() This has made our solution around an order of magnitude more accurate. But how much computational complexity have we saved compared to just solving the entire time interval with the smaller step size? We can summarize this in Table 1.\nTable 1\nStep size Number of iteration steps \\( h = 0.05 \\) 41 \\( t \\leq 1, h = 0.05 \\\\ t\u0026gt;1, h = 0.01\\) 122 \\( h = 0.01 \\) 201 So we are saving a significant amount of computation compared to making the whole interval have a smaller step size.\nChoosing to make the step size smaller at \\( t = 1 \\) was very arbitrary. We could also change the step size in more places \u0026ndash; maybe making it even bigger initially and even smaller for \\( t \u0026gt; 1.5 \\). We should come up with a smart way of choosing when and how to change the step size.\nAdaptive Step Sizing Since we want our solution to be accurate, it would be smart to decrease our step size when our error is getting too big. Also, to keep things efficient, we could make our step size bigger when the error is very small.\nThe issue is we don\u0026rsquo;t have the actual solution, so how can we know what our error is?\nWe need to create some sort of \u0026ldquo;true solution\u0026rdquo; to compare our estimate against. One way we could do this is to use a higher order method, since we know that these should be more accurate and be closer to the actual solution. For example, if we want to use a 1st order Euler solver, we could also calculate a 2nd order Runge-Kutta solution and use that as the \u0026ldquo;true solution\u0026rdquo;.\nSo for every step, we will calculate the next step with two methods: a lower and higher order one. Then, we will use difference in these as our approximation of the error. From that, we can decide whether we should change the step size for the step or if it was ok.\nThe last thing to decide is how much we should change the step size. For now, a simple approach could be to halve the step size if the error was too big, and double it if it was too small.\nMore formally, we will:\nTake a step with the current step size using a 1st and 2nd order method. Compare the solutions to approximate our error. If the \\( error \u0026gt; tol \\), reject the step, halve the step size, and go back to Step 1. If the \\( error \u0026lt; tol \\), accept the step and double the step size for the next step. In code this would look like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def euler_solver(dydt, t0, y0, t_end, tol): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the Euler method with adaptive step sizing. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y t_end: Final time for integration tol: Error tolerance Returns: Arrays of y and t values for each time step \u0026#34;\u0026#34;\u0026#34; h = 0.1 # set some initial step size t = [t0] y_euler = [y0] y_rk2 = [y0] i = 1 while t[-1] \u0026lt; t_end: # Take a Euler step y_euler.append(euler_step(dydt, t[i-1], y_euler[i-1], h)) t.append(t[i-1] + h) # Take a RK2 step, starting from the previous Euler step value y_rk2.append(rk2_step(dydt, t[i-1], y_euler[i-1], h)) isStepGood = False error = np.abs(y_rk2[i] - y_euler[i]) if error \u0026lt; tol: # accept step, make step size bigger for next step isStepGood = True h = h * 2 while not isStepGood: # Take a step with both methods y_euler[i] = euler_step(dydt, t[i-1], y_euler[i-1], h) t[i] = t[i-1] + h y_rk2[i] = rk2_step(dydt, t[i-1], y_euler[i-1], h) # Calculate the error between the methods error = np.abs(y_rk2[i] - y_euler[i]) # Check error to accept or reject the step if error \u0026gt; tol: # reject step, halve step size h = h / 2 else: # accept step, make step size bigger for next step isStepGood = True h = h * 2 i += 1 return t, y_euler If we use this to solve the same function as before, we get the solution shown in Figure 9.\nFigure 9: Euler method solution and error with an adaptive step size, tol = 0.01.\nFigure 9 code\r1\u0026#39;\u0026#39;\u0026#39; 2Implementing an adaptive step size by doubling or halving the step size 3depending on the error. 4 5Created by: simmeon 6Last Modified: 18/05/24 7License: MIT 8 9\u0026#39;\u0026#39;\u0026#39; 10 11import matplotlib.pyplot as plt 12import numpy as np 13 14plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) 15 16# Define function and its derivative 17def dydt(t, y): 18 return 5*t**4 * np.cos(t**5) 19 20def analytical_solution(t): 21 return np.sin(t**5) 22 23 24def euler_step(dydt, t0, y0, h): 25 \u0026#34;\u0026#34;\u0026#34; 26 Takes a single 1st order integration step and returns the y value. 27 28 Args: 29 dydt: The differential equation (dy/dt) as a Python function 30 t0: Initial value of time 31 y0: Initial value of y 32 h: Step size 33 34 Returns: 35 The y value after the step. 36 37 \u0026#34;\u0026#34;\u0026#34; 38 39 y = y0 + h * dydt(t0, y0) 40 41 return y 42 43def rk2_step(dydt, t0, y0, h): 44 \u0026#34;\u0026#34;\u0026#34; 45 Takes a single 2nd order integration step and returns the y value. 46 47 Args: 48 dydt: The differential equation (dy/dt) as a Python function 49 t0: Initial value of time 50 y0: Initial value of y 51 h: Step size 52 53 Returns: 54 The y value after the step. 55 56 \u0026#34;\u0026#34;\u0026#34; 57 58 k1 = h * dydt(t0, y0) 59 k2 = h * dydt(t0 + h, y0 + k1) 60 y = y0 + (k1 + k2) / 2 61 62 return y 63 64 65# Our adaptive Euler numerical solver 66def euler_solver(dydt, t0, y0, t_end, tol): 67 \u0026#34;\u0026#34;\u0026#34; 68 Solves a first-order ODE using the Euler method with adaptive 69 step sizing. 70 71 Args: 72 dydt: The differential equation (dy/dt) as a Python function 73 t0: Initial value of time 74 y0: Initial value of y 75 t_end: Final time for integration 76 tol: Error tolerance 77 78 Returns: 79 Arrays of y, t and local error values for each time step 80 \u0026#34;\u0026#34;\u0026#34; 81 h = 0.1 # set some initial step size 82 t = [t0] 83 y_euler = [y0] 84 y_rk2 = [y0] 85 error = [0] 86 87 i = 1 88 while t[-1] \u0026lt; t_end: 89 # Take a Euler step 90 y_euler.append(euler_step(dydt, t[i-1], y_euler[i-1], h)) 91 t.append(t[i-1] + h) 92 93 # Take a RK2 step, starting from the previous Euler step value 94 y_rk2.append(rk2_step(dydt, t[i-1], y_euler[i-1], h)) 95 96 isStepGood = False 97 98 error.append(np.abs(y_rk2[i] - y_euler[i])) 99 100 if error[i] \u0026lt; tol: 101 # accept step, make step size bigger for next step 102 isStepGood = True 103 h = h * 2 104 105 while not isStepGood: 106 # Take a step with both methods 107 y_euler[i] = euler_step(dydt, t[i-1], y_euler[i-1], h) 108 t[i] = t[i-1] + h 109 y_rk2[i] = rk2_step(dydt, t[i-1], y_euler[i-1], h) 110 111 # Calculate the error between the methods 112 error[i] = np.abs(y_rk2[i] - y_euler[i]) 113 114 # Check error to accept or reject the step 115 if error[i] \u0026gt; tol: 116 # reject step, halve step size 117 h = h / 2 118 else: 119 # accept step, make step size bigger for next step 120 isStepGood = True 121 h = h * 2 122 123 i += 1 124 125 return t, y_euler, error 126 127 128# Define simulation parameters 129t0 = 0 130y0 = 0 131t_end = 2 132tol = 1e-2 133 134# Numerically integrate 135t_euler, y_euler, local_error = euler_solver(dydt, t0, y0, t_end, tol) 136 137t_euler = np.array(t_euler) 138y_euler = np.array(y_euler) 139 140# Get analytical solution 141t_analytical = np.arange(t0, t_end, 0.001) 142y_analytical = analytical_solution(t_analytical) 143 144# Get error 145y_analytical_compare = analytical_solution(t_euler) 146error = np.abs(y_euler - y_analytical_compare) 147 148# Plotting 149plt.subplot(3,1,1) 150plt.plot(t_analytical, y_analytical, label=\u0026#39;Analytical\u0026#39;, linestyle=\u0026#39;-\u0026#39;) 151plt.plot(t_euler, y_euler, label=\u0026#39;Euler method\u0026#39;) 152plt.scatter(t_euler, y_euler, s=30, marker=\u0026#39;o\u0026#39;, facecolors=\u0026#39;none\u0026#39;, color=\u0026#39;C1\u0026#39;) 153 154plt.title(\u0026#34;Integrating with a adaptive step size, tol = 0.01\u0026#34;, fontsize=20) 155plt.ylabel(\u0026#39;y\u0026#39;, fontsize=20) 156plt.legend(fontsize=15) 157plt.text(0, -0.75, f\u0026#34;Number of steps: {len(t_euler)}\u0026#34;, fontsize=15) 158 159plt.subplot(3,1,2) 160plt.plot(t_euler, error) 161plt.ylabel(\u0026#34;absolute error\u0026#34;, fontsize=20) 162plt.xlabel(\u0026#39;t\u0026#39;, fontsize=20) 163 164plt.subplot(3,1,3) 165plt.plot(t_euler, local_error) 166plt.ylabel(\u0026#34;local step error\u0026#34;, fontsize=20) 167plt.xlabel(\u0026#39;t\u0026#39;, fontsize=20) 168 169plt.show() Each dot represents where a step was taken. We can see how the step size is bigger when the function is changing slowly and smaller when the function changes quickly.\nThe error is interesting. We set a tolerance of \\( 0.01 \\) but the absolute error gets up to nearly \\( 0.1 \\). This is because the tolerance is used to set the allowable error for each step, not the overall (or global) error. Another reason is that our error is an estimate but here we are plotting against the actual, analytical error. Howveer, looking at the local error for each step we can see that it never gets above our tolerance.\nChoosing Better Step Size Changes We used a very simple method of halving the step size if the error was too big or doubling the step size if the error was too small. For the 255 steps we ended up with, the method calculated the next step 746 times. This means that, on average for each step, we changed the step size 2.9 times before finding a value that worked. There is a lot of computation that is wasted there.\nLet\u0026rsquo;s try and come up with a better way of choosing the step size to hopefully make our method more efficient.\nThis will depend on what order methods we are using.\nWe will start by doing this for the 1st order Euler method. We write this as:\n$$ y_{n+1} = y_{n} + h \\dfrac{dy}{dt} \\bigg| _{t_n, y_n} + O(h^2) $$\nThe error in our estimate is proportional to \\( h^2 \\). If we assume that this error is constant over time, we can say\n$$ error = \\Delta = c h^2 $$\nwhere \\( c \\) is some constant value. Then, for some step size, \\( h_1 \\), we would get\n$$ \\Delta _1 = c h_1^2 $$\nWe could also write an equation for what the step size would have to be to get an error equal to our tolerance. Let\u0026rsquo;s call this error \\( \\Delta _0 \\).\n$$ \\Delta _0 = c h_0^2 $$\nThis \\( h_0 \\) is the value that we would want to use for the current step size to. In other words, it will give the largest step that stays within our tolerance.\nWe can use these two equations together through the constant \\( c \\) to get the relation:\n$$ \\frac{\\Delta_1}{h_1^2} = \\frac{\\Delta_0}{h_0^2} $$\nThis can then be arranged to solve for what our step size \\( h_0 \\) should be\n$$ h_0 = h_1 \\bigg(\\frac{\\Delta _0}{\\Delta _1} \\bigg) ^{0.5} $$\nor in more familiar notation\n$$ h_{new} = h_{current} \\bigg(\\frac{tol}{error} \\bigg) ^{0.5} $$\nIn theory, using this formula should mean that we get the correct step size in one iteration instead of two or three. However, in practice, our error is only an approximation and so we should put in a safety factor to make the new step size slightly smaller that the theoretical value:\n$$ h_{new} = 0.9 h_{current} \\bigg(\\frac{tol}{error} \\bigg) ^{0.5} $$\nLet\u0026rsquo;s use this instead of our doubling and halving approach and see how many calculations we use. Note that formula has the correct behaviour for errors that are both larger and smaller than the tolerance. We can see the result in Figure 10.\nFigure 10: Euler method solution and error with a better adaptive step size, tol = 0.01.\nFigure 10 code\r1\u0026#39;\u0026#39;\u0026#39; 2Implementing an adaptive step size with better step sizing. 3 4Created by: simmeon 5Last Modified: 19/05/24 6License: MIT 7 8\u0026#39;\u0026#39;\u0026#39; 9 10import matplotlib.pyplot as plt 11import numpy as np 12 13plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) 14 15# Define function and its derivative 16def dydt(t, y): 17 return 5*t**4 * np.cos(t**5) 18 19def analytical_solution(t): 20 return np.sin(t**5) 21 22 23def euler_step(dydt, t0, y0, h): 24 \u0026#34;\u0026#34;\u0026#34; 25 Takes a single 1st order integration step and returns the y value. 26 27 Args: 28 dydt: The differential equation (dy/dt) as a Python function 29 t0: Initial value of time 30 y0: Initial value of y 31 h: Step size 32 33 Returns: 34 The y value after the step. 35 36 \u0026#34;\u0026#34;\u0026#34; 37 38 y = y0 + h * dydt(t0, y0) 39 40 return y 41 42def rk2_step(dydt, t0, y0, h): 43 \u0026#34;\u0026#34;\u0026#34; 44 Takes a single 2nd order integration step and returns the y value. 45 46 Args: 47 dydt: The differential equation (dy/dt) as a Python function 48 t0: Initial value of time 49 y0: Initial value of y 50 h: Step size 51 52 Returns: 53 The y value after the step. 54 55 \u0026#34;\u0026#34;\u0026#34; 56 57 k1 = h * dydt(t0, y0) 58 k2 = h * dydt(t0 + h, y0 + k1) 59 y = y0 + (k1 + k2) / 2 60 61 return y 62 63 64# Our adaptive Euler numerical solver 65def euler_solver(dydt, t0, y0, t_end, tol): 66 \u0026#34;\u0026#34;\u0026#34; 67 Solves a first-order ODE using the Euler method with adaptive 68 step sizing. 69 70 Args: 71 dydt: The differential equation (dy/dt) as a Python function 72 t0: Initial value of time 73 y0: Initial value of y 74 t_end: Final time for integration 75 tol: Error tolerance 76 77 Returns: 78 Arrays of y, t and local error values for each time step 79 \u0026#34;\u0026#34;\u0026#34; 80 h = 0.1 # set some initial step size 81 t = [t0] 82 y_euler = [y0] 83 y_rk2 = [y0] 84 error = [0] 85 86 i = 1 87 count = 0 88 89 while t[-1] \u0026lt; t_end: 90 count += 1 91 # Take a Euler step 92 y_euler.append(euler_step(dydt, t[i-1], y_euler[i-1], h)) 93 t.append(t[i-1] + h) 94 95 # Take a RK2 step, starting from the previous Euler step value 96 y_rk2.append(rk2_step(dydt, t[i-1], y_euler[i-1], h)) 97 98 isStepGood = False 99 100 error.append(np.abs(y_rk2[i] - y_euler[i])) 101 102 if error[i] \u0026lt; tol: 103 # accept step, make step size bigger for next step 104 isStepGood = True 105 h = 0.9 * h * (tol / error[i]) ** 0.5 106 107 while not isStepGood: 108 count += 1 109 # Take a step with both methods 110 y_euler[i] = euler_step(dydt, t[i-1], y_euler[i-1], h) 111 t[i] = t[i-1] + h 112 y_rk2[i] = rk2_step(dydt, t[i-1], y_euler[i-1], h) 113 114 # Calculate the error between the methods 115 error[i] = np.abs(y_rk2[i] - y_euler[i]) 116 117 # Check error to accept or reject the step 118 if error[i] \u0026gt; tol: 119 # reject step, halve step size 120 h = 0.9 * h * (tol / error[i]) ** 0.5 121 else: 122 # accept step, make step size bigger for next step 123 isStepGood = True 124 h = 0.9 * h * (tol / error[i]) ** 0.5 125 126 i += 1 127 print(\u0026#34;Count: \u0026#34;, count) 128 129 return t, y_euler, error 130 131 132# Define simulation parameters 133t0 = 0 134y0 = 0 135t_end = 2 136tol = 1e-2 137 138# Numerically integrate 139t_euler, y_euler, local_error = euler_solver(dydt, t0, y0, t_end, tol) 140 141t_euler = np.array(t_euler) 142y_euler = np.array(y_euler) 143 144# Get analytical solution 145t_analytical = np.arange(t0, t_end, 0.001) 146y_analytical = analytical_solution(t_analytical) 147 148# Get error 149y_analytical_compare = analytical_solution(t_euler) 150error = np.abs(y_euler - y_analytical_compare) 151 152# Plotting 153plt.subplot(3,1,1) 154plt.plot(t_analytical, y_analytical, label=\u0026#39;Analytical\u0026#39;, linestyle=\u0026#39;-\u0026#39;) 155plt.plot(t_euler, y_euler, label=\u0026#39;Euler method\u0026#39;) 156plt.scatter(t_euler, y_euler, s=30, marker=\u0026#39;o\u0026#39;, facecolors=\u0026#39;none\u0026#39;, color=\u0026#39;C1\u0026#39;) 157 158plt.title(\u0026#34;Integrating with a adaptive step size, tol = 0.01\u0026#34;, fontsize=20) 159plt.ylabel(\u0026#39;y\u0026#39;, fontsize=20) 160plt.legend(fontsize=15) 161plt.text(0, -0.75, f\u0026#34;Number of steps: {len(t_euler)}\u0026#34;, fontsize=15) 162 163plt.subplot(3,1,2) 164plt.plot(t_euler, error) 165plt.ylabel(\u0026#34;absolute error\u0026#34;, fontsize=20) 166 167plt.subplot(3,1,3) 168plt.plot(t_euler, local_error) 169plt.ylabel(\u0026#34;local step error\u0026#34;, fontsize=20) 170plt.xlabel(\u0026#39;t\u0026#39;, fontsize=20) 171 172plt.show() This change used 216 steps with a total of 323 step calulations, meaning we changed the step size 1.5 times per step. This is much closer to the minimum of 1 change per step. It also ended up requiring less steps than previously, being even more efficient.\nThis step change formula we created can be applied to higher order methods, for example if we were using a 4th order method with 5th order errors we would use:\n$$ h_{new} = 0.9 h_{current} \\bigg(\\frac{tol}{error} \\bigg) ^{\\frac{1}{5}} $$\nA Full RK45 Method Finally, we have all the parts to create a working 4th order Runge-Kutta numerical solver with adaptive step sizing based on 5th order errors.\nWe will start by writing a function that takes a single 5th order step. We are going to use the 5th order step to approximate our \u0026ldquo;true solution\u0026rdquo; and compare it to the 4th order step to find our error. The function will then return the 4th order step and the error. One reason this method is so good is because calculating the 5th step reuses a lot of the RK4 step calculations, making it very efficient.\nTo do this we will be using the Dormand-Prince coefficients. These are much messier than the standard coefficients we used before. However, they are very good at minimising the error in the 5th order approximation, which is exactly what we want \u0026ndash; we want that 5th order approximation to be as close to the real solution as possible.\nThe intermediate approximations are calculated as follows:\n$$ \\begin{aligned} \u0026amp;k_1 = hf(t_n, y_n) \\\\ {} \\\\ \u0026amp;k_2 = hf \\Big(t_n + \\frac{1}{5}h, y_n + \\frac{1}{5} k_1 \\Big) \\\\ {} \\\\ \u0026amp;k_3 = hf \\Big(t_n + \\frac{3}{10}h, y_n + \\frac{3}{40} k_1 + \\frac{9}{40} k_2 \\Big) \\\\ {} \\\\ \u0026amp;k_4 = hf \\Big(t_n + \\frac{4}{5}h, y_n + \\frac{44}{45} k_1 - \\frac{56}{15} k_2 + \\frac{32}{9} k_3 \\Big) \\\\ {} \\\\ \u0026amp;k_5 = hf \\Big(t_n + \\frac{8}{9}h, y_n + \\frac{19372}{6561} k_1 - \\frac{25360}{2187} k_2 + \\frac{64448}{6561} k_3 - \\frac{212}{729} k_4 \\Big) \\\\ {} \\\\ \u0026amp;k_6 = hf \\Big(t_n + h, y_n + \\frac{9017}{3168} k_1 - \\frac{355}{33} k_2 - \\frac{46732}{5247} k_3 + \\frac{49}{176} k_4 - \\frac{5103}{18656} k_5 \\Big) \\\\ {} \\\\ \u0026amp;k_7 = hf \\Big(t_n + h, y_n + \\frac{35}{848} k_1 + \\frac{500}{1113} k_3 + \\frac{125}{192} k_4 - \\frac{2187}{6784} k_5 + \\frac{11}{84} k_6 \\Big) \\end{aligned} $$\nThen, the next 4th order step is calculated as\n$$ y_{n+1} = y_n + \\frac{35}{384} k_1 + \\frac{500}{1113} k_3 + \\frac{125}{192} k_4 - \\frac{2187}{6784} k_5 + \\frac{11}{84} k_6 $$\nNote that these are the same coefficients that we use for \\( k_7 \\), which is useful. We can rewrite \\( k_7 \\) as\n$$ k_7 = hf \\Big( t_n + h, y_{n+1} \\Big) $$\nThis value is then the same as \\( k_1 \\) will be in the next step (except for \\( h \\) changing). This means that, except for the first step, we only have to calculate 6 derivatives per step.\nThen, the 5th order step, \\( z_{n+1} \\), is calculated as\n$$ z_{n+1} = y_n + \\frac{5179}{57600} k_1 + \\frac{7571}{16695} k_3 + \\frac{393}{640} k_4 - \\frac{92097}{339200} k_5 + \\frac{187}{2100} k_6 + \\frac{1}{40} k_7 $$\nThis gives the error\n$$ error = \\Big| z_{n+1} - y_{n+1} \\Big| = \\Big| -\\frac{71}{57600} k_1 + \\frac{71}{16695} k_3 - \\frac{71}{1920} k_4 + \\frac{17253}{339200} k_5 - \\frac{22}{525} k_6 + \\frac{1}{40} k_7 \\Big| $$\nand from earlier we know that our step size change will be\n$$ h_{new} = 0.9 h_{current} \\bigg(\\frac{tol}{error} \\bigg) ^{0.2} $$\nLet\u0026rsquo;s write this step function now, along with the function that adaptively steps to return the solution.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 # ----- Dormand-Prince coefficients ----- # # Alpha in notation, coefficients describe what percentage of the full # step we evaluate each derivative at A = np.array([0, 1/5, 3/10, 4/5, 8/9, 1, 1]) # Beta in notation, coefficients describe how much of each previous change in y # we add to the current estimate B = np.array([ [0, 0, 0, 0, 0, 0], [1/5, 0, 0, 0, 0, 0], [3/40, 9/40, 0, 0, 0, 0], [44/45, -56/15, 32/9, 0, 0, 0], [19372/6561, -25360/2187, 64448/6561, -212/729, 0, 0], [9017/3168, -355/33, 46732/5247, 49/176, -5103/18656, 0], [35/384, 0, 500/1113, 125/192, -2187/6784, 11/84] ]) # Weighting for each derivative in the approximation, w in notation W = np.array([35/384, 0, 500/1113, 125/192, -2187/6784, 11/84, 0]) #W = np.array([5179/57600, 0, 7571/16695, 393/640, -92097/339200, 187/2100, 1/40]) # Coefficients for error calculation E = np.array([-71/57600, 0, 71/16695, -71/1920, 17253/339200, -22/525, 1/40]) # ------------------------------------------------------------------ # # Define a function to take a single RK45 step # To improve efficiency, k7 can be reused as k1 in the following step def rk45_step(dydt, t0, y0, h): \u0026#34;\u0026#34;\u0026#34; Takes a single 5th order integration step and returns the 4th order step along with the 5th order error. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y h: Step size Returns: The the 4th order y value along with the 5th order error. \u0026#34;\u0026#34;\u0026#34; k1 = h * dydt(t0, y0) k2 = h * dydt(t0 + A[1] * h, y0 + B[1][0]*k1) k3 = h * dydt(t0 + A[2] * h, y0 + B[2][0]*k1 + B[2][1]*k2) k4 = h * dydt(t0 + A[3] * h, y0 + B[3][0]*k1 + B[3][1]*k2 + B[3][2]*k3) k5 = h * dydt(t0 + A[4] * h, y0 + B[4][0]*k1 + B[4][1]*k2 + B[4][2]*k3 + B[4][3]*k4) k6 = h * dydt(t0 + A[5] * h, y0 + B[5][0]*k1 + B[5][1]*k2 + B[5][2]*k3 + B[5][3]*k4 + B[5][4]*k5) y = y0 + W[0]*k1 + W[1]*k2 + W[2]*k3 + W[3]*k4 + W[4]*k5 + W[5]*k6 k7 = h * dydt(t0 + A[6] * h, y) error = np.abs( E[0]*k1 + E[1]*k2 + E[2]*k3 + E[3]*k4 + E[4]*k5 + E[5]*k6 + E[6]*k7 ) return y, error # Define the full RK45 solver function def rk45_solver(dydt, t0, y0, t_end, tol): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the RK45 method. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y t_end: Final time for integration tol: Error tolerance Returns: Arrays of y, t and local error values for each time step \u0026#34;\u0026#34;\u0026#34; h = 1e-3 # set some initial step size t = [t0] y = [y0] error = [0] i = 1 while t[-1] \u0026lt; t_end: # Take a step t.append(t[i-1] + h) y_step, error_step = rk45_step(dydt, t[i-1], y[i-1], h) y.append(y_step) error.append(error_step) # Update step size after step h = 0.9 * h * (tol / error[i]) ** 0.2 isStepGood = False if error[i] \u0026lt; tol: # accept step isStepGood = True while not isStepGood: # If there was too much error... # Take a step t[i] = t[i-1] + h # update our time value with the new step size y_step, error_step = rk45_step(dydt, t[i-1], y[i-1], h) y[i] = y_step error[i] = error_step # Update step size after step h = 0.9 * h * (tol / error[i]) ** 0.2 # Check error to accept or reject the step if error[i] \u0026lt; tol: isStepGood = True i += 1 return t, y, error Using this step function along with updating the step size gives the result in Figure 11.\nFigure 11: RK45 Solver with error.\nFigure 11 code (full solver code + graphing)\r1\u0026#39;\u0026#39;\u0026#39; 2Implementing a complete RK45 algorithm. 3 4Created by: simmeon 5Last Modified: 19/05/24 6License: MIT 7 8\u0026#39;\u0026#39;\u0026#39; 9 10import matplotlib.pyplot as plt 11import numpy as np 12 13plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) 14 15# Define function (for analytical solution) and its derivative 16def dydt(t, y): 17 return 5*t**4 * np.cos(t**5) 18 19def analytical_solution(t): 20 return np.sin(t**5) 21 22 23# ----- Dormand-Prince coefficients ----- # 24 25# Alpha in notation, coefficients describe what percentage of the full 26# step we evaluate each derivative at 27A = np.array([0, 1/5, 3/10, 4/5, 8/9, 1, 1]) 28 29# Beta in notation, coefficients describe how much of each previous change in y 30# we add to the current estimate 31B = np.array([ 32 [0, 0, 0, 0, 0, 0], 33 [1/5, 0, 0, 0, 0, 0], 34 [3/40, 9/40, 0, 0, 0, 0], 35 [44/45, -56/15, 32/9, 0, 0, 0], 36 [19372/6561, -25360/2187, 64448/6561, -212/729, 0, 0], 37 [9017/3168, -355/33, 46732/5247, 49/176, -5103/18656, 0], 38 [35/384, 0, 500/1113, 125/192, -2187/6784, 11/84] 39 ]) 40 41# Weighting for each derivative in the approximation, w in notation 42W = np.array([35/384, 0, 500/1113, 125/192, -2187/6784, 11/84, 0]) 43#W = np.array([5179/57600, 0, 7571/16695, 393/640, -92097/339200, 187/2100, 1/40]) 44 45# Coefficients for error calculation 46E = np.array([-71/57600, 0, 71/16695, -71/1920, 17253/339200, -22/525, 1/40]) 47 48# ------------------------------------------------------------------ # 49 50# Define a function to take a single RK45 step 51# To improve efficiency, k7 can be reused as k1 in the following step 52def rk45_step(dydt, t0, y0, h): 53 \u0026#34;\u0026#34;\u0026#34; 54 Takes a single 5th order integration step and returns the 55 4th order step along with the 5th order error. 56 57 Args: 58 dydt: The differential equation (dy/dt) as a Python function 59 t0: Initial value of time 60 y0: Initial value of y 61 h: Step size 62 63 Returns: 64 The the 4th order y value along with the 5th order error. 65 66 \u0026#34;\u0026#34;\u0026#34; 67 68 k1 = h * dydt(t0, y0) 69 k2 = h * dydt(t0 + A[1] * h, y0 + B[1][0]*k1) 70 k3 = h * dydt(t0 + A[2] * h, y0 + B[2][0]*k1 + B[2][1]*k2) 71 k4 = h * dydt(t0 + A[3] * h, y0 + B[3][0]*k1 + B[3][1]*k2 + B[3][2]*k3) 72 k5 = h * dydt(t0 + A[4] * h, y0 + B[4][0]*k1 + B[4][1]*k2 + B[4][2]*k3 + B[4][3]*k4) 73 k6 = h * dydt(t0 + A[5] * h, y0 + B[5][0]*k1 + B[5][1]*k2 + B[5][2]*k3 + B[5][3]*k4 + B[5][4]*k5) 74 75 y = y0 + W[0]*k1 + W[1]*k2 + W[2]*k3 + W[3]*k4 + W[4]*k5 + W[5]*k6 76 77 k7 = h * dydt(t0 + A[6] * h, y) 78 79 error = np.abs( E[0]*k1 + E[1]*k2 + E[2]*k3 + E[3]*k4 + E[4]*k5 + E[5]*k6 + E[6]*k7 ) 80 81 return y, error 82 83# Define the full RK45 solver function 84def rk45_solver(dydt, t0, y0, t_end, tol): 85 \u0026#34;\u0026#34;\u0026#34; 86 Solves a first-order ODE using the RK45 method. 87 88 Args: 89 dydt: The differential equation (dy/dt) as a Python function 90 t0: Initial value of time 91 y0: Initial value of y 92 t_end: Final time for integration 93 tol: Error tolerance 94 95 Returns: 96 Arrays of y, t and local error values for each time step 97 \u0026#34;\u0026#34;\u0026#34; 98 h = 1e-3 # set some initial step size 99 t = [t0] 100 y = [y0] 101 error = [0] 102 103 i = 1 104 while t[-1] \u0026lt; t_end: 105 # Take a step 106 t.append(t[i-1] + h) 107 y_step, error_step = rk45_step(dydt, t[i-1], y[i-1], h) 108 y.append(y_step) 109 error.append(error_step) 110 111 # Update step size after step 112 h = 0.9 * h * (tol / error[i]) ** 0.2 113 114 isStepGood = False 115 116 if error[i] \u0026lt; tol: 117 # accept step 118 isStepGood = True 119 120 while not isStepGood: 121 # If there was too much error... 122 123 # Take a step 124 t[i] = t[i-1] + h # update our time value with the new step size 125 y_step, error_step = rk45_step(dydt, t[i-1], y[i-1], h) 126 y[i] = y_step 127 error[i] = error_step 128 129 # Update step size after step 130 h = 0.9 * h * (tol / error[i]) ** 0.2 131 132 # Check error to accept or reject the step 133 if error[i] \u0026lt; tol: 134 isStepGood = True 135 136 i += 1 137 138 return t, y, error 139 140 141# Define simulation parameters 142t0 = 0 143y0 = 0 144t_end = 2 145tol = 1e-6 146 147# Numerically integrate 148t_rk45, y_rk45, local_error = rk45_solver(dydt, t0, y0, t_end, tol) 149 150t_rk45 = np.array(t_rk45) 151y_rk45 = np.array(y_rk45) 152 153# Get analytical solution 154t_analytical = np.arange(t0, t_end, 0.001) 155y_analytical = analytical_solution(t_analytical) 156 157# Get error 158y_analytical_compare = analytical_solution(t_rk45) 159error = np.abs(y_rk45 - y_analytical_compare) 160 161# Plotting 162plt.subplot(3,1,1) 163plt.plot(t_analytical, y_analytical, label=\u0026#39;Analytical\u0026#39;, linestyle=\u0026#39;-\u0026#39;) 164plt.plot(t_rk45, y_rk45, label=\u0026#39;RK45 method\u0026#39;) 165#plt.scatter(t_rk45, y_rk45, s=30, marker=\u0026#39;o\u0026#39;, facecolors=\u0026#39;none\u0026#39;, color=\u0026#39;C1\u0026#39;) 166 167plt.title(f\u0026#34;RK45 Solver, tol = {tol}\u0026#34;, fontsize=20) 168plt.ylabel(\u0026#39;y\u0026#39;, fontsize=20) 169plt.legend(fontsize=15) 170plt.text(0, -0.75, f\u0026#34;Number of steps: {len(t_rk45)}\u0026#34;, fontsize=15) 171 172plt.subplot(3,1,2) 173plt.plot(t_rk45, error) 174plt.ylabel(\u0026#34;absolute error\u0026#34;, fontsize=20) 175 176plt.subplot(3,1,3) 177plt.plot(t_rk45, local_error) 178plt.ylabel(\u0026#34;local step error\u0026#34;, fontsize=20) 179plt.xlabel(\u0026#39;t\u0026#39;, fontsize=20) 180 181plt.show() As we can see, the method is incredibly efficient and accurate. In comparison, our 1st order adaptive method used over 200 steps for a tolerance of only 1e-03. This combination of efficiency and accuracy makes the RK45 method an incredibly good general solver.\nA Final Application So far we have used mostly trivial or contrived examples to demonstrate why the RK45 solver is good. To finish off, we will use a real-world example where the adaptability, efficiency, and accuracy of the RK45 solver is very useful.\nWe will be using the solver to propagate a satellite orbit around the earth. We will skip some details of deriving the relevant physics, but the important equation is:\n$$ F = \\frac{G m_{earth} m_{sat}}{r^2} $$\nLet\u0026rsquo;s define the gravitational parameter, \\( \\mu \\)\n$$ \\mu = G(m_{earth} + m_{sat}) $$\nBy assuming an inertial reference frame centred on the Earth, we get\n$$ \\ddot{\\vec{r}} = - \\frac{G(m_{earth} + m_{sat})}{r^3}\\vec{r} \\\\ {} \\\\ \\ddot{\\vec{r}} = - \\frac{\\mu}{r^3}\\vec{r} $$\nwhere \\( \\vec{r} \\) is the position vector of the satellite we want to solve for.\nThis is currently a second order ODE. We can use a state space to transform it into being first order:\n$$ \\bold{x} = \\begin{bmatrix} r_x \\\\ r_y \\\\ \\dot{r}_x \\\\ \\dot{r}_y \\end{bmatrix} \\\\ {} \\\\ {} \\\\ \\bold{\\dot{x}} = \\begin{bmatrix} \\dot{r}_x \\\\ {} \\\\ \\dot{r}_y \\\\ {} \\\\ \\ddot{r}_x \\\\ {} \\\\ \\ddot{r}_y \\\\ \\end{bmatrix} = \\begin{bmatrix} \\dot{r}_x \\\\ {} \\\\ \\dot{r}_y \\\\ {} \\\\ -\\Large \\frac{\\mu r_x}{|r|^3} \\\\ {} \\\\ -\\Large \\frac{\\mu r_y}{|r|^3} \\\\ \\end{bmatrix} \\\\ {} \\\\ {} \\\\ \\bold{\\dot{x}} = \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ -\\Large \\frac{\\mu}{|r|^3} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; -\\Large \\frac{\\mu}{|r|^3} \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} \\bold{x} $$\nThis is now a system of first order ODEs where we are solving for \\( \\bold{x} \\). With some small code changes to allow for vector inputs, Figure 12 shows the resulting satellite position and velocity for some initial state. To deal with multiple ODE systems with arrays of solutions instead of just a single value, we use the maximum error as our test for whether the step was good or not.\nFigure 12: Propagating an orbit with RK45.\nFigure 12 code\r1\u0026#39;\u0026#39;\u0026#39; 2An application of the RK45 ODE solver. We will solve for the position 3of a satellite around Earth given an initial position and velocity. 4 5Created by: simmeon 6Last Modified: 19/05/24 7License: MIT 8 9\u0026#39;\u0026#39;\u0026#39; 10 11import matplotlib.pyplot as plt 12import numpy as np 13 14plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) 15 16# Define derivative 17def dxdt(t, x): 18 r_mag = np.sqrt(x[0]**2 + x[1]**2) 19 mu = 398600.4415; # [km^3 / (kg*s^2)] 20 21 return np.array([x[2], x[3], -mu * x[0] / (r_mag**3), -mu * x[1] / (r_mag**3)]) 22 23 24 25# ----- Dormand-Prince coefficients ----- # 26 27# Alpha in notation, coefficients describe what percentage of the full 28# step we evaluate each derivative at 29A = np.array([0, 1/5, 3/10, 4/5, 8/9, 1, 1]) 30 31# Beta in notation, coefficients describe how much of each previous change in y 32# we add to the current estimate 33B = np.array([ 34 [0, 0, 0, 0, 0, 0], 35 [1/5, 0, 0, 0, 0, 0], 36 [3/40, 9/40, 0, 0, 0, 0], 37 [44/45, -56/15, 32/9, 0, 0, 0], 38 [19372/6561, -25360/2187, 64448/6561, -212/729, 0, 0], 39 [9017/3168, -355/33, 46732/5247, 49/176, -5103/18656, 0], 40 [35/384, 0, 500/1113, 125/192, -2187/6784, 11/84] 41 ]) 42 43# Weighting for each derivative in the approximation, w in notation 44W = np.array([35/384, 0, 500/1113, 125/192, -2187/6784, 11/84, 0]) 45#W = np.array([5179/57600, 0, 7571/16695, 393/640, -92097/339200, 187/2100, 1/40]) 46 47# Coefficients for error calculation 48E = np.array([-71/57600, 0, 71/16695, -71/1920, 17253/339200, -22/525, 1/40]) 49 50# ------------------------------------------------------------------ # 51 52# Define a function to take a single RK45 step 53# To improve efficiency, k7 can be reused as k1 in the following step 54def rk45_step(dydt, t0, y0, h): 55 \u0026#34;\u0026#34;\u0026#34; 56 Takes a single 5th order integration step and returns the 57 4th order step along with the 5th order error. 58 59 Args: 60 dydt: The differential equation (dy/dt) as a Python function 61 t0: Initial value of time 62 y0: Initial value of y 63 h: Step size 64 65 Returns: 66 The the 4th order y value along with the 5th order error. 67 68 \u0026#34;\u0026#34;\u0026#34; 69 70 k1 = h * dydt(t0, y0) 71 k2 = h * dydt(t0 + A[1] * h, y0 + B[1][0]*k1) 72 k3 = h * dydt(t0 + A[2] * h, y0 + B[2][0]*k1 + B[2][1]*k2) 73 k4 = h * dydt(t0 + A[3] * h, y0 + B[3][0]*k1 + B[3][1]*k2 + B[3][2]*k3) 74 k5 = h * dydt(t0 + A[4] * h, y0 + B[4][0]*k1 + B[4][1]*k2 + B[4][2]*k3 + B[4][3]*k4) 75 k6 = h * dydt(t0 + A[5] * h, y0 + B[5][0]*k1 + B[5][1]*k2 + B[5][2]*k3 + B[5][3]*k4 + B[5][4]*k5) 76 77 y = y0 + W[0]*k1 + W[1]*k2 + W[2]*k3 + W[3]*k4 + W[4]*k5 + W[5]*k6 78 79 k7 = h * dydt(t0 + A[6] * h, y) 80 81 error = np.abs( E[0]*k1 + E[1]*k2 + E[2]*k3 + E[3]*k4 + E[4]*k5 + E[5]*k6 + E[6]*k7 ) 82 83 return y, error 84 85# Define the full RK45 solver function 86def rk45_solver(dydt, t0, y0, t_end, tol): 87 \u0026#34;\u0026#34;\u0026#34; 88 Solves a first-order ODE using the RK45 method. 89 90 Args: 91 dydt: The differential equation (dy/dt) as a Python function 92 t0: Initial value of time 93 y0: Initial value of y 94 t_end: Final time for integration 95 tol: Error tolerance 96 97 Returns: 98 Arrays of y, t and local error values for each time step 99 \u0026#34;\u0026#34;\u0026#34; 100 h = 1e-3 # set some initial step size 101 t = [t0] 102 y = np.array([y0]) 103 error = [0] 104 105 i = 1 106 while t[-1] \u0026lt; t_end: 107 # Take a step 108 t.append(t[i-1] + h) 109 y_step, error_step = rk45_step(dydt, t[i-1], y[i-1], h) 110 y = np.append(y, np.array([y_step]), axis=0) 111 error.append(error_step) 112 113 # Update step size after step 114 h = 0.9 * h * (tol / max(error[i])) ** 0.2 115 116 isStepGood = False 117 118 if max(error[i]) \u0026lt; tol: 119 # accept step 120 isStepGood = True 121 122 while not isStepGood: 123 # If there was too much error... 124 125 # Take a step 126 t[i] = t[i-1] + h # update our time value with the new step size 127 y_step, error_step = rk45_step(dydt, t[i-1], y[i-1], h) 128 y[i] = y_step 129 error[i] = error_step 130 131 # Update step size after step 132 h = 0.9 * h * (tol / max(error[i])) ** 0.2 133 134 # Check error to accept or reject the step 135 if max(error[i]) \u0026lt; tol: 136 isStepGood = True 137 138 i += 1 139 140 return t, y, error 141 142 143# Define orbit parameters 144mu = 398600.4415 145rp = 6678 146e = 0.9 147a = rp/(1-e) 148T = 2*np.pi*np.sqrt(a**3/mu) 149 150# Define simulation parameters 151t0 = 0 152x0 = np.array([rp, 0, 0, np.sqrt(2*mu/rp - mu/a)]) 153t_end = T 154tol = 1e-12 155 156# Numerically integrate 157t_rk45, y_rk45, local_error = rk45_solver(dxdt, t0, x0, t_end, tol) 158 159y_rk45 = np.array(y_rk45) 160 161# Plotting 162plt.subplot(2,1,1) 163plt.plot(y_rk45[:, 0], y_rk45[:, 1]) 164plt.axis(\u0026#39;equal\u0026#39;) 165plt.scatter(0, 0, s=50) # dot for Earth 166plt.title(\u0026#34;Satellite Orbit Around Earth\u0026#34;, fontsize=20) 167plt.ylabel(\u0026#39;y [km]\u0026#39;, fontsize=20) 168plt.xlabel(\u0026#39;x [km]\u0026#39;, fontsize=20) 169 170plt.subplot(2,1,2) 171plt.plot(t_rk45, np.sqrt(y_rk45[:, 2]**2 + y_rk45[:, 3]**2), color=\u0026#39;C1\u0026#39;) 172plt.title(\u0026#34;Satellite Velocity\u0026#34;, fontsize=20) 173plt.ylabel(\u0026#39;velocity [km/s]\u0026#39;, fontsize=20) 174plt.xlabel(\u0026#39;t [s]\u0026#39;, fontsize=20) 175 176plt.show() The adaptive step size is able to deal with velocity changing by orders of magnitude. With a constant step size, we would be wasting a huge amount of computation on the far side of the orbit. The accuracy of the RK45 method also allows us to use very small tolerances (1e-12) and still have the computation time be relatively quick.\nConclusion This guide should serve as a single comprehensive reference for what the RK45 method is, how it is derived, and how and why we use it to solve engineering problems. There is still room to improve on the ideas and code that have been shown here (the code lacks a lot of polish and dealing with edge cases for example).\nHowever, for those like me who wanted to understand where this method comes from and why it is so good, I hope this has been a useful reference. The goal was to make these quite dense mathematical concepts as clear as possible for other engineers like me.\nReferences [1] Roson J. S., \u0026ldquo;The Runge-Kutta Equations by Quadrature Methods\u0026rdquo;, 1967, NASA.\n[2] \u0026ldquo;Runge-Kutta Methods\u0026rdquo;, 10.001: Numerical Solution of Ordinary Differential Equations\n[3] Explanation and proof of the 4th order Runge-Kutta method\n[4] Numerical Recipes in C: The Art of Scientific Computing\n[5] S. Brorson, \u0026ldquo;Numerically Solving Ordinary Differential Equations\u0026rdquo;\n[6] Adaptive Runge-Kutta Methods | Lecture 54 | Numerical Methods for Engineers\n[7] Dormand, J. R. and P. J. Prince, A family of embedded Runge-Kutta formulae, J. Comp. Appl. Math., Vol. 6, 1980, pp. 1926.\n[8] Dormand-Prince Method\n","permalink":"http://localhost:1313/blog/posts/rk45/rk45/","summary":"The Initial Value Problem In engineering, we often encounter systems that evolve over time, such as circuits, mechanical systems, or chemical reactions. These systems are best described using differential equations. For example, Newton\u0026rsquo;s law of cooling states:\n$$ \\dfrac{dT}{dt} = -k(T - T_{surr}) $$\nwhere T is the temperature of some point, k is a proportionality constant, and Tsurr is the temperature surrounding the point of interest.\nTo solve this equation is to find a solution for the temperature, T, over time.","title":"An Engineer's Guide to the Runge-Kutta (RK45) Method"}]