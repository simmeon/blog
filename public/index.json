[{"content":"\rWhat is this all about? A frequency response gives information about how an output of a system will react to a sinusoidal input at different frequencies. They are very useful in analysing the stability and behaviour of systems. They are also central to frequency-domain system identification methods, which is what we will be interested in using them for.\nIn general, for some system input \\( x(t) \\) and output \\( y(t) \\), the frequency response is\n$$ H(f) = \\frac{Y(f)}{X(f)} $$\nwhere \\( X(f) \\) and \\( Y(f) \\) are the Fourier transforms of the input and output.\nBut this is not always true. In systems where there are multiple partially correlated inputs and those inputs all affect the output, we have to be more careful about how we calculate these frequency responses. In other words, we have to condition the frequency responses to remove these effects.\nThroughout this post, we will develop a very useful way to calculate frequency responses (especially for system identification) using spectral density functions. Then, we will apply this method in single-input single-output (SISO) systems. We will build up some theory on how to describe these correlation effects in multiple-input single-output (MISO) systems and see why the above equation for frequency responses is lacking. Finally, we will discuss how to change our method so we are still able to find the frequency responses we want.\nStationary Random Processes We are mostly going to be interested in frequency responses for the purpose of system identification. That means we will have real measured data for \\( x(t) \\) and \\( y(t) \\). If we then want to find the frequency response, you may first think to just follow the earlier equation and Fourier transform both of these.\nAnd that would work! But it\u0026rsquo;s important to remember that we\u0026rsquo;re working with real-world measured data here, and real-world data always has a lot of noise, disturbances, and other issues that make our clean theoretical equations less useful. Take the following example in Figure 1:\nFigure 1: Seeing how noisy data can affect direct calculation of the frequency response from Fourier transforms.\nFigure 1 code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 \u0026#39;\u0026#39;\u0026#39; Seeing how noisy data can affect direct calculation of the frequency response from Fourier transforms. Created by: simmeon Last Modified: 2025-01-25 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np from scipy.integrate import odeint from scipy.fft import rfft, rfftfreq plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) pi = np.pi # Let\u0026#39;s set up a simple spring, mass, damper system to mess around with. fn = 1 # natural frequency of system [Hz] zeta = 0.05 # system damping ratio wn = 2 * pi * fn # natural freq [rad/s] # We can find the analytical transfer function to get the true frequency response def H_func(f): s = 1.0j * 2 * pi * f return wn**2 / (s**2 + 2 * zeta * wn * s + wn**2) fmin = 0.1 # min frequency in response fmax = 10 # max frequency in response f_analytical = np.logspace(np.log10(fmin), np.log10(fmax), 500) # array of freqs to calculate response for H_analytical = H_func(f_analytical) # Frequency response mag_analytical = 20 * np.log10(abs(H_analytical)) # Magnitude of response (in dB) phase_analytical = np.rad2deg(np.angle(H_analytical)) # Phase of response (in deg) # Now let\u0026#39;s create some noisy input and output data to simulate measurements # To do this we will solve the system with a frequency sweep over the range of interest (0.1 - 10 Hz) T = 20 # period to simulate system over [s] # Define our input function, a frequency sweep # NOTE: to get a correct frequency response over all frequencies of interest, the input must # necessarily contaion information on all the frequencies you are interested in. def u_func(t): f = t / T * fmax * 2 return 10 * np.sin(2 * pi * f * t) # Define the derivative function of the system (state space derivative) def dxdt(x, t): u = u_func(t) return np.array([x[1], - wn**2 * x[0] - 2 * zeta * wn * x[1] + wn**2 * u]) # Solve system N = 5000 # number of sample points t = np.linspace(0, T, N) # time array x0 = np.array([0, 0]) # initial state conditions sol = odeint(dxdt, x0, t) y = sol[:, 0] # pull out position from state as our output x = u_func(t) # get array of inputs for use later # Now we will add some output noise to simulate noisy measurements y = y + np.random.normal(0, 2, len(y)) # Then, we can fourier transform our input and output to calculate the freqency response Y = rfft(y) X = rfft(x) f_estimated = rfftfreq(N, T / N) H_estimated = Y / X # Estimated freq response from noisy data mag_estimated = 20 * np.log10(abs(H_estimated)) # Magnitude of response (in dB) phase_estimated = np.rad2deg(np.angle(H_estimated)) # Phase of response (in deg) # Plotting plt.figure(figsize=(12, 8)) # Subplot 1: Time domain response plt.subplot(3, 1, 1) plt.plot(t, y) plt.title(\u0026#34;System Response (y) over Time\u0026#34;, fontsize=20) plt.xlabel(\u0026#34;Time (s)\u0026#34;, fontsize=18) plt.ylabel(\u0026#34;Output (y)\u0026#34;, fontsize=18) plt.grid(True) plt.tick_params(axis=\u0026#39;both\u0026#39;, which=\u0026#39;major\u0026#39;, labelsize=16) # Subplot 2: Bode magnitude plot plt.subplot(3, 1, 2) plt.semilogx(f_estimated, mag_estimated, label=\u0026#34;Estimated\u0026#34;, linewidth=2) plt.semilogx(f_analytical, mag_analytical, label=\u0026#34;Analytical\u0026#34;, linestyle=\u0026#39;--\u0026#39;, linewidth=2) plt.title(\u0026#34;Bode Magnitude Plot\u0026#34;, fontsize=20) plt.ylabel(\u0026#34;Magnitude (dB)\u0026#34;, fontsize=18) plt.legend(fontsize=16) plt.grid(True, which=\u0026#39;both\u0026#39;, axis=\u0026#39;both\u0026#39;) plt.xscale(\u0026#39;log\u0026#39;) plt.xlim((fmin, fmax)) plt.tick_params(axis=\u0026#39;both\u0026#39;, which=\u0026#39;major\u0026#39;, labelsize=12) # Subplot 3: Bode phase plot plt.subplot(3, 1, 3) plt.semilogx(f_estimated, phase_estimated, label=\u0026#34;Estimated\u0026#34;, linewidth=2) plt.semilogx(f_analytical, phase_analytical, label=\u0026#34;Analytical\u0026#34;, linestyle=\u0026#39;--\u0026#39;, linewidth=2) plt.title(\u0026#34;Bode Phase Plot\u0026#34;, fontsize=20) plt.ylabel(\u0026#34;Phase (Degrees)\u0026#34;, fontsize=18) plt.xlabel(\u0026#34;Frequency (Hz)\u0026#34;, fontsize=18) plt.legend(fontsize=16) plt.grid(True, which=\u0026#39;both\u0026#39;, axis=\u0026#39;both\u0026#39;) plt.xscale(\u0026#39;log\u0026#39;) plt.xlim((fmin, fmax)) plt.tick_params(axis=\u0026#39;both\u0026#39;, which=\u0026#39;major\u0026#39;, labelsize=12) plt.tight_layout() plt.show() While the frequency response estimate is still pretty good where it matters, you can see how the noise starts to introduce larger errors as the frequency increases. What would be nice is a way of somehow modelling this noise and creating methods that are able to account for noise and let us minimise the impact of it.\nTo do this we will turn to some statistical methods and see where they can take us\u0026hellip;\nBasics First, let\u0026rsquo;s look at the concept of a stationary random process. A random process meaning some function whose value at any given time is random. Each time index can be described by a random variable. This is good as our measurement noise can be modelled by these random variables. The random process is stationary if the probability density function at any time is the same.\nLet\u0026rsquo;s look at an example to help. In Figure 2, we are using random normal noise as our random variable at each time \\( t_{i} \\) to generate a \u0026lsquo;sample function\u0026rsquo;, \\( x_{1}(t) \\), of the random process.\nIf we do this multiple times to create lots of sample functions \\( x_{k}(t), k = 1, 2, \u0026hellip; \\), we can then estimate the probability density function of the random variable at a particular time \\( t_{i} \\). For example, if we focus on time \\( t = 0 \\), we could estimate the probability density function \\( f_{X}(x) \\) by counting how many sample functions had a value \\( x_{k}(0) \\) within certain ranges. Eg. how many had a value between 0 and 0.5? Between 0.5 and 1? Between -2 and -1.5?\nBasically we are creating a histogram of all the \\( x_{k}(0) \\) values. We can then do the same thing for every time index \\( t_{i} \\).\nFigure 2: Exploring what a stationary random process is and some useful features of it.\nFigure 2 code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 \u0026#39;\u0026#39;\u0026#39; Exploring what a stationary random process is and some useful features of it. Created by: simmeon Last Modified: 2025-01-25 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np from scipy.ndimage import histogram plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) # We will want to store each \u0026#39;sample function\u0026#39; in an array x = [] N = 50 # sample length T = 10 # time period sample is over t = np.linspace(0, T, N) num_samples = 100 # number of sample functions to run for i in range(num_samples): # The stationary random process we will use is random normal noise x.append(np.random.normal(0, 1, N)) # We can then estimate the probability density function at a particular time (t) # by counting how many of the sample functions have values within a particular # range at that time. ti_data = np.zeros((N, num_samples)) pdfs = [] num_bins = 20 pdf_vals = np.linspace(-5, 5, num_bins) for i in range(N): for j in range(num_samples): ti_data[i, j] = (x[j][i]) pdfs.append(histogram(ti_data[i], -5, 5, num_bins) / num_samples) # Plotting fig= plt.figure() ax1 = fig.add_subplot(2, 2, 1) ax2 = fig.add_subplot(2, 2, 3) ax3 = fig.add_subplot(2, 2, 2) ax4 = fig.add_subplot(2, 2, 4) ax1.set_title(\u0026#34;One Sample Function\u0026#34;, fontsize=20) ax1.set_xlabel(\u0026#34;t\u0026#34;, fontsize=20) ax1.set_ylabel(\u0026#34;x(t)\u0026#34;, fontsize=20) for i in range(1): ax1.plot(t, x[i], alpha=1.0) ax1.set_ylim((-4, 4)) ax2.set_title(r\u0026#34;Probability Density Function for $t = 0$\u0026#34;, fontsize=20) ax2.set_xlabel(r\u0026#34;$x(t = 0$)\u0026#34;, fontsize=20) ax2.set_ylabel(r\u0026#34;$f_{X}(x)$\u0026#34;, fontsize=20) for i in range(1): ax2.plot(pdf_vals, pdfs[i], alpha=1.0) ax2.set_ylim((0, 0.3)) ax3.set_title(\u0026#34;All Sample Functions\u0026#34;, fontsize=20) ax3.set_xlabel(\u0026#34;t\u0026#34;, fontsize=20) ax3.set_ylabel(\u0026#34;x(t)\u0026#34;, fontsize=20) for i in range(num_samples): ax3.plot(t, x[i], alpha=0.05) ax3.plot(t, x[0], color=\u0026#39;yellow\u0026#39;) ax3.set_ylim((-4, 4)) ax4.set_title(r\u0026#34;Probability Density Functions for each $t_{i}$\u0026#34;, fontsize=20) ax4.set_xlabel(r\u0026#34;$x(t_{i}$)\u0026#34;, fontsize=20) ax4.set_ylabel(r\u0026#34;$f_{X}(x)$\u0026#34;, fontsize=20) for i in range(N): ax4.plot(pdf_vals, pdfs[i], alpha=0.1) ax4.plot(pdf_vals, pdfs[0], alpha=0.1) ax4.set_ylim((0, 0.3)) plt.show() Something that stands out is how the probability density function for every time index looks about the same. And, in fact, if we had worked it out analytically instead of estimating it, we would see that it is exactly the same probability density function for every time. This is the key feature that makes our random process a stationary random process.\nThis is really useful because it allows us to model a function over time in a way that doesn\u0026rsquo;t care about what specific time we are looking at. All the information about what value the function may be at any given time is encapsulated by a single probability density function that\u0026rsquo;s independant of time.\nBut why is this going to be useful? What can we do with this? And how does it relate to calculating frequency responses?\nYou might be starting to think that if we assume our input and output records are stationary random processes, we might be able to do some fancy statistical manipulation with them. Which is exactly what we will try do next\u0026hellip;\nCorrelation Functions What kinds of things can we do with these stationary random processes? Let\u0026rsquo;s use a different, simpler example to help.\nTake the following digital signal which can either be 1 V or 0 V to represent either a 1 or 0 bit. Each bit has a length of \\( T \\) and each sample function may be offset from others. We will also assume it is random whether we see a 1 V or 0 V at any time. Figure 3 has some example sample functions of this signal.\nFigure 3: Some sample functions of our digital signal stationary random process.\nFigure 3 code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 \u0026#39;\u0026#39;\u0026#39; Some sample functions of our digital signal stationary random process. Created by: simmeon Last Modified: 2025-01-25 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) num_bits = 10 # Calculate random series of bits bits1 = np.random.randint(0, 2, num_bits) bits2 = np.random.randint(0, 2, num_bits) bits3 = np.random.randint(0, 2, num_bits) N = 100 # Number of sample points per bit # Make length of each bit 1 s t = np.linspace(0, num_bits, num_bits * N) # sample functions x1 = [] x2 = [] x3 = [] # Make sample function have N points per bit and add offsets for bit in bits1: x1 = x1 + [bit] * N for bit in bits2: x2 = x2 + [bit] * N x2 = x2[35:] + x2[:35] # offset for bit in bits3: x3 = x3 + [bit] * N x3 = x3[60:] + x3[:60] # offset # Plotting fig = plt.figure() ax1 = fig.add_subplot(3,1,1) ax2 = fig.add_subplot(3,1,2) ax3 = fig.add_subplot(3,1,3) ax1.set_title(\u0026#39;Digital Signal Sample Functions\u0026#39;, fontsize=20) ax1.set_xticks(np.arange(0, 11, 1)) ax2.set_xticks(np.arange(0, 11, 1)) ax3.set_xticks(np.arange(0, 11, 1)) ax1.grid() ax2.grid() ax3.grid() ax1.grid(visible=True, axis=\u0026#39;x\u0026#39;, which=\u0026#39;both\u0026#39;) ax2.grid(visible=True, axis=\u0026#39;x\u0026#39;, which=\u0026#39;both\u0026#39;) ax3.grid(visible=True, axis=\u0026#39;x\u0026#39;, which=\u0026#39;both\u0026#39;) ax1.plot(t, x1) ax2.plot(t, x2) ax3.plot(t, x3) ax2.set_ylabel(\u0026#39;Voltage (V)\u0026#39;, fontsize=20) ax3.set_xlabel(\u0026#39;Time (s)\u0026#39;, fontsize=20) plt.show() Hopefully you can convince yourself that the probability density function of this random process will be the same at any time: in half the samples we will get a 1 and the other half we will get a 0. So this is another example of a stationary random process.\nOne fairly simple thing we could look at is the mean (or expected value) of this stationary random process. In more formal terms, we would use the expectation operator:\n$$ \\text{mean} = E\\{x_{k}(t)\\} $$\nSince the probability density function is the same for all time, we can also think of the expected value as the integral of the probability density function over all possible values the random variable can be. Or, more simply, the area under the probability density function.\n$$ E\\{x(t)\\} = \\int_{-\\infty}^{\\infty}{x f_{X}(x)} dx $$\nIn our digital signal example, it\u0026rsquo;s pretty simple to calculate this expected value since we only have two discrete values the random variable can be: 0 and 1. And each has a 50 % probability, giving\n$$ \\text{mean} = 0 \\times 0.5 + 1 \\times 0.5 = 0.5 \\text{ V} $$\nBuilding on this idea, we might decide to put some other quantity in this expectation operator. For example,\n$$ E\\{x(t) x(t + \\tau) \\} $$\nwhere \\( \\tau \\) here is acting as a time offset. Now we know that since this is a stationary random process, the probabilty densit function is time independant, so the only variable in this new function is \\( \\tau \\).\nLet\u0026rsquo;s name this new function the correlation function, for reasons we should see soon.\n$$ R_{xx}(\\tau) = E\\{x(t) x(t + \\tau) \\} $$\nCalculating some examples should help see what this correlation function tells us. Let\u0026rsquo;s start with the simplest case where \\( \\tau = 0 \\). That means we will be calculating the expected value\n$$ E\\{x(t) x(t) \\} $$\nwhere again, since this our probability density function is independant of time, means we can choose any time to evaluate this at. Referring back to Figure 3 let\u0026rsquo;s look at the purple line where \\( t = 7 \\).\nThe value of the random variable \\( x(t = 7) \\) will be 1 half the time and 0 half the time. So when multiplying it with itself and taking the expected value we will get:\n$$ R_{xx}(\\tau = 0) = (1 \\times 1) \\times 0.5 + (0 \\times 0) \\times 0.5 = 1 \\text{ V}^{2} $$\nThings get more interesting when \\( \\tau \\) gets larger - specifically larger than the bit length \\( T \\). Let\u0026rsquo;s use \\( x(t = 7) \\) again but this time have \\( \\tau = 1.8 \\), which is shown on Figure 3 as the orange line.\nIn this case, \\( x(7) \\) will be 1 half the time and 0 half the time as before, but now, \\( x(7 + 1.8) \\) will also be 1 half the time and 0 half the time, independant of what \\( x(7) \\) is. This gives us four possible combinations:\n$$ R_{xx}(\\tau = 1.8) = (1 \\times 1 ) + (1 \\times 0) \\times 0.5 + (0 \\times 1) \\times 0.5 + (0 \\times 0) \\times 0.5 $$\n$$ R_{xx}(\\tau = 0) = 0.25 \\text{ V}^{2} $$\nAs a final case, let\u0026rsquo;s look at a value of \\( \\tau \\) that is less than the bit period \\( T \\). For example, \\( \\tau = 0.5 \\). As before, \\( x(7) \\) will be 1 half the time and 0 half the time, but calculating the probability that \\( x(7 + 0.5) \\) will be 1 or 0 is slightly harder due to the offset we have included. Notice in Figure 3 that if \\( x(7) = 0 \\), it\u0026rsquo;s somewhat more likely that \\( x(7 + 0.5) \\) will also be a 0 than a 1, because they might fall in the same bit (and vice versa if \\( x(7) = 1 \\)). In fact, the smaller \\( \\tau \\) is, the higher the probability that the two random variables will have the same value, affecting our correlation function.\nWe can plot the full correlation function over a range of \\( \\tau \\) values as shown in Figure 4.\nFigure 4: Plotting the correlation function of the digital signal stationary random process example.\nFigure 4 code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 \u0026#39;\u0026#39;\u0026#39; Plotting the correlation function of the digital signal stationary random process example. Created by: simmeon Last Modified: 2025-01-25 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) # Array of tau values to calculate the correlation function over tau = np.linspace(-5, 5, 500) # Correlation function # NOTE: this is not calculated properly in any way, # it is quick and dirty but gives the correct result def Rxx(tau_array): Rxx = [] for tau in tau_array: if abs(tau) \u0026lt; 1: Rxx.append(((1 - abs(tau)) / 1) * 0.25 + 0.25) else: Rxx.append(0.25) return Rxx R = Rxx(tau) plt.plot(tau, R) plt.title(\u0026#39;Correlation Function of Digital Signal\u0026#39;, fontsize=20) plt.xlabel(r\u0026#39;$\\tau$\u0026#39;, fontsize=20) plt.ylabel(r\u0026#39;$R_{xx}(\\tau)$\u0026#39;, fontsize=20) plt.yticks([0, 0.25, 0.5, 0.75, 1.0]) plt.xticks(np.arange(-5, 6, 1)) plt.ylim((0, 1)) plt.show() As we can see, as \\( \\tau \\) gets smaller than the bit length \\( T \\), the correlation function increases linearly. When \\( \\tau \\) is bigger than \\( T \\), the two random variables are completely independant and have the value of \\( 0.25 \\text{ V}^{2} \\) as we calculated. And when \\( \\tau = 0 \\) we get \\( 0.5 \\text{ V}^{2} \\) \u0026ndash; also what we calculated.\nYou may be starting to realise now why this is called the correlation function. It is telling us something about whether we can predict the future value of a function based on its current value. In other words, it tells us whether there is any correlation between the current value and the future value.\nWhen \\( \\tau = 0 \\), that future value is always the current value, so we have the highest correlation. As \\( \\tau \\) gets bigger, we get less sure about whether we are still on the same bit and therefore if we are able to predict the future value. Finally, after \\( \\tau \u0026gt; 1\\), we are definitely on a different bit and so have no way of predicting the future value from our current value.\nNote, however, that when \\( \\tau \u0026gt; 1 \\) the correlation function is not 0. This is somewhat deceptive as it suggests there is some correlation even though we know this isn\u0026rsquo;t true. Because of this, a better name for this function may be the \u0026lsquo;average shared directional power\u0026rsquo; [1]. This won\u0026rsquo;t be very relevant for how we will be using this function, but it\u0026rsquo;s worth keeping in mind.\nTo summarise, we now have a tool called the correlation function defined as\n$$ R_{xx}(\\tau) = E\\{x(t) x(t + \\tau) \\} $$\nWhen we use the same stationary random process, we call this the auto-correlation function. We can also use two different stationary random processes:\n$$ R_{xy}(\\tau) = E\\{x(t) y(t + \\tau) \\} $$\nwhich we would call the cross-correlation function. This can be interpreted as telling us about whether a future value of \\( y(t) \\) can be predicted from the current value of \\( x(t) \\).\nWhile it may not seem like it yet, these correlation functions form the basis for the method we will use to derive our transfer functions. But we need to derive one more function before we get there.\nSpectral Density Functions We know that, to get our frequency responses, we will have to go into the frequency domain at some point. Spectral density functions will give our statistical methods derived so far a link between the time and frequency domains.\nSpectra Via Correlation Function Transforming our newly-derived correlation function into the frequency domain seems like a good place to start. Luckily, this is pretty straightforward with the Fourier transform.\n$$ S_{xx}(f) = \\int_{-\\infty}^{\\infty}{R_{xx}(\\tau) e^{-j 2 \\pi f \\tau}} d\\tau $$\nAs this integral is from \\( -\\infty \\) to \\( \\infty \\), we will get both positive and negative frequencies as a result. \\( S_{xx}(f) \\) is the two-sided power spectral density function. It related to power since the correlation function has an \\( x^{2} \\) relationship, often proportional to power. It is a spectral density because the Fourier transform gives us power per Hertz, ie. a density over frequency.\nWhen dealing with real systems, we don\u0026rsquo;t really care about negative frequencies. And since the Fourier transform is symmetric, we don\u0026rsquo;t get any additional information from those negative frequencies. So we can instead use the one-sided spectral density function \\( G_{xx}(f) \\) where we cut off the negative frequencies and shift all that power into the positive frequencies:\n$$ G_{xx}(f) = 2 S_{xx}(f) \\text{ , where } f \u0026gt; 0 $$\nMuch like with the correlation function, if we use the same two stationary random processes, we will call this the autospectral density function.\nWe can also define the cross spectral density function with two different processes:\n$$ G_{xy}(f) = 2 S_{xy}(f) \\text{ , where } f \u0026gt; 0 $$\nFigure 5 shows both the one-sided and two-sided power spectral density functions for our digital signal example.\nFigure 5: Plotting the spectral density function of the digital signal stationary random process example.\nFigure 5 code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 \u0026#39;\u0026#39;\u0026#39; Plotting the spectral density function of the digital signal stationary random process example. Created by: simmeon Last Modified: 2025-01-25 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np from scipy.fft import fft, fftfreq from scipy.signal import zoom_fft plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) # Array of tau values to calculate the correlation function over n = 5000 T = 5 tau = np.linspace(-T/2, T/2, n) # Correlation function # NOTE: this is not calculated properly in any way, # it is quick and dirty but gives the correct result def Rxx(tau_array): Rxx = [] for tau in tau_array: if abs(tau) \u0026lt; 1: Rxx.append(((1 - abs(tau)) / 1) * 0.25 + 0.25) else: Rxx.append(0.25) return Rxx R = Rxx(tau) # Fourier transform of correlation function using a fancy chirp-z tranform method # for better frequency resolution G = zoom_fft(R, [0, 1], m=len(R)//2, fs=n/(2*T)) G = abs(G) / n # Frequencies corresponding to Fourier transform data f = np.linspace(0, 1, len(R)//2) # zoom_fft gives only positive freqs, aka. one-sided # since Fourier is symmetric, we can flip it for negative freqs S_negative = np.flip(G) f_negative = - np.flip(f) G = G.tolist() S_negative = S_negative.tolist() f = f.tolist() f_negative = f_negative.tolist() # Adding positive and negative freqs and halving amplitude gives full two-sided spectral density S = S_negative + G f = f_negative + f S = np.array(S) S = S / 2 # Plotting fig = plt.figure() ax1 = fig.add_subplot(2,1,1) ax2 = fig.add_subplot(2,1,2) ax1.plot(f[n//2:], S[n//2:], color=\u0026#39;white\u0026#39;) ax1.plot(f[0:n//2], S[0:n//2], color=\u0026#39;white\u0026#39;) ax2.plot(f[n//2:], G, color=\u0026#39;white\u0026#39;) ax1.set_title(\u0026#39;Two and One-sided Spectral Density Functions of Digital Signal\u0026#39;, fontsize=20) ax2.set_xlabel(\u0026#39;f\u0026#39;, fontsize=20) ax1.set_ylabel(r\u0026#39;$S_{xx}(\\tau)$\u0026#39;, fontsize=20) ax2.set_ylabel(r\u0026#39;$G_{xx}(\\tau)$\u0026#39;, fontsize=20) ax1.set_xlim((-1, 1)) ax2.set_xlim((-1, 1)) ax1.set_ylim((0, 0.3)) ax2.set_ylim((0, 0.3)) ax1.set_xticks(np.arange(-1, 1.1, 0.1)) ax2.set_xticks(np.arange(-1, 1.1, 0.1)) plt.show() This is the final tool we need to calculate our frequency responses. However, calculating these spectral density functions is quite a process. In particular, calculating or estimating the probability density functions of our stationary random processes can be very tricky.\nIt would be nice if there was a simpler way to get these spectral densities\u0026hellip;\nSpectra Via Finite Fourier Transform Luckily, there is an easier way! Unfortunately, it\u0026rsquo;s a bit messy to derive.\nWe will start by showing the result we want to end up with, then showing that it gives the same result as the previous method.\nWe will be deriving the cross-spectra of two different stationary random processes, \\( x(t) \\) and \\( y(t) \\). And what we are going to guess is that for a finite time interval \\( 0 \u0026lt; t \u0026lt; T \\), our cross-spectra will be\n$$ S_{xy}(f, T, k) = \\frac{1}{T} X_{k}^{*}(f, T) Y_{k}(f, T) $$\nwhere \\( X_{k}^{*}(f, T) \\) is the complex conjugate of the finite Fourier transform of the stationary random process \\( x_{k}(t) \\) over the interval \\( [0, T] \\). Similarly, \\( Y_{k}(f, T) \\) is the finite Fourier transform of \\( y_{k}(t) \\) over the same interval.\n$$ X_{k}(f, T) = \\int_{0}^{T}{x_{k}(t) e^{-j 2 \\pi f t}}dt $$\n$$ Y_{k}(f, T) = \\int_{0}^{T}{y_{k}(t) e^{-j 2 \\pi f t}}dt $$\nThis would be a very useful result that would make calculating spectral density functions much easier. What we need to prove is that our guess will lead to the same definition as earlier:\n$$ S_{xy}(f) = \\int_{-\\infty}^{\\infty}{R_{xy}(\\tau) e^{-j 2 \\pi f \\tau}} d\\tau $$\nFirst of all, our guess is currently dependant on a finite value of \\( T \\) and is defined in terms of the stationary random processes. To turn in into something more useful, we need to take the limit as \\( T \\to \\infty \\) and then also take the expected value of the stationary random processes.\n$$ S_{xy}(f) = \\lim_{T \\to \\infty} E\\{ S_{xy}(f, T, k) \\} $$\nLet\u0026rsquo;s now write out fully what \\( S_{xy}(f, T, k) \\) looks like.\n$$ S_{xy}(f, T, k) = \\int_{0}^{T}{x_{k}(\\alpha) e^{-j 2 \\pi f \\alpha}}d\\alpha \\int_{0}^{T}{y_{k}(\\beta) e^{-j 2 \\pi f \\beta}}d\\beta $$\nInstead of \\( t \\) we will use \\( \\alpha \\) and \\( \\beta \\) to help make it clear which variables are in which integral.\n$$ S_{xy}(f, T, k) = \\frac{1}{T} \\int_{0}^{T} \\int_{0}^{T} x_{k}(\\alpha) y_{k}(\\beta) e^{-j 2 \\pi f ( \\beta -\\alpha)} d\\alpha d\\beta $$\nWe also know that, if we want this to match our earlier derivation, this integral will have to be over \\( \\tau \\). So we will change our integration variables to be \\( \\alpha \\) and \\( \\tau \\), where \\( \\tau = \\beta - \\alpha \\). This should make sense as \\( \\tau \\) was the difference between the two times we would look at when creating the correlation function.\nAs for how this will change our integral, we can show that\n$$ \\int_{0}^{T} \\int_{0}^{T} d\\alpha d\\beta = \\int_{-T}^{0} \\int_{-\\tau}^{T} d\\alpha d\\tau + \\int_{0}^{T} \\int_{0}^{T - \\tau} d\\alpha d\\tau $$\nwhere both sides give the value \\( T^{2} \\).\nPutting our full integral back in gives us\n$$ S_{xy}(f, T, k) = \\int_{-T}^{0} \\bigg[ \\frac{1}{T} \\int_{-\\tau}^{T} x_{k}(\\alpha) y_{k}(\\alpha + \\tau) d\\alpha \\bigg] e^{-j 2 \\pi f \\tau} d\\tau + \\int_{0}^{T} \\bigg[ \\frac{1}{T} \\int_{0}^{T - \\tau} x_{k}(\\alpha) y_{k}(\\alpha + \\tau) d\\alpha \\bigg] e^{-j 2 \\pi f \\tau} d\\tau $$\nWe can then take the expected value of both sides, remembering that \\( R_{xy}(\\tau) = E\\{ x_{k}(\\alpha) y_{k}(\\alpha + \\tau) \\} \\).\n$$ E\\{ S_{xy}(f, T, k) \\} = \\int_{-T}^{0} \\bigg[ \\frac{1}{T} \\int_{-\\tau}^{T} R_{xy}(\\tau) d\\alpha \\bigg] e^{-j 2 \\pi f \\tau} d\\tau + \\int_{0}^{T} \\bigg[ \\frac{1}{T} \\int_{0}^{T - \\tau} R_{xy}(\\tau) d\\alpha \\bigg] e^{-j 2 \\pi f \\tau} d\\tau $$\nIf we look at the integrals inside the square brackets, we can actually evaluate these fairly easily.\n$$ \\begin{aligned} \\frac{1}{T} \\int_{-\\tau}^{T} R_{xy}(\\tau) d\\alpha \u0026amp;= R_{xy}(\\tau) \\frac{1}{T} \\int_{-\\tau}^{T} d\\alpha \\\\ \u0026amp;= R_{xy}(\\tau) \\frac{1}{T} \\big[ \\alpha \\big]_{\\alpha = -\\tau}^{T} \\\\ \u0026amp;= R_{xy}(\\tau) \\frac{1}{T} (T + \\tau) \\\\ \u0026amp;= R_{xy}(\\tau) (1 + \\frac{\\tau}{T}) \\end{aligned} $$\n$$ \\begin{aligned} \\frac{1}{T} \\int_{0}^{T - \\tau} R_{xy}(\\tau) d\\alpha \u0026amp;= R_{xy}(\\tau) \\frac{1}{T} \\int_{0}^{T - \\tau} d\\alpha \\\\ \u0026amp;= R_{xy}(\\tau) \\frac{1}{T} \\big[ \\alpha \\big]_{\\alpha = 0}^{T - \\tau} \\\\ \u0026amp;= R_{xy}(\\tau) \\frac{1}{T} (T - \\tau) \\\\ \u0026amp;= R_{xy}(\\tau) (1 - \\frac{\\tau}{T}) \\end{aligned} $$\nSubbing these back into our equation gives\n$$ E\\{ S_{xy}(f, T, k) \\} = \\int_{-T}^{0} R_{xy}(\\tau) (1 + \\frac{\\tau}{T}) e^{-j 2 \\pi f \\tau} d\\tau + \\int_{0}^{T} R_{xy}(\\tau) (1 - \\frac{\\tau}{T}) e^{-j 2 \\pi f \\tau} d\\tau $$\nLet\u0026rsquo;s now take the limit as \\( T \\to \\infty \\) of both sides. This will make \\( \\frac{\\tau}{T} \\to 0 \\).\n$$ \\lim_{T \\to \\infty} E\\{ S_{xy}(f, T, k) \\} = \\int_{-\\infty}^{0} R_{xy}(\\tau) e^{-j 2 \\pi f \\tau} d\\tau + \\int_{0}^{\\infty} R_{xy}(\\tau) e^{-j 2 \\pi f \\tau} d\\tau $$\nWe now have the same integrand in both integrals and the upper bound of the first is the same as the lower bound of the second (they are both 0). This means we can combine them into a single integral giving\n$$ \\lim_{T \\to \\infty} E\\{ S_{xy}(f, T, k) \\} = \\int_{-\\infty}^{\\infty} R_{xy}(\\tau) e^{-j 2 \\pi f \\tau} d\\tau = S_{xy}(f) $$\nGreat! This shows that our guess from the beginning actually does give the same spectral density function. To be clear, we can write\n$$ S_{xy}(f) = \\lim_{T \\to \\infty} \\frac{1}{T} E\\{ X_{k}^{*}(f, T) Y_{k}(f, T) \\} $$\nand similarly with our one-sided spectral density functions\n$$ G_{xy}(f) = \\lim_{T \\to \\infty} \\frac{2}{T} E\\{ X_{k}^{*}(f, T) Y_{k}(f, T) \\} \\text{ , where } f \u0026gt; 0 $$\nIn practice, \\( T \\) will always be a finite value, and we will not be able to calculate the true expected value of the Fourier transforms of our stationary random processes. Instead, we will roughly estimate that the expected value will be what our single sample functions give us: \\( x(t) \\) and \\( y(t) \\). This gives us a rough spectral density estimate of\n$$ \\hat{G}_{xy}(f) = \\frac{2}{T} \\big[ X^{*}(f) Y(f) \\big] $$\nwhere \\( X(f) \\) and \\( Y(f) \\) are the finite Fourier transforms of our single sample functions.\nWe can see what this estimate looks like in Figure 6 and Figure 7.\nFigure 6: Plotting the spectral density and estimated spectral density functions of the digital signal stationary random process example.\nFigure 7: Plotting the spectral density and 100 estimated spectral density functions.\nFigure 6 \u0026amp; 7 code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 \u0026#39;\u0026#39;\u0026#39; Estimated spectral density function of the digital signal from direct finite Fourier transforms. Created by: simmeon Last Modified: 2025-01-26 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np from scipy.signal import zoom_fft plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) # Same proper G from last example # Array of tau values to calculate the correlation function over n = 5000 T = 5 tau = np.linspace(-T/2, T/2, n) # Correlation function # NOTE: this is not calculated properly in any way, # it is quick and dirty but gives the correct result def Rxx(tau_array): Rxx = [] for tau in tau_array: if abs(tau) \u0026lt; 1: Rxx.append(((1 - abs(tau)) / 1) * 0.25 + 0.25) else: Rxx.append(0.25) return Rxx R = Rxx(tau) # Fourier transform of correlation function using a fancy chirp-z tranform method # for better frequency resolution G = zoom_fft(R, [0, 1], m=len(R)//2, fs=n/(2*T)) G_actual = abs(G) / len(R) f_actual = np.linspace(0, 1, len(R)//2) # --------------------------------------------------------------------- # G_array = [] for i in range(100): num_bits = 10 # Calculate random series of bits bits = np.random.randint(0, 2, num_bits) N = 100 # Number of sample points per bit bit_length = 1 # seconds # Make length of each bit 1 s T = num_bits * bit_length t = np.linspace(0, T, num_bits * N) # sample function x = [] # Make sample function have N points per bit and add offsets for bit in bits: x = x + [bit] * N # Fourier transform sample function X = zoom_fft(x, [0, 1], m=len(x)//2, fs=N) X = abs(X) / len(x) * 2 # Calculate estimated spectral density G = 2 / T * (np.conjugate(X) * X) G_array.append(G) # Frequencies corresponding to Fourier transform data f = np.linspace(0, 1, len(x)//2) for G in G_array: plt.plot(f, G, alpha=0.05) plt.plot(f_actual, G_actual, label=\u0026#39;Actual\u0026#39;) plt.title(\u0026#39;100 Estimates of Spectral Density Function\u0026#39;, fontsize=20) plt.xlim((0, 1)) plt.xticks(np.arange(0, 1.1, 0.1)) plt.ylabel(r\u0026#39;$G_{xx}(f)$\u0026#39;, fontsize=20) plt.xlabel(\u0026#39;f\u0026#39;, fontsize=20) plt.legend(fontsize=20) plt.show() Now that we have a good way of calculating these spectra, let\u0026rsquo;s find out how they will be useful for finding frequency responses\u0026hellip;\nSingle Input Single Output (SISO) Frequency Responses Starting from the familiar convolution integral that describes a SISO system,\n$$ y(t) = \\int_{0}^{\\infty} h(\\alpha) x(t - \\alpha) d\\alpha $$\nwe can also describe the system at time \\( t + \\tau \\) as\n$$ y(t + \\tau) = \\int_{0}^{\\infty} h(\\alpha) x(t + \\tau - \\alpha) d\\alpha $$\nMultiplying both sides by \\( x(t) \\) gives\n$$ x(t) y(t + \\tau) = \\int_{0}^{\\infty} h(\\alpha) x(t) x(t + \\tau - \\alpha) d\\alpha $$\nRemember that we are going to assume our input \\( x(t) \\) and output \\( y(t) \\) are both stationary random processes, so we can take the expected value of both sides to get\n$$ E\\{x(t) y(t + \\tau)\\} = \\int_{0}^{\\infty} h(\\alpha) E\\{x(t) x(t + \\tau - \\alpha)\\} d\\alpha $$\n$$ R_{xy}(\\tau) = \\int_{0}^{\\infty} h(\\alpha) R_{xx}(\\tau - \\alpha) d\\alpha $$\nNow we have the same form of convolution integbral just in terms of correlation functions instead of our stationary random processes. So we can transform this into the Fourier domain to change the convolution integral into a multiplication.\n$$ S_{xy}(f) = H(f) S_{xx}(f) $$\nFinally getting the relationship between our statistical methods and the frequency response we\u0026rsquo;ve been building up.\n$$ H(f) = \\frac{S_{xy}(f)}{S_{xx}(f)} = \\frac{G_{xy}(f)}{G_{xx}(f)} $$\nMultiple Input Single Output (MISO) Frequency Responses Theory on Partially Correlated Inputs To be able to what happens when we have correlated inputs, we first need to think about what a system with correlated inputs would look like. To do this, we will make use of some helpful system diagrams.\nFigure 8 shows a diagram of a single input single output system as a reference.\nFigure 8: Single input single output system diagram.\nIf we have two inputs, \\( x_{1} \\) and \\( x_{2} \\), if they are correlated, we could think of \\( x_{2} \\) as being made up of an uncorrelated part, \\( x_{2_{UC}} \\) and a part that is correlated with \\( x_{1} \\) which we can call \\( x_{2_{C}} \\).\nWe can think of this correlated part as being a linear transformation of \\( x_{1} \\), which we can represent by a transfer function \\( L_{12}(f) \\). Note that often these correlation effects will be non-linear so we will use the notation \\( L_{12}(f) \\) to represent the transfer function with the optimum linear effects between the input \\( x_{1} \\) and output \\( x_{2_{C}} \\).\nFigure 9 shows this idea in a diagram.\nFigure 8: Two-input single-output system diagram with partially correlated inputs.\nNow, if we wanted to find the frequency response between \\( x_{1} \\) and \\( y \\), we would think to calculate\n$$ \\frac{Y(f)}{X_{1}(f)} $$\nbut because \\( x_{2} \\) also effects the output \\( y \\) through \\( H_{2y} \\) and because \\( x_{1} \\) is correlated with \\( x_{2} \\) through \\( L_{12} \\), doing this will actually calculate\n$$ \\frac{Y(f)}{X_{1}(f)} = H_{1y}(f) + H_{2y}(f) L_{12}(f) $$\nwhich does not just give us \\( H_{1y}(f) \\) as we want. Because \\( x_{1} \\) gets to \\( y \\) through multiple paths (not just through \\( H_{1y} \\)), we have to account for this \u0026rsquo;error\u0026rsquo; to isolate the actual frequency response we care about: \\( H_{1y}(f) \\).\nConditioned Frequency Responses So how do we use the tools we developed to correct, or condition, these frequency responses to actually give us what we want? Since we\u0026rsquo;re now defining our frequency response in terms of spectral functions, for our two input example we know we will need \\( G_{1y} \\) and \\( G_{11} \\). Let\u0026rsquo;s recall the definition of our spectral density function using \\( G_{1y} \\) (ignoring the limit as \\( T \\to \\infty \\)).\n$$ G_{1y}(f) = \\frac{2}{T} E\\{ X^{*}_{1}(f) Y(f) \\} $$\nwhere we just showed that\n$$ Y(f) = H_{1y}(f) X_{1}(f) + H_{2y}(f) L_{12}(f) X_{1}(f) $$\nSubstituing this into our spectral density function gives\n$$ \\begin{aligned} G_{1y}(f) \u0026amp;= \\frac{2}{T} E\\{ X^{*}_{1}(f) \\big[ H_{1y}(f) X_{1}(f) + H_{2y}(f) L_{12}(f) X_{1}(f) \\big] \\} \\\\ \u0026amp;= \\frac{2}{T} E\\{ H_{1y}(f) X^*_{1}(f) X_{1}(f) + H_{2y}(f) L_{12}(f) X^*_{1}(f) X_{1}(f) \\} \\\\ \u0026amp;= \\frac{2}{T} \\bigg( H_{1y}(f) \\frac{T}{2} G_{11}(f) + H_{2y}(f) L_{12}(f) \\frac{T}{2} G_{11}(f) \\bigg) \\\\ \u0026amp;= H_{1y}(f) G_{11}(f) + H_{2y}(f) L_{12}(f) G_{11}(f) \\end{aligned} $$\nFrom this, we can see that\n$$ \\frac{G_{1y}(f)}{G_{11}(f)} = H_{1y}(f) + H_{2y}(f) L_{12}(f) $$\nand therefore\n$$ H_{1y}(f) = \\frac{G_{1y}(f)}{G_{11}(f)} - H_{2y}(f) L_{12}(f) $$\nas we saw before.\nThe important equation here is\n$$ G_{1y}(f) = H_{1y}(f) G_{11}(f) + H_{2y}(f) L_{12}(f) G_{11}(f) $$\nNotice that\n$$ L_{12}(f) G_{11}(f) = G_{12}(f) $$\nso we can write\n$$ G_{1y}(f) = H_{1y}(f) G_{11}(f) + H_{2y}(f) G_{12}(f) $$\nA very similar method could be used for \\( G_{2y}(f) \\).\n$$ G_{2y}(f) = H_{1y}(f) G_{21}(f) + H_{2y}(f) G_{22}(f) $$\nThis gives us two equations with only two unknows: \\( H_{1y}(f) \\) and \\( H_{2y}(f) \\), the two frequency responses of the system. We can calculate all the spectral functions here.\nWe can solve these simultaneously in matrix form for each discrete frequency \\( f_{i} \\).\n$$ \\begin{bmatrix} G_{11}(f_{i}) \u0026amp; G_{12}(f_{i}) \\\\ G_{21}(f_{i}) \u0026amp; G_{22}(f_{i}) \\end{bmatrix} \\begin{bmatrix} H_{1y}(f_{i}) \\\\ H_{2y}(f_{i}) \\end{bmatrix} = \\begin{bmatrix} G_{1y}(f_{i}) \\\\ G_{2y}(f_{i}) \\end{bmatrix} $$\n$$ \\begin{bmatrix} H_{1y}(f_{i}) \\\\ H_{2y}(f_{i}) \\end{bmatrix} = \\begin{bmatrix} G_{11}(f_{i}) \u0026amp; G_{12}(f_{i}) \\\\ G_{21}(f_{i}) \u0026amp; G_{22}(f_{i}) \\end{bmatrix} ^{-1} \\begin{bmatrix} G_{1y}(f_{i}) \\\\ G_{2y}(f_{i}) \\end{bmatrix} $$\nReferences [1] What is Autocorrelation?\n","permalink":"http://localhost:1313/blog/posts/conditionedfrequencyresponses/conditionedfrequencyresponses/","summary":"\u003c!-- Import KaTeX --\u003e\r\n\r\n\r\n\u003clink rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css\" integrity=\"sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq\" crossorigin=\"anonymous\"\u003e\r\n\u003cscript defer src=\"https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js\" integrity=\"sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\r\n\u003cscript defer src=\"https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js\" integrity=\"sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI\" crossorigin=\"anonymous\" onload=\"renderMathInElement(document.body);\"\u003e\u003c/script\u003e\r\n\r\n\n\u003ch2 id=\"what-is-this-all-about\"\u003eWhat is this all about?\u003c/h2\u003e\n\u003cp\u003eA frequency response gives information about how an output of a system will react to a sinusoidal input at different frequencies. They are very useful in analysing the stability and behaviour of systems. They are also central to frequency-domain system identification methods, which is what we will be interested in using them for.\u003c/p\u003e\n\u003cp\u003eIn general, for some system input \\( x(t) \\) and output \\( y(t) \\), the frequency response is\u003c/p\u003e","title":"Conditioned Frequency Responses"},{"content":"\rThe Initial Value Problem In engineering, we often encounter systems that evolve over time, such as circuits, mechanical systems, or chemical reactions. These systems are best described using differential equations. For example, Newton\u0026rsquo;s law of cooling states:\n$$ \\dfrac{dT}{dt} = -k(T - T_{surr}) $$\nwhere T is the temperature of some point, k is a proportionality constant, and Tsurr is the temperature surrounding the point of interest.\nTo solve this equation is to find a solution for the temperature, T, over time. In this case, this is fairly straightforward and we can come up with an analytical solution of the form:\n$$ T(t) = T_{surr} + (T(0) - T_{surr})e^{-kt} $$\nNotice that the solution depends on the initial temperature, as Figure 1 shows. We need a point of reference to define the solution that is relevant to our system.\nFigure 1: Temperature of a point over time with different initial temperatures. Tsurr = 20 C, k = 1.\nFigure 1 code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 \u0026#39;\u0026#39;\u0026#39; Plotting the temperature of a point with different starting temperatures. Created by: simmeon Last Modified: 28/04/24 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np # Define system parameters T_surr = 20 # surrounding temperature of 20 C k = 1 # proportionality constant of 1 for simplicity # Define our cooling function def solve_T(t, T0): return T_surr + (T0 - T_surr)*np.exp(-k*t) # Create an array of times from 0-5 seconds t = np.arange(0, 5, 0.1) # Solve and plot the solutions for different initial values of T T0_choices = [30, 25, 20, 15, 10] plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) for T0 in T0_choices: T = solve_T(t, T0) plt.plot(t, T, label=\u0026#39;T0 = {} C\u0026#39;.format(T0)) plt.title(\u0026#39;Temperature over time\u0026#39;, fontsize=20) plt.xlabel(\u0026#39;Time [s]\u0026#39;, fontsize=20) plt.ylabel(\u0026#39;Temperature [C]\u0026#39;, fontsize=20) plt.xlim(0, 5) plt.ylim(10, 30) plt.grid(alpha=0.7) plt.legend() plt.show() The initial value problem deals with solving these ordinary differential equations where we have an initial state of the system, eg. the initial temperature is 25 C. However, often these systems are hard or impossible to solve analytically. To solve them, we use numerical methods to approximate the solution. Let\u0026rsquo;s look at how we might be able to do that\u0026hellip;\nNumerical Methods Let\u0026rsquo;s take the previous cooling equation,\n$$ \\dfrac{dT}{dt} = -k(T - T_{surr}) $$\nand assume we can\u0026rsquo;t find an analytical solution. Let\u0026rsquo;s consider the information we do have that could help us approximate the solution. We have an expression for the derivative that we can calculate at any time, t, assuming we know the current temperature at that time. We know the temperature at some initial time (t = 0 in this case), so we can calculate the derivative at that time. But how does this help us find the temperature at other times?\nThe Euler Method From calculus we know that we get the derivative of a function by taking two points on the function and approximating the derivative as if it were a striaght line. As the distance, h, between the two points gets closer to 0, the approximation gets better. Formally,\n$$ f^{\\prime}(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h} $$\nWe can rearrange this and assume a finite step size, h, to get\n$$ f(x+h) \\approx f(x) + h f^{\\prime}(x) $$\nwhich gives us an expression to approximate the next step of a function using only the current known function value and the function derivative. We can see what this looks like with different step sizes in Figure 2.\nFigure 2: Estimating the value of y = x2 with different step sizes.\nFigure 2 code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 \u0026#39;\u0026#39;\u0026#39; Visualising how the derivative can be used to estimate the value of a function. Created by: simmeon Last Modified: 28/04/24 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) # Define some function we want to find the value of def func(t): return t**2 # Define the derivative line of the function def dydt(t, t0, y0): m = 2 * t0 return m * (t-t0) + y0 # Create an array of time values to plot the function t_func = np.arange(0.5, 2, 0.1) # Estimation parameters t0 = 1.0 # known initial value y0 = func(t0) # known initial value h = [1, 0.5, 0.2] # step size options # Plotting # Find and plot function values y_func = func(t_func) plt.plot(t_func, y_func) plt.plot(t0, y0, \u0026#39;x\u0026#39;, color=\u0026#39;C0\u0026#39;) # Plot the derivative line for each step size colours = [\u0026#39;C1\u0026#39;, \u0026#39;C2\u0026#39;, \u0026#39;C3\u0026#39;] for i in range(len(h)): t_dydt = np.arange(t0, t0 + h[i], 0.1) y_dydt = dydt(t_dydt, t0, y0) plt.plot(t_dydt, y_dydt, color=colours[i], label=\u0026#39;Step = {:.1f}\u0026#39;.format(h[i])) plt.plot(t_dydt[-1], y_dydt[-1], \u0026#39;o\u0026#39;, color=colours[i]) # Show plot plt.title(\u0026#34;Estimating a function by using the derivative\u0026#34;, fontsize=20) plt.legend(loc=\u0026#39;lower right\u0026#39;, fontsize=10) plt.show() This is Euler\u0026rsquo;s method for solving the initial value problem. We might also write it as:\n$$ f_{k+1} \\approx f_{k} + h f^{\\prime}(x_{k}) $$\nfor some index, k.\nWe can iterate through time with this method, using the newly found fk+1 as our new fk and so on. In code, that would look like the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def euler(dydt, y0, t0, t_end, h=0.1): \u0026#39;\u0026#39;\u0026#39; Takes the derivative function, an initial condition, the time we want to integrate until, and a step size. Returns arrays of time and y values. \u0026#39;\u0026#39;\u0026#39; t = np.arange(t0, t_end+h, h) y = np.zeros(len(t)) y[0] = y0 for i in range(1,len(t)): y[i] = y[i-1] + h * dydt(y[i-1]) return t, y The highlighted line shows the Euler method itself where the next value of the function is estimated.\nLet\u0026rsquo;s try do that with Newton\u0026rsquo;s cooling law and see how the result compares to the analytical solution. Figure 3 shows what that would look like.\nFigure 3: Analytical and Euler method solutions to Newton\u0026rsquo;s cooling law. Tsurr = 20 C, k = 1.\nFigure 3 code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 \u0026#39;\u0026#39;\u0026#39; Performing Euler integration to solve the cooling equation. We will compare with the analytical solution. Created by: simmeon Last Modified: 28/04/24 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) # Define system T_surr = 20 # surrounding temperature of 20 C k = 1 # proportionality constant of 1 for simplicity # Define our cooling function def solve_T(t, T0): return T_surr + (T0 - T_surr)*np.exp(-k*t) # Define the derivative (only depends on the current temp, not time) def dTdt(T): return -k * (T - T_surr) # Define our Euler integration function def euler(dTdt, T0, t0, t_end, h=0.1): \u0026#39;\u0026#39;\u0026#39; Takes the derivative function, an initial condition, the time we want to integrate until, and a step size. Returns arrays of time and temperature values \u0026#39;\u0026#39;\u0026#39; t = np.arange(t0, t_end+h, h) T = np.zeros(len(t)) T[0] = T0 for i in range(1,len(t)): T[i] = T[i-1] + h * dTdt(T[i-1]) return t, T # Parameters t0 = 0 T0 = 30 t_end = 5 # Get analytical solution t_analytical = np.arange(t0, t_end+0.1, 0.1) T_analytical = solve_T(t_analytical, T0) # Get Euler numerical solution t_euler_05, T_euler_05 = euler(dTdt, T0, t0, t_end, 0.5) t_euler_01, T_euler_01 = euler(dTdt, T0, t0, t_end, 0.1) # Plotting plt.plot(t_analytical, T_analytical, label=\u0026#39;Analytical\u0026#39;, linestyle=\u0026#39;--\u0026#39;) plt.plot(t_euler_05, T_euler_05, label=\u0026#39;Euler method, h = 0.5\u0026#39;) plt.plot(t_euler_01, T_euler_01, label=\u0026#39;Euler method, h = 0.1\u0026#39;) plt.title(\u0026#34;Analytical and Euler method solutions to Newton\u0026#39;s cooling law\u0026#34;, fontsize=20) plt.xlabel(\u0026#39;Time [s]\u0026#39;, fontsize=20) plt.ylabel(\u0026#39;Temperature [C]\u0026#39;, fontsize=20) plt.legend(fontsize=15) plt.show() It is clear that the step size plays a big role in the accuracy of the solution. We could continue to reduce the step size, but that will quickly increase the time it takes to get a solution beyond a reasonable amount. Maybe we can think of a better way of doing this\u0026hellip;\nRunge-Kutta Methods A major problem with Euler\u0026rsquo;s method is that using the derivative of the point we\u0026rsquo;re at is not very good at estimating the next value of the function (for the kind of step sizes we want to use). The derivative is constantly changing and could be vastly different at the new value. We might get a more accurate step if we use a mix of the derivative at our start point and end point.\nThis is the basis for how Runge-Kutta methods work. We find derivatives between the start and end points of each step and use a weighted average of those as the actual derivative for the step. The simplest version of this would be to just use the derivative at the start of the step \u0026ndash; which is exactly the Euler method! The Euler method is a 1st order Runge-Kutta method.\nLet\u0026rsquo;s define the method more concretely.\nDerivation of Runge-Kutta Methods We will start by examining the initial problem again, being that we want to solve the following general equation for y:\n$$ \\dfrac{dy}{dt} = f(y, t) $$\nThis could be our cooling equation from before,\n$$ \\dfrac{dT}{dt} = -k(T - T_{surr}) $$\nor something more complex like a state space defining a spring, mass, damper system:\n$$ \\bold{\\dot{x}} = \\begin{bmatrix} 0 \u0026amp; 1 \\\\ \\frac{-k}{m} \u0026amp; \\frac{-c}{m} \\end{bmatrix} \\bold{x} + \\begin{bmatrix} 0 \\\\ \\frac{1}{m} \\end{bmatrix} u(t) $$\nIn both cases, we are given the derivative of the state we want to solve for (eg. temperature) and the derivative depends on the value of the state and time. In the case of the cooling equation, the derivative only depends on the state, T, and not time.\nIdeally, we could just integrate the derivative to find the value at \\( t + h \\):\n$$ y(t+h) = y(t) + \\int_{\\tau=t}^{\\tau=t+h}{\\dfrac{dy(\\tau)}{d\\tau}} d\\tau $$\nHowever, as we said earlier, this is often hard or impossible. We can instead approximate the integral with a weighted sum.\nWeighted Sum Approximations Let\u0026rsquo;s build this up slowly. Take the following function, for example:\n$$ f(t) = t^3 - 2t^2 - t + 3 $$\nHow would we evaluate\n$$ \\int_0^2{f(t)}dt $$\nWe can do this analytically, which is like summing up tiny vertical slices of the function to find the total area underneath it. But we could think about this a different way. We can get that same area by finding the average value of the function over the interval, then multiplying by the length of the interval. You can see this in Figure 4.\nFigure 4: Comparison of areas found by integration and weighted sum approximation.\nFigure 4 code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 \u0026#39;\u0026#39;\u0026#39; Making sense of weighted sums as integral approximations. Created by: simmeon Last Modified: 28/04/24 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) def f(t): return t**3 -2*t**2 - t + 3 # Perform a weighted sum approximation N = 10 # higher number of samples, N, gives a better approximation sum = 0 t0 = 0 h = 2 nodes = np.linspace(t0, t0+h, N) for node in nodes: sum += f(node) weighted_sum = sum / N print(\u0026#39;Weighted sum area: \u0026#39;, weighted_sum * h) # Plotting t = np.arange(t0, t0 + h, 0.01) y = f(t) # Integration result plt.subplot(1,2,1) plt.plot(t,y) plt.fill_between(t, y, alpha=0.3) plt.annotate(\u0026#39;Area = 2.67\u0026#39;, (1,2.5), fontsize=20) plt.title(\u0026#39;Area from integration\u0026#39;, fontsize=20) # Weighted sum result plt.subplot(1,2,2) plt.plot(t,y) plt.fill_between(t, weighted_sum, alpha=0.3) plt.annotate(\u0026#39;Area = {:.2f}\u0026#39;.format(weighted_sum * h), (1,2.5), fontsize=20) plt.title(\u0026#39;Area from weighted sum\u0026#39;, fontsize=20) plt.show() To put this idea into an equation, we can say:\n$$ \\int_0^2{f(t)}dt \\approx \\frac{2}{\\sum{w_i}} \\sum_{i=1}^N{w_i f(t_i)} $$\nWhere \\( N \\) is the number of points we sample from the function to find the average, \\( w_i \\) is the weighting we give to each point, and \\( t_i \\) is some point inside the integration bounds of \\( [0, 2] \\). We can simplify this by giving every point a weighting of 1:\n$$ \\int_0^2{f(t)}dt \\approx \\frac{2}{N} \\sum_{i=1}^N{f(t_i)} $$\nThe more points we sample, the closer the average will be to the actual average of the function and therefore the actual integral.\nWe can extend this concept to our derivative from earlier:\n$$ \\int_{\\tau=t}^{\\tau=t+h}{\\dfrac{dy(\\tau)}{d\\tau}} d\\tau \\approx \\frac{h}{\\sum{w_i}} \\sum_{i=1}^N{w_i y^{\\prime}(t+v_i h, y(t+v_i h))} $$\nWhat a mess.\nWe\u0026rsquo;re still using the same concept of a weighted sum to find the average value of the derivative function, but since the derivative depends on \\( t \\) and \\( y \\) it\u0026rsquo;s a bit more complicated. We introduced \\( v_i \\) to help define which values of the function we sample, also called nodes. This can range from 0 to 1 to cover the interval from \\( t \\) to \\( t+h \\). Of course, whatever time we evaluate our derivative at must be the same time we use to get the \\( y \\) values for the derivative.\nFor the sake of simplicity let\u0026rsquo;s say that all our weights will sum up to 1. We are not, however, going to assume that all our weights will be the same value as we did previously.\nConsidering all this, the equation we are trying to solve is now:\n$$ y(t+h) = y(t) + h \\sum_{i=1}^N{w_i y^{\\prime}(t+v_i h, y(t+v_i h))} $$\nThe Runge-Kutta Family You might have noticed there is a problem with the sum we just defined. In particular, we don\u0026rsquo;t know what \\( y(t+v_i h) \\) is. In fact, finding this is basically the point of the whole method. So how do we deal with this?\nSimply, we are going to make worse approximations of \\( y(t+v_i h) \\) so that we can make a much better approximation of \\( y(t + h) \\). Again, these \\( y(t+v_i h) \\) values are used to find values of the derivative that we will then average. Let\u0026rsquo;s define what we will do to find these approximations.\nWe will start by saying that \\( v_1 = 0 \\). This means the first term in the sum will be\n$$ w_1 y^{\\prime}(t, y(t)) $$\nIf this was the only term in our sum (so \\( w_1 = 1 \\) ), then the equation we would be solving would be:\n$$ y(t+h) \\approx y(t) + h y^{\\prime}(t, y(t)) $$\nThis should look familiar, it\u0026rsquo;s the Euler method! This is what we mean by the Euler method is a 1st order Runge-Kutta method \u0026ndash; because it uses one term in the weighted sum. Or in other words, it is a linear approximation.\nFor simplicity, we will write this first sum term as:\n$$ k_1 = y^{\\prime}(t, y(t)) h $$\n\\( k_1 \\) is a 1st order estimate of the change in \\( y \\) between \\( y(t) \\) and \\( y(t+h) \\).\nThe second term gets trickier as we have to somehow estimate \\( y(t+v_2 h) \\). Conveniently, we have just found an estimate for how \\( y \\) changes: \\( k_1 \\). So we can utilise some fraction of this change to create our estimate for the second weighted sum term where:\n$$ y^{\\prime}(t + \\alpha_2 h, y(t) + \\beta_{2,1} k_1) $$\nWe don\u0026rsquo;t know yet how much of \\( k_1 \\) we should add, we will figure that out later. Same with what time we should sample at.\nWe have changed notation slightly to help set up higher order Runge-Kutta methods. Instead of \\( v_i \\) we are now using \\( \\alpha_i \\) to tell us about what time we are sampling at. And since our \\( y \\) value is no longer defined in simple terms of time, we are going to use \\( \\beta_{i,j} \\) to describe how much of previous estimates we will add to the estimate for the new term.\nWe define\n$$ k_2 = y^{\\prime}(t + \\alpha_2 h, y(t) + \\beta_{2,1} k_1) h $$\nso that the equation we are solving is now:\n$$ y(t+h) \\approx y(t) + w_1 k_1 + w_2 k_2 $$\nWith that, we have defined the 2nd order Runge-Kutta Method!\nWe can continue the weighted sum with a third and fourth term, following similar logic of using the previous estimates to inform the new value of \\( y(t + v_i h) \\). This will give us:\n$$ k_3 = y^{\\prime}(t + \\alpha_3 h, y(t) + \\beta_{3,1} k_1 + \\beta_{3,2} k_2) h \\\\ k_4 = y^{\\prime}(t + \\alpha_4 h, y(t) + \\beta_{4,1} k_1 + \\beta_{4,2} k_2 + \\beta_{4,3} k_3) h $$\nThis gives the 4th order Runge-Kutta method:\n$$ y(t+h) \\approx y(t) + w_1 k_1 + w_2 k_2 + w_3 k_3 + w_4 k_4 $$\nNow to actually solve these higher order methods, we need to define these coefficients\u0026hellip;\nDefining Coefficients We need to define all the \\( \\alpha_i \\), \\( \\beta_{i,j} \\), and \\( w_i \\) coefficients to be able to use these methods. We can do this by comparing the Taylor series expansion of our approximation to the Taylor series expansion of \\( y(t+h) \\) and equating coefficients. Let\u0026rsquo;s do this for the 2nd order method:\n$$ y(t+h) \\approx y(t) + w_1 k_1 + w_2 k_2 $$\nAs this is a 2nd order method, we will need to find the 2nd order expansions of the left and right sides.\nLet\u0026rsquo;s start with the left. The 2nd order Taylor series expansion of \\( y(t+h) \\) about \\( t \\) is:\n$$ y(t+h) \\approx y(t) + h \\dfrac{dy}{dt} \\Big| _{t,y} + \\frac{h^2}{2} \\dfrac{d^2y}{dt^2} \\Big| _{t,y} + O(h^3) $$\nWe can write our derivative:\n$$ \\dfrac{dy}{dt} = f(t, y) \\\\ {} \\\\ \\dfrac{d^2y}{dt^2} = \\dfrac{df(t,y)}{dt} = \\dfrac{\\partial f}{\\partial t} + \\dfrac{\\partial f}{\\partial y} \\dfrac{dy}{dt} = \\dfrac{\\partial f}{\\partial t} + f \\dfrac{\\partial f}{\\partial y} $$\nThis makes our left hand side (using slightly different notation):\n$$ y_{n+1} \\approx y_n + h f(t_n, y_n) + \\frac{h^2}{2} \\bigg(\\dfrac{\\partial f}{\\partial t} + f \\dfrac{\\partial f}{\\partial y}\\bigg) \\bigg|_{t_n, y_n} + O(h^3) $$\nGreat! Let\u0026rsquo;s do the right hand side now. We can write the right hand same with the same notation as above:\n$$ y_n + w_1 k_{1,n} + w_2 k_{2,n} $$\n\\( k_2 \\) is a bit tricky to expand. Our \\( k_2 \\) term is made up mostly of a function with the form \\( f(t + \\Delta t, y + \\Delta y) \\), which will need to be expanded. In general, the 2nd order expansion looks like:\n$$ f(t + \\Delta t, y + \\Delta y) = f(t, y) + \\Delta t \\dfrac{\\partial f}{\\partial t}\\bigg| _{t, y} + \\Delta y \\dfrac{\\partial f}{\\partial y} \\bigg| _{t, y} + O(h^3) $$\nApplying this to \\( k_2 \\) gives:\n$$ k_{2,n} = h f(t + \\alpha_2 h, y_n + \\beta_{2,1} k_1) \\approx h \\bigg( f(t_n, y_n) + \\alpha _2 h \\dfrac{\\partial f}{\\partial t} \\bigg| _{t_n, y_n} + \\beta _{2,1} k_1 \\dfrac{\\partial f}{\\partial y} \\bigg| _{t_n, y_n} \\bigg) $$\nAll together, the right hand side is then\n$$ y_n + w_1 k_{1,n} + w_2 k_{2,n} \\approx y_n + w_1 h f(t_n, y_n) + w_2 h \\bigg( f(t_n, y_n) + \\alpha _2 h \\dfrac{\\partial f}{\\partial t} \\bigg| _{t_n, y_n} + \\beta _{2,1} k_1 \\dfrac{\\partial f}{\\partial y} \\bigg| _{t_n, y_n} \\bigg) + O(h^3) $$\nwhich we can rearrange to be in the same form (substituting in for \\( k_1 \\)) as the left hand side:\n$$ y_n + w_1 k_{1,n} + w_2 k_{2,n} \\approx y_n + (w_1 + w_2) h f(t_n, y_n) + \\frac{h^2}{2} \\bigg(2 w_2 \\alpha_2 \\dfrac{\\partial f}{\\partial t} + 2 w_2 \\beta _{2,1} f \\dfrac{\\partial f}{\\partial y} \\bigg) \\bigg| _{t_n, y_n} + O(h^3) $$\nFinally, we have both expansions and can equate the coefficients:\n$$ y(t+h) \\approx y(t) + w_1 k_1 + w_2 k_2 \\\\ {} \\\\ \\big\\downarrow \\\\ {} \\\\ y_n + h f(t_n, y_n) + \\frac{h^2}{2} \\bigg(\\dfrac{\\partial f}{\\partial t} + f \\dfrac{\\partial f}{\\partial y}\\bigg) \\bigg|_{t_n, y_n} + O(h^3) \\\\ {} \\\\ = \\\\ {} \\\\ y_n + (w_1 + w_2) h f(t_n, y_n) + \\frac{h^2}{2} \\bigg(2 w_2 \\alpha_2 \\dfrac{\\partial f}{\\partial t} + 2 w_2 \\beta _{2,1} f \\dfrac{\\partial f}{\\partial y} \\bigg) \\bigg| _{t_n, y_n} + O(h^3) $$\nFrom this, we can see that our coefficients must satisfy the following equations:\n$$ w_1 + w_2 = 1 \\\\ {} \\\\ w_2 \\alpha_2= \\frac{1}{2} \\\\ {} \\\\ w_2 \\beta_{2,1} = \\frac{1}{2} $$\nWith 3 equations and 4 unknows, there are infinitely many solutions. However, the standard choices are:\n$$ \\alpha_2 = \\beta_{2,1} = 1 \\\\ {} \\\\ w_1 = w_2 = \\frac{1}{2} $$\nThis brings us finally to the complete 2nd order Runge-Kutta Method:\n$$ k_1 = h y^\\prime (t_n, y_n) \\\\ {} \\\\ k_2 = h y^\\prime (t_n + h, y_n + k_1) \\\\ {} \\\\ y_{n+1} = y_n + \\frac{1}{2} k_1 + \\frac{1}{2} k_2 $$\nA similar (messier) method of expansions can be used to for higher order methods. The standard terms for the 4th order Runge-Kutta method are:\n$$ k_1 = h y^\\prime (t_n, y_n) \\\\ {} \\\\ k_2 = h y^\\prime (t_n + \\frac{h}{2}, y_n + \\frac{k_1}{2}) \\\\ {} \\\\ k_3 = h y^\\prime (t_n + \\frac{h}{2}, y_n + \\frac{k_2}{2}) \\\\ {} \\\\ k_4 = h y^\\prime (t_n + h, y_n + k_3) \\\\ {} \\\\ y_{n+1} = y_n + \\frac{1}{6} (k_1 + 2 k_2 + 2 k_3 + k_4) $$\nKeep in mind that there are infinitely many choices of these coefficients and lots of research has gone into figuring out which ones work best. We will stick to these simple standard ones for now.\nImplementing Runge-Kutta Now that we have finally derived the Runge-Kutta methods, let\u0026rsquo;s implement them in code. We will do both the 2nd order and 4th order methods and compare their accuracy.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def rk2(dydt, t0, y0, t_end, h): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the 2nd order Runge-Kutta method Args: dydt: The derivative function to integrate t0: Initial value of time y0: Initial value of y t_end: Final time for integration h: Step size Returns: An array of y values for each time step \u0026#34;\u0026#34;\u0026#34; t = np.arange(t0, t_end+h, h) # Array of time points y = np.zeros_like(t) y[0] = y0 for i in range(1, len(t)): k1 = h * dydt(t[i - 1], y[i - 1]) k2 = h * dydt(t[i - 1] + h, y[i - 1] + k1) y[i] = y[i - 1] + (k1 + k2) / 2 return y We can see how, at each time step, we calculate the \\( k_i \\) values and use these to estimate the next value of the function.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def rk4(dydt, t0, y0, t_end, h): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the 4th order Runge-Kutta method Args: dydt: The derivative function to integrate t0: Initial value of time y0: Initial value of y t_end: Final time for integration h: Step size Returns: An array of y values for each time step \u0026#34;\u0026#34;\u0026#34; t = np.arange(t0, t_end+h, h) y = np.zeros_like(t) y[0] = y0 for i in range(1, len(t)): k1 = h * dydt(t[i - 1], y[i - 1]) k2 = h * dydt(t[i - 1] + h / 2, y[i - 1] + k1 / 2) k3 = h * dydt(t[i - 1] + h / 2, y[i - 1] + k2 / 2) k4 = h * dydt(t[i], y[i - 1] + k3) y[i] = y[i - 1] + (k1 + 2 * k2 + 2 * k3 + k4) / 6 return y After all that derivation, the actual method is remarkably clean and simple to implement. Figure 5 gives an idea of how these higher order functions perform compared to our original Euler method.\nFigure 5: Comparing the accuracy of different order ODE solvers, h = 0.1\nAll the solvers are using the same step size here. As we can see, the 4th order method is better than the 2nd order method. They are both much more accurate than the 1st order Euler method.\nAs a final comparison, let\u0026rsquo;s look at Newton\u0026rsquo;s law of cooling one last time. Figure 6 shows how our 4th order solver compares to our previous test with the Euler method.\nFigure 6: Comparing the accuracy of the RK4 and Euler methods on the cooling equation. Tsurr = 20 C, k = 1, h = 0.5\nFigure 5 and 6 code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 \u0026#39;\u0026#39;\u0026#39; Implementing and comparing the Runge-Kutta 2nd and 4th order methods. We can also compare to the Euler method (1st order). Created by: simmeon Last Modified: 29/04/24 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) # Define some functions to test our solvers with def dydt(t, y): return 3 * t**2 + 20 * np.cos(10 * t) def analytical_solution(t): return t**3 + 2 * np.sin(10 * t) # Analytical solution to the ODE # # Define our cooling function # def solve_T(t, T0): # return T_surr + (T0 - T_surr)*np.exp(-k*t) # # Define the derivative (only depends on the current temp, not time) # def dTdt(T): # return -k * (T - T_surr) def euler_solver(dydt, t0, y0, t_end, h): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the Euler method Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y t_end: Final time for integration h: Step size Returns: An array of y values for each time step \u0026#34;\u0026#34;\u0026#34; t = np.arange(t0, t_end+h, h) y = np.zeros_like(t) y[0] = y0 for i in range(1, len(t)): y[i] = y[i - 1] + h * dydt(t[i - 1], y[i - 1]) return y def rk2(dydt, t0, y0, t_end, h): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the 2nd order Runge-Kutta method Args: dydt: The derivative function to integrate t0: Initial value of time y0: Initial value of y t_end: Final time for integration h: Step size Returns: An array of y values for each time step \u0026#34;\u0026#34;\u0026#34; t = np.arange(t0, t_end+h, h) # Array of time points y = np.zeros_like(t) y[0] = y0 for i in range(1, len(t)): k1 = h * dydt(t[i - 1], y[i - 1]) k2 = h * dydt(t[i - 1] + h, y[i - 1] + k1) y[i] = y[i - 1] + (k1 + k2) / 2 return y def rk4(dydt, t0, y0, t_end, h): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the 4th order Runge-Kutta method Args: dydt: The derivative function to integrate t0: Initial value of time y0: Initial value of y t_end: Final time for integration h: Step size Returns: An array of y values for each time step \u0026#34;\u0026#34;\u0026#34; t = np.arange(t0, t_end+h, h) y = np.zeros_like(t) y[0] = y0 for i in range(1, len(t)): k1 = dydt(t[i - 1], y[i - 1]) k2 = dydt(t[i - 1] + h / 2, y[i - 1] + k1 / 2) k3 = dydt(t[i - 1] + h / 2, y[i - 1] + k2 / 2) k4 = dydt(t[i], y[i - 1] + k3) y[i] = y[i - 1] + (k1 + 2 * k2 + 2 * k3 + k4) / 6 * h return y # Define simulation parameters t0 = 0 y0 = 0 t_end = 5 h = 0.1 # Solve using both methods y_euler = euler_solver(dydt, t0, y0, t_end, h) y_rk2 = rk2(dydt, t0, y0, t_end, h) y_rk4 = rk4(dydt, t0, y0, t_end, h) # Get analytical solution t = np.arange(t0, t_end+h, h) # Array of time points y_analytical = analytical_solution(t) # Calculate errors error_euler = np.abs(y_euler - y_analytical) error_rk2 = np.abs(y_rk2 - y_analytical) error_rk4 = np.abs(y_rk4 - y_analytical) # Plot the numerical solutions plt.subplot(1,2,1) plt.plot(t, y_euler, label=\u0026#39;Euler\u0026#39;) plt.plot(t, y_rk2, label=\u0026#39;RK2\u0026#39;) plt.plot(t, y_rk4, label=\u0026#39;RK4\u0026#39;) plt.plot(t, y_analytical, label=\u0026#39;Analytical\u0026#39;, linestyle=\u0026#39;--\u0026#39;) plt.xlabel(\u0026#39;Time (t)\u0026#39;, fontsize=20) plt.ylabel(\u0026#39;y(t)\u0026#39;, fontsize=20) plt.title(\u0026#39;Numerical Solutions\u0026#39;, fontsize=20) plt.legend(fontsize=15) # Plot the errors plt.subplot(1,2,2) plt.plot(t, error_euler, label=\u0026#39;Euler\u0026#39;) plt.plot(t, error_rk2, label=\u0026#39;RK2\u0026#39;) plt.plot(t, error_rk4, label=\u0026#39;RK4\u0026#39;) plt.xlabel(\u0026#39;Time (t)\u0026#39;, fontsize=20) plt.ylabel(\u0026#39;Abs Error\u0026#39;, fontsize=20) plt.title(\u0026#39;Absolute Errors of the Numerical Solutions\u0026#39;, fontsize=20) plt.legend(fontsize=15) plt.show() Even with a relatively large step size, the 4th order method is still much better than the Euler method. In fact, we could make the step size even bigger and still have a solution that fell within some tiny tolerance.\nBut how can we choose a good value to make this step size? If we want to be as efficient as possible, each step would as large as it can be while staying within the tolerance we want\u0026hellip;\nStep Sizing Constant Step Size Issues Take the following function,\n$$ y = \\sin{(t^5)} \\\\ {} \\\\ \\dfrac{dy}{dt} = 5t^4 \\cos{(t^5)} $$\nWe can use the derivative to solve for \\( y \\) using one of our numerical methods from earlier. Figure 7 shows us doing this using the Euler method, along with the error in our solution.\nFigure 7: Euler method solution and error using a constant step size, h = 0.05.\nFigure 7 code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 \u0026#39;\u0026#39;\u0026#39; Seeing how a constant step size numerical solver can have wildly varying error on diffrent steps. Created by: simmeon Last Modified: 18/05/24 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) # Define function and its derivative def dydt(t, y): return 5*t**4 * np.cos(t**5) def analytical_solution(t): return np.sin(t**5) # Our Euler numerical solver def euler_solver(dydt, t0, y0, t_end, h): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the Euler method Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y t_end: Final time for integration h: Step size Returns: An array of y values for each time step \u0026#34;\u0026#34;\u0026#34; t = np.arange(t0, t_end+h, h) y = np.zeros_like(t) y[0] = y0 for i in range(1, len(t)): y[i] = y[i - 1] + h * dydt(t[i - 1], y[i - 1]) return t, y # Define simulation parameters t0 = 0 y0 = 0 t_end = 2 h = 0.05 # we are using a constant step size here # Numerically integrate t_euler, y_euler = euler_solver(dydt, t0, y0, t_end, h) # Get analytical solution t_analytical = np.arange(t0, t_end+h, 0.001) y_analytical = analytical_solution(t_analytical) # Get error y_analytical_sampled = y_analytical[0:-1:50] error = np.abs(y_euler - y_analytical_sampled) # Plotting plt.subplot(2,1,1) plt.plot(t_analytical, y_analytical, label=\u0026#39;Analytical\u0026#39;, linestyle=\u0026#39;-\u0026#39;) plt.plot(t_euler, y_euler, label=\u0026#39;Euler method\u0026#39;) plt.title(\u0026#34;Error when integrating with a constant step size\u0026#34;, fontsize=20) plt.ylabel(\u0026#39;y\u0026#39;, fontsize=20) plt.legend(fontsize=15) plt.subplot(2,1,2) plt.plot(t_euler, error) plt.ylabel(\u0026#34;absolute error\u0026#34;, fontsize=20) plt.xlabel(\u0026#39;t\u0026#39;, fontsize=20) plt.show() The method works well initially when the function is changing slowly. However, when the function is changing more rapidly, our Euler method solver error gets large. You can try increasing the time to integrate over and see how the error becomes even bigger.\nThis is happening because our step size (\\( h = 0.05 \\)) becomes too large to properly capture how the function is changing. We could make our step size smaller, but we don\u0026rsquo;t need that additional resolution for the start of the function, it\u0026rsquo;s already pretty accurate.\nUsing Multiple Step Sizes Instead of decreasing the step size for the entire integration period, it would be much more efficient if we could just decrease it where we need to.\nIn this example, we could try switching to a smaller step size (say, \\( h = 0.01 \\)) after \\( t = 1 \\) when our solution starts getting less accurate. Doing this gives the solution shown in Figure 8.\nFigure 8: Euler method solution and error using two step sizes: t \u0026lt;= 1, h = 0.05; t \u0026gt; 1, h = 0.01\nFigure 8 code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 \u0026#39;\u0026#39;\u0026#39; Seeing how we can improve our solution and efficiency by using two different step sizes at different times. Created by: simmeon Last Modified: 18/05/24 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) # Define function and its derivative def dydt(t, y): return 5*t**4 * np.cos(t**5) def analytical_solution(t): return np.sin(t**5) # Our Euler numerical solver def euler_solver(dydt, t0, y0, t_end, h): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the Euler method Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y t_end: Final time for integration h: Step size Returns: An array of y values for each time step \u0026#34;\u0026#34;\u0026#34; t = np.arange(t0, t_end+h, h) y = np.zeros_like(t) y[0] = y0 for i in range(1, len(t)): y[i] = y[i - 1] + h * dydt(t[i - 1], y[i - 1]) return t, y # Define simulation parameters t0 = 0 y0 = 0 t_end1 = 1 t_end2 = 2 h1 = 0.05 # first, bigger step size h2 = 0.01 # second. smaller step size # Numerically integrate t_euler1, y_euler1 = euler_solver(dydt, t0, y0, t_end1, h1) t_euler2, y_euler2 = euler_solver(dydt, t_end1, y_euler1[-1], t_end2, h2) y_euler = np.concatenate((y_euler1, y_euler2)) t_euler = np.concatenate((t_euler1, t_euler2)) # Get analytical solution t_analytical = np.arange(t0, t_end2+h2, 0.001) y_analytical = analytical_solution(t_analytical) # Get error y_analytical_compare = analytical_solution(t_euler) error = np.abs(y_euler - y_analytical_compare) # Plotting plt.subplot(2,1,1) plt.plot(t_analytical, y_analytical, label=\u0026#39;Analytical\u0026#39;, linestyle=\u0026#39;-\u0026#39;) plt.plot(t_euler, y_euler, label=\u0026#39;Euler method\u0026#39;) plt.vlines(t_end1, -1, 1, colors=\u0026#39;white\u0026#39;, alpha=0.5) plt.title(\u0026#34;Error when integrating with two different step sizes\u0026#34;, fontsize=20) plt.ylabel(\u0026#39;y\u0026#39;, fontsize=20) plt.legend(fontsize=15) plt.subplot(2,1,2) plt.plot(t_euler, error) plt.vlines(t_end1, 0, 0.5, colors=\u0026#39;white\u0026#39;, alpha=0.5) plt.ylabel(\u0026#34;absolute error\u0026#34;, fontsize=20) plt.xlabel(\u0026#39;t\u0026#39;, fontsize=20) plt.show() This has made our solution around an order of magnitude more accurate. But how much computational complexity have we saved compared to just solving the entire time interval with the smaller step size? We can summarize this in Table 1.\nTable 1\nStep size Number of iteration steps \\( h = 0.05 \\) 41 \\( t \\leq 1, h = 0.05 \\\\ t\u0026gt;1, h = 0.01\\) 122 \\( h = 0.01 \\) 201 So we are saving a significant amount of computation compared to making the whole interval have a smaller step size.\nChoosing to make the step size smaller at \\( t = 1 \\) was very arbitrary. We could also change the step size in more places \u0026ndash; maybe making it even bigger initially and even smaller for \\( t \u0026gt; 1.5 \\). We should come up with a smart way of choosing when and how to change the step size.\nAdaptive Step Sizing Since we want our solution to be accurate, it would be smart to decrease our step size when our error is getting too big. Also, to keep things efficient, we could make our step size bigger when the error is very small.\nThe issue is we don\u0026rsquo;t have the actual solution, so how can we know what our error is?\nWe need to create some sort of \u0026ldquo;true solution\u0026rdquo; to compare our estimate against. One way we could do this is to use a higher order method, since we know that these should be more accurate and be closer to the actual solution. For example, if we want to use a 1st order Euler solver, we could also calculate a 2nd order Runge-Kutta solution and use that as the \u0026ldquo;true solution\u0026rdquo;.\nSo for every step, we will calculate the next step with two methods: a lower and higher order one. Then, we will use difference in these as our approximation of the error. From that, we can decide whether we should change the step size for the step or if it was ok.\nThe last thing to decide is how much we should change the step size. For now, a simple approach could be to halve the step size if the error was too big, and double it if it was too small.\nMore formally, we will:\nTake a step with the current step size using a 1st and 2nd order method. Compare the solutions to approximate our error. If the \\( error \u0026gt; tol \\), reject the step, halve the step size, and go back to Step 1. If the \\( error \u0026lt; tol \\), accept the step and double the step size for the next step. In code this would look like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def euler_solver(dydt, t0, y0, t_end, tol): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the Euler method with adaptive step sizing. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y t_end: Final time for integration tol: Error tolerance Returns: Arrays of y and t values for each time step \u0026#34;\u0026#34;\u0026#34; h = 0.1 # set some initial step size t = [t0] y_euler = [y0] y_rk2 = [y0] i = 1 while t[-1] \u0026lt; t_end: # Take a Euler step y_euler.append(euler_step(dydt, t[i-1], y_euler[i-1], h)) t.append(t[i-1] + h) # Take a RK2 step, starting from the previous Euler step value y_rk2.append(rk2_step(dydt, t[i-1], y_euler[i-1], h)) isStepGood = False error = np.abs(y_rk2[i] - y_euler[i]) if error \u0026lt; tol: # accept step, make step size bigger for next step isStepGood = True h = h * 2 while not isStepGood: # Take a step with both methods y_euler[i] = euler_step(dydt, t[i-1], y_euler[i-1], h) t[i] = t[i-1] + h y_rk2[i] = rk2_step(dydt, t[i-1], y_euler[i-1], h) # Calculate the error between the methods error = np.abs(y_rk2[i] - y_euler[i]) # Check error to accept or reject the step if error \u0026gt; tol: # reject step, halve step size h = h / 2 else: # accept step, make step size bigger for next step isStepGood = True h = h * 2 i += 1 return t, y_euler If we use this to solve the same function as before, we get the solution shown in Figure 9.\nFigure 9: Euler method solution and error with an adaptive step size, tol = 0.01.\nFigure 9 code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 \u0026#39;\u0026#39;\u0026#39; Implementing an adaptive step size by doubling or halving the step size depending on the error. Created by: simmeon Last Modified: 18/05/24 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) # Define function and its derivative def dydt(t, y): return 5*t**4 * np.cos(t**5) def analytical_solution(t): return np.sin(t**5) def euler_step(dydt, t0, y0, h): \u0026#34;\u0026#34;\u0026#34; Takes a single 1st order integration step and returns the y value. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y h: Step size Returns: The y value after the step. \u0026#34;\u0026#34;\u0026#34; y = y0 + h * dydt(t0, y0) return y def rk2_step(dydt, t0, y0, h): \u0026#34;\u0026#34;\u0026#34; Takes a single 2nd order integration step and returns the y value. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y h: Step size Returns: The y value after the step. \u0026#34;\u0026#34;\u0026#34; k1 = h * dydt(t0, y0) k2 = h * dydt(t0 + h, y0 + k1) y = y0 + (k1 + k2) / 2 return y # Our adaptive Euler numerical solver def euler_solver(dydt, t0, y0, t_end, tol): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the Euler method with adaptive step sizing. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y t_end: Final time for integration tol: Error tolerance Returns: Arrays of y, t and local error values for each time step \u0026#34;\u0026#34;\u0026#34; h = 0.1 # set some initial step size t = [t0] y_euler = [y0] y_rk2 = [y0] error = [0] i = 1 while t[-1] \u0026lt; t_end: # Take a Euler step y_euler.append(euler_step(dydt, t[i-1], y_euler[i-1], h)) t.append(t[i-1] + h) # Take a RK2 step, starting from the previous Euler step value y_rk2.append(rk2_step(dydt, t[i-1], y_euler[i-1], h)) isStepGood = False error.append(np.abs(y_rk2[i] - y_euler[i])) if error[i] \u0026lt; tol: # accept step, make step size bigger for next step isStepGood = True h = h * 2 while not isStepGood: # Take a step with both methods y_euler[i] = euler_step(dydt, t[i-1], y_euler[i-1], h) t[i] = t[i-1] + h y_rk2[i] = rk2_step(dydt, t[i-1], y_euler[i-1], h) # Calculate the error between the methods error[i] = np.abs(y_rk2[i] - y_euler[i]) # Check error to accept or reject the step if error[i] \u0026gt; tol: # reject step, halve step size h = h / 2 else: # accept step, make step size bigger for next step isStepGood = True h = h * 2 i += 1 return t, y_euler, error # Define simulation parameters t0 = 0 y0 = 0 t_end = 2 tol = 1e-2 # Numerically integrate t_euler, y_euler, local_error = euler_solver(dydt, t0, y0, t_end, tol) t_euler = np.array(t_euler) y_euler = np.array(y_euler) # Get analytical solution t_analytical = np.arange(t0, t_end, 0.001) y_analytical = analytical_solution(t_analytical) # Get error y_analytical_compare = analytical_solution(t_euler) error = np.abs(y_euler - y_analytical_compare) # Plotting plt.subplot(3,1,1) plt.plot(t_analytical, y_analytical, label=\u0026#39;Analytical\u0026#39;, linestyle=\u0026#39;-\u0026#39;) plt.plot(t_euler, y_euler, label=\u0026#39;Euler method\u0026#39;) plt.scatter(t_euler, y_euler, s=30, marker=\u0026#39;o\u0026#39;, facecolors=\u0026#39;none\u0026#39;, color=\u0026#39;C1\u0026#39;) plt.title(\u0026#34;Integrating with a adaptive step size, tol = 0.01\u0026#34;, fontsize=20) plt.ylabel(\u0026#39;y\u0026#39;, fontsize=20) plt.legend(fontsize=15) plt.text(0, -0.75, f\u0026#34;Number of steps: {len(t_euler)}\u0026#34;, fontsize=15) plt.subplot(3,1,2) plt.plot(t_euler, error) plt.ylabel(\u0026#34;absolute error\u0026#34;, fontsize=20) plt.xlabel(\u0026#39;t\u0026#39;, fontsize=20) plt.subplot(3,1,3) plt.plot(t_euler, local_error) plt.ylabel(\u0026#34;local step error\u0026#34;, fontsize=20) plt.xlabel(\u0026#39;t\u0026#39;, fontsize=20) plt.show() Each dot represents where a step was taken. We can see how the step size is bigger when the function is changing slowly and smaller when the function changes quickly.\nThe error is interesting. We set a tolerance of \\( 0.01 \\) but the absolute error gets up to nearly \\( 0.1 \\). This is because the tolerance is used to set the allowable error for each step, not the overall (or global) error. Another reason is that our error is an estimate but here we are plotting against the actual, analytical error. Howveer, looking at the local error for each step we can see that it never gets above our tolerance.\nChoosing Better Step Size Changes We used a very simple method of halving the step size if the error was too big or doubling the step size if the error was too small. For the 255 steps we ended up with, the method calculated the next step 746 times. This means that, on average for each step, we changed the step size 2.9 times before finding a value that worked. There is a lot of computation that is wasted there.\nLet\u0026rsquo;s try and come up with a better way of choosing the step size to hopefully make our method more efficient.\nThis will depend on what order methods we are using.\nWe will start by doing this for the 1st order Euler method. We write this as:\n$$ y_{n+1} = y_{n} + h \\dfrac{dy}{dt} \\bigg| _{t_n, y_n} + O(h^2) $$\nThe error in our estimate is proportional to \\( h^2 \\). If we assume that this error is constant over time, we can say\n$$ error = \\Delta = c h^2 $$\nwhere \\( c \\) is some constant value. Then, for some step size, \\( h_1 \\), we would get\n$$ \\Delta _1 = c h_1^2 $$\nWe could also write an equation for what the step size would have to be to get an error equal to our tolerance. Let\u0026rsquo;s call this error \\( \\Delta _0 \\).\n$$ \\Delta _0 = c h_0^2 $$\nThis \\( h_0 \\) is the value that we would want to use for the current step size to. In other words, it will give the largest step that stays within our tolerance.\nWe can use these two equations together through the constant \\( c \\) to get the relation:\n$$ \\frac{\\Delta_1}{h_1^2} = \\frac{\\Delta_0}{h_0^2} $$\nThis can then be arranged to solve for what our step size \\( h_0 \\) should be\n$$ h_0 = h_1 \\bigg(\\frac{\\Delta _0}{\\Delta _1} \\bigg) ^{0.5} $$\nor in more familiar notation\n$$ h_{new} = h_{current} \\bigg(\\frac{tol}{error} \\bigg) ^{0.5} $$\nIn theory, using this formula should mean that we get the correct step size in one iteration instead of two or three. However, in practice, our error is only an approximation and so we should put in a safety factor to make the new step size slightly smaller that the theoretical value:\n$$ h_{new} = 0.9 h_{current} \\bigg(\\frac{tol}{error} \\bigg) ^{0.5} $$\nLet\u0026rsquo;s use this instead of our doubling and halving approach and see how many calculations we use. Note that formula has the correct behaviour for errors that are both larger and smaller than the tolerance. We can see the result in Figure 10.\nFigure 10: Euler method solution and error with a better adaptive step size, tol = 0.01.\nFigure 10 code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 \u0026#39;\u0026#39;\u0026#39; Implementing an adaptive step size with better step sizing. Created by: simmeon Last Modified: 19/05/24 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) # Define function and its derivative def dydt(t, y): return 5*t**4 * np.cos(t**5) def analytical_solution(t): return np.sin(t**5) def euler_step(dydt, t0, y0, h): \u0026#34;\u0026#34;\u0026#34; Takes a single 1st order integration step and returns the y value. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y h: Step size Returns: The y value after the step. \u0026#34;\u0026#34;\u0026#34; y = y0 + h * dydt(t0, y0) return y def rk2_step(dydt, t0, y0, h): \u0026#34;\u0026#34;\u0026#34; Takes a single 2nd order integration step and returns the y value. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y h: Step size Returns: The y value after the step. \u0026#34;\u0026#34;\u0026#34; k1 = h * dydt(t0, y0) k2 = h * dydt(t0 + h, y0 + k1) y = y0 + (k1 + k2) / 2 return y # Our adaptive Euler numerical solver def euler_solver(dydt, t0, y0, t_end, tol): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the Euler method with adaptive step sizing. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y t_end: Final time for integration tol: Error tolerance Returns: Arrays of y, t and local error values for each time step \u0026#34;\u0026#34;\u0026#34; h = 0.1 # set some initial step size t = [t0] y_euler = [y0] y_rk2 = [y0] error = [0] i = 1 count = 0 while t[-1] \u0026lt; t_end: count += 1 # Take a Euler step y_euler.append(euler_step(dydt, t[i-1], y_euler[i-1], h)) t.append(t[i-1] + h) # Take a RK2 step, starting from the previous Euler step value y_rk2.append(rk2_step(dydt, t[i-1], y_euler[i-1], h)) isStepGood = False error.append(np.abs(y_rk2[i] - y_euler[i])) if error[i] \u0026lt; tol: # accept step, make step size bigger for next step isStepGood = True h = 0.9 * h * (tol / error[i]) ** 0.5 while not isStepGood: count += 1 # Take a step with both methods y_euler[i] = euler_step(dydt, t[i-1], y_euler[i-1], h) t[i] = t[i-1] + h y_rk2[i] = rk2_step(dydt, t[i-1], y_euler[i-1], h) # Calculate the error between the methods error[i] = np.abs(y_rk2[i] - y_euler[i]) # Check error to accept or reject the step if error[i] \u0026gt; tol: # reject step, halve step size h = 0.9 * h * (tol / error[i]) ** 0.5 else: # accept step, make step size bigger for next step isStepGood = True h = 0.9 * h * (tol / error[i]) ** 0.5 i += 1 print(\u0026#34;Count: \u0026#34;, count) return t, y_euler, error # Define simulation parameters t0 = 0 y0 = 0 t_end = 2 tol = 1e-2 # Numerically integrate t_euler, y_euler, local_error = euler_solver(dydt, t0, y0, t_end, tol) t_euler = np.array(t_euler) y_euler = np.array(y_euler) # Get analytical solution t_analytical = np.arange(t0, t_end, 0.001) y_analytical = analytical_solution(t_analytical) # Get error y_analytical_compare = analytical_solution(t_euler) error = np.abs(y_euler - y_analytical_compare) # Plotting plt.subplot(3,1,1) plt.plot(t_analytical, y_analytical, label=\u0026#39;Analytical\u0026#39;, linestyle=\u0026#39;-\u0026#39;) plt.plot(t_euler, y_euler, label=\u0026#39;Euler method\u0026#39;) plt.scatter(t_euler, y_euler, s=30, marker=\u0026#39;o\u0026#39;, facecolors=\u0026#39;none\u0026#39;, color=\u0026#39;C1\u0026#39;) plt.title(\u0026#34;Integrating with a adaptive step size, tol = 0.01\u0026#34;, fontsize=20) plt.ylabel(\u0026#39;y\u0026#39;, fontsize=20) plt.legend(fontsize=15) plt.text(0, -0.75, f\u0026#34;Number of steps: {len(t_euler)}\u0026#34;, fontsize=15) plt.subplot(3,1,2) plt.plot(t_euler, error) plt.ylabel(\u0026#34;absolute error\u0026#34;, fontsize=20) plt.subplot(3,1,3) plt.plot(t_euler, local_error) plt.ylabel(\u0026#34;local step error\u0026#34;, fontsize=20) plt.xlabel(\u0026#39;t\u0026#39;, fontsize=20) plt.show() This change used 216 steps with a total of 323 step calulations, meaning we changed the step size 1.5 times per step. This is much closer to the minimum of 1 change per step. It also ended up requiring less steps than previously, being even more efficient.\nThis step change formula we created can be applied to higher order methods, for example if we were using a 4th order method with 5th order errors we would use:\n$$ h_{new} = 0.9 h_{current} \\bigg(\\frac{tol}{error} \\bigg) ^{\\frac{1}{5}} $$\nA Full RK45 Method Finally, we have all the parts to create a working 4th order Runge-Kutta numerical solver with adaptive step sizing based on 5th order errors.\nWe will start by writing a function that takes a single 5th order step. We are going to use the 5th order step to approximate our \u0026ldquo;true solution\u0026rdquo; and compare it to the 4th order step to find our error. The function will then return the 4th order step and the error. One reason this method is so good is because calculating the 5th step reuses a lot of the RK4 step calculations, making it very efficient.\nTo do this we will be using the Dormand-Prince coefficients. These are much messier than the standard coefficients we used before. However, they are very good at minimising the error in the 5th order approximation, which is exactly what we want \u0026ndash; we want that 5th order approximation to be as close to the real solution as possible.\nThe intermediate approximations are calculated as follows:\n$$ \\begin{aligned} \u0026amp;k_1 = hf(t_n, y_n) \\\\ {} \\\\ \u0026amp;k_2 = hf \\Big(t_n + \\frac{1}{5}h, y_n + \\frac{1}{5} k_1 \\Big) \\\\ {} \\\\ \u0026amp;k_3 = hf \\Big(t_n + \\frac{3}{10}h, y_n + \\frac{3}{40} k_1 + \\frac{9}{40} k_2 \\Big) \\\\ {} \\\\ \u0026amp;k_4 = hf \\Big(t_n + \\frac{4}{5}h, y_n + \\frac{44}{45} k_1 - \\frac{56}{15} k_2 + \\frac{32}{9} k_3 \\Big) \\\\ {} \\\\ \u0026amp;k_5 = hf \\Big(t_n + \\frac{8}{9}h, y_n + \\frac{19372}{6561} k_1 - \\frac{25360}{2187} k_2 + \\frac{64448}{6561} k_3 - \\frac{212}{729} k_4 \\Big) \\\\ {} \\\\ \u0026amp;k_6 = hf \\Big(t_n + h, y_n + \\frac{9017}{3168} k_1 - \\frac{355}{33} k_2 - \\frac{46732}{5247} k_3 + \\frac{49}{176} k_4 - \\frac{5103}{18656} k_5 \\Big) \\\\ {} \\\\ \u0026amp;k_7 = hf \\Big(t_n + h, y_n + \\frac{35}{848} k_1 + \\frac{500}{1113} k_3 + \\frac{125}{192} k_4 - \\frac{2187}{6784} k_5 + \\frac{11}{84} k_6 \\Big) \\end{aligned} $$\nThen, the next 4th order step is calculated as\n$$ y_{n+1} = y_n + \\frac{35}{384} k_1 + \\frac{500}{1113} k_3 + \\frac{125}{192} k_4 - \\frac{2187}{6784} k_5 + \\frac{11}{84} k_6 $$\nNote that these are the same coefficients that we use for \\( k_7 \\), which is useful. We can rewrite \\( k_7 \\) as\n$$ k_7 = hf \\Big( t_n + h, y_{n+1} \\Big) $$\nThis value is then the same as \\( k_1 \\) will be in the next step (except for \\( h \\) changing). This means that, except for the first step, we only have to calculate 6 derivatives per step.\nThen, the 5th order step, \\( z_{n+1} \\), is calculated as\n$$ z_{n+1} = y_n + \\frac{5179}{57600} k_1 + \\frac{7571}{16695} k_3 + \\frac{393}{640} k_4 - \\frac{92097}{339200} k_5 + \\frac{187}{2100} k_6 + \\frac{1}{40} k_7 $$\nThis gives the error\n$$ error = \\Big| z_{n+1} - y_{n+1} \\Big| = \\Big| -\\frac{71}{57600} k_1 + \\frac{71}{16695} k_3 - \\frac{71}{1920} k_4 + \\frac{17253}{339200} k_5 - \\frac{22}{525} k_6 + \\frac{1}{40} k_7 \\Big| $$\nand from earlier we know that our step size change will be\n$$ h_{new} = 0.9 h_{current} \\bigg(\\frac{tol}{error} \\bigg) ^{0.2} $$\nLet\u0026rsquo;s write this step function now, along with the function that adaptively steps to return the solution.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 # ----- Dormand-Prince coefficients ----- # # Alpha in notation, coefficients describe what percentage of the full # step we evaluate each derivative at A = np.array([0, 1/5, 3/10, 4/5, 8/9, 1, 1]) # Beta in notation, coefficients describe how much of each previous change in y # we add to the current estimate B = np.array([ [0, 0, 0, 0, 0, 0], [1/5, 0, 0, 0, 0, 0], [3/40, 9/40, 0, 0, 0, 0], [44/45, -56/15, 32/9, 0, 0, 0], [19372/6561, -25360/2187, 64448/6561, -212/729, 0, 0], [9017/3168, -355/33, 46732/5247, 49/176, -5103/18656, 0], [35/384, 0, 500/1113, 125/192, -2187/6784, 11/84] ]) # Weighting for each derivative in the approximation, w in notation W = np.array([35/384, 0, 500/1113, 125/192, -2187/6784, 11/84, 0]) #W = np.array([5179/57600, 0, 7571/16695, 393/640, -92097/339200, 187/2100, 1/40]) # Coefficients for error calculation E = np.array([-71/57600, 0, 71/16695, -71/1920, 17253/339200, -22/525, 1/40]) # ------------------------------------------------------------------ # # Define a function to take a single RK45 step def rk45_step(dydt, t0, y0, h): \u0026#34;\u0026#34;\u0026#34; Takes a single 5th order integration step and returns the 4th order step along with the 5th order error. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y h: Step size Returns: The the 4th order y value along with the 5th order error. \u0026#34;\u0026#34;\u0026#34; k1 = h * dydt(t0, y0) k2 = h * dydt(t0 + A[1] * h, y0 + B[1][0]*k1) k3 = h * dydt(t0 + A[2] * h, y0 + B[2][0]*k1 + B[2][1]*k2) k4 = h * dydt(t0 + A[3] * h, y0 + B[3][0]*k1 + B[3][1]*k2 + B[3][2]*k3) k5 = h * dydt(t0 + A[4] * h, y0 + B[4][0]*k1 + B[4][1]*k2 + B[4][2]*k3 + B[4][3]*k4) k6 = h * dydt(t0 + A[5] * h, y0 + B[5][0]*k1 + B[5][1]*k2 + B[5][2]*k3 + B[5][3]*k4 + B[5][4]*k5) y = y0 + W[0]*k1 + W[1]*k2 + W[2]*k3 + W[3]*k4 + W[4]*k5 + W[5]*k6 k7 = h * dydt(t0 + A[6] * h, y) error = np.abs( E[0]*k1 + E[1]*k2 + E[2]*k3 + E[3]*k4 + E[4]*k5 + E[5]*k6 + E[6]*k7 ) return y, error # Define the full RK45 solver function def rk45_solver(dydt, t0, y0, t_end, tol): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the RK45 method. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y t_end: Final time for integration tol: Error tolerance Returns: Arrays of y, t and local error values for each time step \u0026#34;\u0026#34;\u0026#34; h = 1e-3 # set some initial step size t = [t0] y = [y0] error = [0] i = 1 while t[-1] \u0026lt; t_end: # Take a step t.append(t[i-1] + h) y_step, error_step = rk45_step(dydt, t[i-1], y[i-1], h) y.append(y_step) error.append(error_step) # Update step size after step h = 0.9 * h * (tol / error[i]) ** 0.2 isStepGood = False if error[i] \u0026lt; tol: # accept step isStepGood = True while not isStepGood: # If there was too much error... # Take a step t[i] = t[i-1] + h # update our time value with the new step size y_step, error_step = rk45_step(dydt, t[i-1], y[i-1], h) y[i] = y_step error[i] = error_step # Update step size after step h = 0.9 * h * (tol / error[i]) ** 0.2 # Check error to accept or reject the step if error[i] \u0026lt; tol: isStepGood = True i += 1 return t, y, error Using this step function along with updating the step size gives the result in Figure 11.\nFigure 11: RK45 Solver with error.\nFigure 11 code (full solver code + graphing)\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 \u0026#39;\u0026#39;\u0026#39; Implementing a complete RK45 algorithm. Created by: simmeon Last Modified: 19/05/24 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) # Define function (for analytical solution) and its derivative def dydt(t, y): return 5*t**4 * np.cos(t**5) def analytical_solution(t): return np.sin(t**5) # ----- Dormand-Prince coefficients ----- # # Alpha in notation, coefficients describe what percentage of the full # step we evaluate each derivative at A = np.array([0, 1/5, 3/10, 4/5, 8/9, 1, 1]) # Beta in notation, coefficients describe how much of each previous change in y # we add to the current estimate B = np.array([ [0, 0, 0, 0, 0, 0], [1/5, 0, 0, 0, 0, 0], [3/40, 9/40, 0, 0, 0, 0], [44/45, -56/15, 32/9, 0, 0, 0], [19372/6561, -25360/2187, 64448/6561, -212/729, 0, 0], [9017/3168, -355/33, 46732/5247, 49/176, -5103/18656, 0], [35/384, 0, 500/1113, 125/192, -2187/6784, 11/84] ]) # Weighting for each derivative in the approximation, w in notation W = np.array([35/384, 0, 500/1113, 125/192, -2187/6784, 11/84, 0]) #W = np.array([5179/57600, 0, 7571/16695, 393/640, -92097/339200, 187/2100, 1/40]) # Coefficients for error calculation E = np.array([-71/57600, 0, 71/16695, -71/1920, 17253/339200, -22/525, 1/40]) # ------------------------------------------------------------------ # # Define a function to take a single RK45 step # To improve efficiency, k7 could be reused as k1 in the following step def rk45_step(dydt, t0, y0, h): \u0026#34;\u0026#34;\u0026#34; Takes a single 5th order integration step and returns the 4th order step along with the 5th order error. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y h: Step size Returns: The the 4th order y value along with the 5th order error. \u0026#34;\u0026#34;\u0026#34; k1 = h * dydt(t0, y0) k2 = h * dydt(t0 + A[1] * h, y0 + B[1][0]*k1) k3 = h * dydt(t0 + A[2] * h, y0 + B[2][0]*k1 + B[2][1]*k2) k4 = h * dydt(t0 + A[3] * h, y0 + B[3][0]*k1 + B[3][1]*k2 + B[3][2]*k3) k5 = h * dydt(t0 + A[4] * h, y0 + B[4][0]*k1 + B[4][1]*k2 + B[4][2]*k3 + B[4][3]*k4) k6 = h * dydt(t0 + A[5] * h, y0 + B[5][0]*k1 + B[5][1]*k2 + B[5][2]*k3 + B[5][3]*k4 + B[5][4]*k5) y = y0 + W[0]*k1 + W[1]*k2 + W[2]*k3 + W[3]*k4 + W[4]*k5 + W[5]*k6 k7 = h * dydt(t0 + A[6] * h, y) error = np.abs( E[0]*k1 + E[1]*k2 + E[2]*k3 + E[3]*k4 + E[4]*k5 + E[5]*k6 + E[6]*k7 ) return y, error # Define the full RK45 solver function def rk45_solver(dydt, t0, y0, t_end, tol): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the RK45 method. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y t_end: Final time for integration tol: Error tolerance Returns: Arrays of y, t and local error values for each time step \u0026#34;\u0026#34;\u0026#34; h = 1e-3 # set some initial step size t = [t0] y = [y0] error = [0] i = 1 while t[-1] \u0026lt; t_end: # Take a step t.append(t[i-1] + h) y_step, error_step = rk45_step(dydt, t[i-1], y[i-1], h) y.append(y_step) error.append(error_step) # Update step size after step h = 0.9 * h * (tol / error[i]) ** 0.2 isStepGood = False if error[i] \u0026lt; tol: # accept step isStepGood = True while not isStepGood: # If there was too much error... # Take a step t[i] = t[i-1] + h # update our time value with the new step size y_step, error_step = rk45_step(dydt, t[i-1], y[i-1], h) y[i] = y_step error[i] = error_step # Update step size after step h = 0.9 * h * (tol / error[i]) ** 0.2 # Check error to accept or reject the step if error[i] \u0026lt; tol: isStepGood = True i += 1 return t, y, error # Define simulation parameters t0 = 0 y0 = 0 t_end = 2 tol = 1e-6 # Numerically integrate t_rk45, y_rk45, local_error = rk45_solver(dydt, t0, y0, t_end, tol) t_rk45 = np.array(t_rk45) y_rk45 = np.array(y_rk45) # Get analytical solution t_analytical = np.arange(t0, t_end, 0.001) y_analytical = analytical_solution(t_analytical) # Get error y_analytical_compare = analytical_solution(t_rk45) error = np.abs(y_rk45 - y_analytical_compare) # Plotting plt.subplot(3,1,1) plt.plot(t_analytical, y_analytical, label=\u0026#39;Analytical\u0026#39;, linestyle=\u0026#39;-\u0026#39;) plt.plot(t_rk45, y_rk45, label=\u0026#39;RK45 method\u0026#39;) #plt.scatter(t_rk45, y_rk45, s=30, marker=\u0026#39;o\u0026#39;, facecolors=\u0026#39;none\u0026#39;, color=\u0026#39;C1\u0026#39;) plt.title(f\u0026#34;RK45 Solver, tol = {tol}\u0026#34;, fontsize=20) plt.ylabel(\u0026#39;y\u0026#39;, fontsize=20) plt.legend(fontsize=15) plt.text(0, -0.75, f\u0026#34;Number of steps: {len(t_rk45)}\u0026#34;, fontsize=15) plt.subplot(3,1,2) plt.plot(t_rk45, error) plt.ylabel(\u0026#34;absolute error\u0026#34;, fontsize=20) plt.subplot(3,1,3) plt.plot(t_rk45, local_error) plt.ylabel(\u0026#34;local step error\u0026#34;, fontsize=20) plt.xlabel(\u0026#39;t\u0026#39;, fontsize=20) plt.show() As we can see, the method is incredibly efficient and accurate. In comparison, our 1st order adaptive method used over 200 steps for a tolerance of only 1e-03. This combination of efficiency and accuracy makes the RK45 method an incredibly good general solver.\nA Final Application So far we have used mostly trivial or contrived examples to demonstrate why the RK45 solver is good. To finish off, we will use a real-world example where the adaptability, efficiency, and accuracy of the RK45 solver is very useful.\nWe will be using the solver to propagate a satellite orbit around the earth. We will skip some details of deriving the relevant physics, but the important equation is:\n$$ F = \\frac{G m_{earth} m_{sat}}{r^2} $$\nLet\u0026rsquo;s define the gravitational parameter, \\( \\mu \\)\n$$ \\mu = G(m_{earth} + m_{sat}) $$\nBy assuming an inertial reference frame centred on the Earth, we get\n$$ \\ddot{\\vec{r}} = - \\frac{G(m_{earth} + m_{sat})}{r^3}\\vec{r} \\\\ {} \\\\ \\ddot{\\vec{r}} = - \\frac{\\mu}{r^3}\\vec{r} $$\nwhere \\( \\vec{r} \\) is the position vector of the satellite we want to solve for.\nThis is currently a second order ODE. We can use a state space to transform it into being first order:\n$$ \\bold{x} = \\begin{bmatrix} r_x \\\\ r_y \\\\ \\dot{r}_x \\\\ \\dot{r}_y \\end{bmatrix} \\\\ {} \\\\ {} \\\\ \\bold{\\dot{x}} = \\begin{bmatrix} \\dot{r}_x \\\\ {} \\\\ \\dot{r}_y \\\\ {} \\\\ \\ddot{r}_x \\\\ {} \\\\ \\ddot{r}_y \\\\ \\end{bmatrix} = \\begin{bmatrix} \\dot{r}_x \\\\ {} \\\\ \\dot{r}_y \\\\ {} \\\\ -\\Large \\frac{\\mu r_x}{|r|^3} \\\\ {} \\\\ -\\Large \\frac{\\mu r_y}{|r|^3} \\\\ \\end{bmatrix} \\\\ {} \\\\ {} \\\\ \\bold{\\dot{x}} = \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ -\\Large \\frac{\\mu}{|r|^3} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; -\\Large \\frac{\\mu}{|r|^3} \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} \\bold{x} $$\nThis is now a system of first order ODEs where we are solving for \\( \\bold{x} \\). With some small code changes to allow for vector inputs, Figure 12 shows the resulting satellite position and velocity for some initial state. To deal with multiple ODE systems with arrays of solutions instead of just a single value, we use the maximum error as our test for whether the step was good or not.\nFigure 12: Propagating an orbit with RK45.\nFigure 12 code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 \u0026#39;\u0026#39;\u0026#39; An application of the RK45 ODE solver. We will solve for the position of a satellite around Earth given an initial position and velocity. Created by: simmeon Last Modified: 19/05/24 License: MIT \u0026#39;\u0026#39;\u0026#39; import matplotlib.pyplot as plt import numpy as np plt.style.use(\u0026#39;https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\u0026#39;) # Define derivative def dxdt(t, x): r_mag = np.sqrt(x[0]**2 + x[1]**2) mu = 398600.4415; # [km^3 / (kg*s^2)] return np.array([x[2], x[3], -mu * x[0] / (r_mag**3), -mu * x[1] / (r_mag**3)]) # ----- Dormand-Prince coefficients ----- # # Alpha in notation, coefficients describe what percentage of the full # step we evaluate each derivative at A = np.array([0, 1/5, 3/10, 4/5, 8/9, 1, 1]) # Beta in notation, coefficients describe how much of each previous change in y # we add to the current estimate B = np.array([ [0, 0, 0, 0, 0, 0], [1/5, 0, 0, 0, 0, 0], [3/40, 9/40, 0, 0, 0, 0], [44/45, -56/15, 32/9, 0, 0, 0], [19372/6561, -25360/2187, 64448/6561, -212/729, 0, 0], [9017/3168, -355/33, 46732/5247, 49/176, -5103/18656, 0], [35/384, 0, 500/1113, 125/192, -2187/6784, 11/84] ]) # Weighting for each derivative in the approximation, w in notation W = np.array([35/384, 0, 500/1113, 125/192, -2187/6784, 11/84, 0]) #W = np.array([5179/57600, 0, 7571/16695, 393/640, -92097/339200, 187/2100, 1/40]) # Coefficients for error calculation E = np.array([-71/57600, 0, 71/16695, -71/1920, 17253/339200, -22/525, 1/40]) # ------------------------------------------------------------------ # # Define a function to take a single RK45 step # To improve efficiency, k7 can be reused as k1 in the following step def rk45_step(dydt, t0, y0, h): \u0026#34;\u0026#34;\u0026#34; Takes a single 5th order integration step and returns the 4th order step along with the 5th order error. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y h: Step size Returns: The the 4th order y value along with the 5th order error. \u0026#34;\u0026#34;\u0026#34; k1 = h * dydt(t0, y0) k2 = h * dydt(t0 + A[1] * h, y0 + B[1][0]*k1) k3 = h * dydt(t0 + A[2] * h, y0 + B[2][0]*k1 + B[2][1]*k2) k4 = h * dydt(t0 + A[3] * h, y0 + B[3][0]*k1 + B[3][1]*k2 + B[3][2]*k3) k5 = h * dydt(t0 + A[4] * h, y0 + B[4][0]*k1 + B[4][1]*k2 + B[4][2]*k3 + B[4][3]*k4) k6 = h * dydt(t0 + A[5] * h, y0 + B[5][0]*k1 + B[5][1]*k2 + B[5][2]*k3 + B[5][3]*k4 + B[5][4]*k5) y = y0 + W[0]*k1 + W[1]*k2 + W[2]*k3 + W[3]*k4 + W[4]*k5 + W[5]*k6 k7 = h * dydt(t0 + A[6] * h, y) error = np.abs( E[0]*k1 + E[1]*k2 + E[2]*k3 + E[3]*k4 + E[4]*k5 + E[5]*k6 + E[6]*k7 ) return y, error # Define the full RK45 solver function def rk45_solver(dydt, t0, y0, t_end, tol): \u0026#34;\u0026#34;\u0026#34; Solves a first-order ODE using the RK45 method. Args: dydt: The differential equation (dy/dt) as a Python function t0: Initial value of time y0: Initial value of y t_end: Final time for integration tol: Error tolerance Returns: Arrays of y, t and local error values for each time step \u0026#34;\u0026#34;\u0026#34; h = 1e-3 # set some initial step size t = [t0] y = np.array([y0]) error = [0] i = 1 while t[-1] \u0026lt; t_end: # Take a step t.append(t[i-1] + h) y_step, error_step = rk45_step(dydt, t[i-1], y[i-1], h) y = np.append(y, np.array([y_step]), axis=0) error.append(error_step) # Update step size after step h = 0.9 * h * (tol / max(error[i])) ** 0.2 isStepGood = False if max(error[i]) \u0026lt; tol: # accept step isStepGood = True while not isStepGood: # If there was too much error... # Take a step t[i] = t[i-1] + h # update our time value with the new step size y_step, error_step = rk45_step(dydt, t[i-1], y[i-1], h) y[i] = y_step error[i] = error_step # Update step size after step h = 0.9 * h * (tol / max(error[i])) ** 0.2 # Check error to accept or reject the step if max(error[i]) \u0026lt; tol: isStepGood = True i += 1 return t, y, error # Define orbit parameters mu = 398600.4415 rp = 6678 e = 0.9 a = rp/(1-e) T = 2*np.pi*np.sqrt(a**3/mu) # Define simulation parameters t0 = 0 x0 = np.array([rp, 0, 0, np.sqrt(2*mu/rp - mu/a)]) t_end = T tol = 1e-12 # Numerically integrate t_rk45, y_rk45, local_error = rk45_solver(dxdt, t0, x0, t_end, tol) y_rk45 = np.array(y_rk45) # Plotting plt.subplot(2,1,1) plt.plot(y_rk45[:, 0], y_rk45[:, 1]) plt.axis(\u0026#39;equal\u0026#39;) plt.scatter(0, 0, s=50) # dot for Earth plt.title(\u0026#34;Satellite Orbit Around Earth\u0026#34;, fontsize=20) plt.ylabel(\u0026#39;y [km]\u0026#39;, fontsize=20) plt.xlabel(\u0026#39;x [km]\u0026#39;, fontsize=20) plt.subplot(2,1,2) plt.plot(t_rk45, np.sqrt(y_rk45[:, 2]**2 + y_rk45[:, 3]**2), color=\u0026#39;C1\u0026#39;) plt.title(\u0026#34;Satellite Velocity\u0026#34;, fontsize=20) plt.ylabel(\u0026#39;velocity [km/s]\u0026#39;, fontsize=20) plt.xlabel(\u0026#39;t [s]\u0026#39;, fontsize=20) plt.show() The adaptive step size is able to deal with velocity changing by orders of magnitude. With a constant step size, we would be wasting a huge amount of computation on the far side of the orbit. The accuracy of the RK45 method also allows us to use very small tolerances (1e-12) and still have the computation time be relatively quick.\nConclusion This guide should serve as a single comprehensive reference for what the RK45 method is, how it is derived, and how and why we use it to solve engineering problems. There is still room to improve on the ideas and code that have been shown here (the code lacks a lot of polish and dealing with edge cases for example).\nHowever, for those like me who wanted to understand where this method comes from and why it is so good, I hope this has been a useful reference. The goal was to make these quite dense mathematical concepts as clear as possible for other engineers like me.\nReferences [1] Roson J. S., \u0026ldquo;The Runge-Kutta Equations by Quadrature Methods\u0026rdquo;, 1967, NASA.\n[2] \u0026ldquo;Runge-Kutta Methods\u0026rdquo;, 10.001: Numerical Solution of Ordinary Differential Equations\n[3] Explanation and proof of the 4th order Runge-Kutta method\n[4] Numerical Recipes in C: The Art of Scientific Computing\n[5] S. Brorson, \u0026ldquo;Numerically Solving Ordinary Differential Equations\u0026rdquo;\n[6] Adaptive Runge-Kutta Methods | Lecture 54 | Numerical Methods for Engineers\n[7] Dormand, J. R. and P. J. Prince, “A family of embedded Runge-Kutta formulae,” J. Comp. Appl. Math., Vol. 6, 1980, pp. 19–26.\n[8] Dormand-Prince Method\n","permalink":"http://localhost:1313/blog/posts/rk45/rk45/","summary":"\u003c!-- KaTeX --\u003e\r\n\u003clink rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css\" integrity=\"sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq\" crossorigin=\"anonymous\"\u003e\r\n\u003cscript defer src=\"https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js\" integrity=\"sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\r\n\u003cscript defer src=\"https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js\" integrity=\"sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI\" crossorigin=\"anonymous\" onload=\"renderMathInElement(document.body);\"\u003e\u003c/script\u003e\r\n\r\n\n\u003ch2 id=\"the-initial-value-problem\"\u003eThe Initial Value Problem\u003c/h2\u003e\n\u003cp\u003eIn engineering, we often encounter systems that evolve over time, such as circuits, mechanical systems, or chemical reactions. These systems are best described using differential equations.\nFor example, Newton\u0026rsquo;s law of cooling states:\u003c/p\u003e\n\u003cp\u003e$$\n\\dfrac{dT}{dt} = -k(T - T_{surr})\n$$\u003c/p\u003e\n\u003cp\u003ewhere \u003cstrong\u003eT\u003c/strong\u003e is the temperature of some point, \u003cstrong\u003ek\u003c/strong\u003e is a proportionality constant, and \u003cstrong\u003eT\u003csub\u003esurr\u003csub\u003e\u003c/strong\u003e is the temperature surrounding the point of interest.\u003c/p\u003e","title":"An Engineer's Guide to the Runge-Kutta (RK45) Method"}]