---
title: "An Engineer's Guide to the Runge-Kutta (RK45) Method"
date: '2024-05-18'
# weight: 1
# aliases: ["/first"]
tags: ["runge kutta", "ode", "numerical integration", "rk45", "python"]
author: "simmeon"
showToc: true
TocOpen: false
draft: false
hidemeta: false
comments: false
description: "Understanding what the RK45 method is, a complete derivation, and applying it to engineering problems."
canonicalURL: "https://simmeon.github.io/blog/post/rk45.md"
disableHLJS: true # to disable highlightjs
disableShare: false
disableHLJS: false
hideSummary: false
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
ShowRssButtonInSectionTermList: true
UseHugoToc: false
cover:
    image: "" # image path/url
    alt: "<alt text>" # alt text
    caption: "<text>" # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: true # only hide on current single page
editPost:
    URL: "https://github.com/simmeon/blog/content"
    Text: "Suggest Changes" # edit text
    appendFilePath: true # to append file path to Edit link
---

{{< math.inline >}}
{{ if or .Page.Params.math .Site.Params.math }}

<!-- KaTeX -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
{{ end }}
{{</ math.inline >}}



## The Initial Value Problem

In engineering, we often encounter systems that evolve over time, such as circuits, mechanical systems, or chemical reactions. These systems are best described using differential equations.
For example, Newton's law of cooling states:

$$
    \dfrac{dT}{dt} = -k(T - T_{surr})
$$


where **T** is the temperature of some point, **k** is a proportionality constant, and **T<sub>surr<sub>** is the temperature surrounding the point of interest.

To solve this equation is to find a solution for the temperature, **T**, over time. In this case, this is fairly straightforward and we can come up with an analytical solution of the form:

$$
    T(t) = T_{surr} + (T(0) - T_{surr})e^{-kt}
$$

Notice that the solution ***depends on the initial temperature***, as **Figure 1** shows. We need a point of reference to define the solution that is relevant to our system.

{{< figure src="../img/heat-transfer.png" align=center caption="**Figure 1**: Temperature of a point over time with different initial temperatures. T<sub>surr</sub> = 20 C, k = 1." >}}

{{< collapse summary="Figure 1 code" >}}
 
```python {linenos=inline}
'''
Plotting the temperature of a point with different starting temperatures.

Created by: simmeon
Last Modified: 28/04/24
License: MIT

'''

import matplotlib.pyplot as plt
import numpy as np

# Define system parameters
T_surr = 20     # surrounding temperature of 20 C
k = 1           # proportionality constant of 1 for simplicity

# Define our cooling function
def solve_T(t, T0):
    return T_surr + (T0 - T_surr)*np.exp(-k*t)

# Create an array of times from 0-5 seconds
t = np.arange(0, 5, 0.1)

# Solve and plot the solutions for different initial values of T
T0_choices = [30, 25, 20, 15, 10]

plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle')

for T0 in T0_choices:
    T = solve_T(t, T0)
    plt.plot(t, T, label='T0 = {} C'.format(T0))

plt.title('Temperature over time', fontsize=20)
plt.xlabel('Time [s]', fontsize=20)
plt.ylabel('Temperature [C]', fontsize=20)
plt.xlim(0, 5)
plt.ylim(10, 30)
plt.grid(alpha=0.7)
plt.legend()
plt.show()
```
{{< /collapse >}}

The initial value problem deals with solving these ordinary differential equations where we have an initial state of the system, eg. the initial temperature is 25 C. However, often these systems are hard or impossible to solve analytically. To solve them, we use numerical methods to approximate the solution. Let's look at how we might be able to do that...

## Numerical Methods

Let's take the previous cooling equation,

$$
    \dfrac{dT}{dt} = -k(T - T_{surr})
$$

 and assume we can't find an analytical solution. Let's consider the information we *do* have that could help us approximate the solution. We have an expression for the derivative that we can calculate at any time, ***t***, assuming we know the current temperature at that time. We know the temperature at some initial time (***t*** = 0 in this case), so we can calculate the derivative at that time. But how does this help us find the temperature at other times?

### The Euler Method

 From calculus we know that we get the derivative of a function by taking two points on the function and approximating the derivative as if it were a striaght line. As the distance, ***h***, between the two points gets closer to 0, the approximation gets better. Formally,

 $$
    f^{\prime}(x) = \lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h}
 $$

 We can rearrange this and assume a finite step size, ***h***, to get

 $$
    f(x+h) \approx f(x) + h f^{\prime}(x)
 $$

 which gives us an expression to approximate the next step of a function using only the current known function value and the function derivative. We can see what this looks like with different step sizes in **Figure 2**.

 {{< figure src="../img/the_derivative.png" align=center caption="**Figure 2**: Estimating the value of y = x<sup>2</sup> with different step sizes." >}}

{{< collapse summary="Figure 2 code" >}}

```python {linenos=inline}
'''
Visualising how the derivative can be used to estimate the value of a function.

Created by: simmeon
Last Modified: 28/04/24
License: MIT

'''

import matplotlib.pyplot as plt
import numpy as np

plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle')

# Define some function we want to find the value of
def func(t):
    return t**2

# Define the derivative line of the function
def dydt(t, t0, y0):
    m = 2 * t0
    return m * (t-t0) + y0

# Create an array of time values to plot the function
t_func = np.arange(0.5, 2, 0.1)

# Estimation parameters
t0 = 1.0                    # known initial value
y0 = func(t0)               # known initial value
h = [1, 0.5, 0.2]           # step size options

# Plotting
# Find and plot function values
y_func = func(t_func)
plt.plot(t_func, y_func)
plt.plot(t0, y0, 'x', color='C0')

# Plot the derivative line for each step size
colours = ['C1', 'C2', 'C3']
for i in range(len(h)):
    t_dydt = np.arange(t0, t0 + h[i], 0.1)
    y_dydt = dydt(t_dydt, t0, y0)
    plt.plot(t_dydt, y_dydt, color=colours[i], label='Step = {:.1f}'.format(h[i]))
    plt.plot(t_dydt[-1], y_dydt[-1], 'o', color=colours[i])

# Show plot
plt.title("Estimating a function by using the derivative", fontsize=20)
plt.legend(loc='lower right', fontsize=10)
plt.show()
```

{{< /collapse >}}

This is Euler's method for solving the initial value problem. We might also write it as:

$$
    f_{k+1} \approx f_{k} + h f^{\prime}(x_{k})
$$

for some index, ***k***.

We can iterate through time with this method, using the newly found *f<sub>k+1</sub>* as our new *f<sub>k</sub>* and so on. In code, that would look like the following: 

```python {linenos=inline, hl_lines=[14]}
def euler(dydt, y0, t0, t_end, h=0.1):
    '''
    Takes the derivative function, an initial condition, 
    the time we want to integrate until, and a step size.

    Returns arrays of time and y values.
    '''
    t = np.arange(t0, t_end+h, h)
    y = np.zeros(len(t))

    y[0] = y0

    for i in range(1,len(t)):
        y[i] = y[i-1] + h * dydt(y[i-1])

    return t, y
```
The highlighted line shows the Euler method itself where the next value of the function is estimated.

Let's try do that with Newton's cooling law and see how the result compares to the analytical solution. **Figure 3** shows what that would look like.

{{< figure src="../img/euler_method.png" align=center caption="**Figure 3**: Analytical and Euler method solutions to Newton's cooling law. T<sub>surr</sub> = 20 C, k = 1." >}}

{{< collapse summary="Figure 3 code" >}}

```python {linenos=inline}
'''
Performing Euler integration to solve the cooling equation. We will compare with the analytical solution.

Created by: simmeon
Last Modified: 28/04/24
License: MIT

'''

import matplotlib.pyplot as plt
import numpy as np

plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle')

# Define system
T_surr = 20     # surrounding temperature of 20 C
k = 1           # proportionality constant of 1 for simplicity

# Define our cooling function
def solve_T(t, T0):
    return T_surr + (T0 - T_surr)*np.exp(-k*t)

# Define the derivative (only depends on the current temp, not time)
def dTdt(T):
    return -k * (T - T_surr)

# Define our Euler integration function
def euler(dTdt, T0, t0, t_end, h=0.1):
    '''
    Takes the derivative function, an initial condition, the time we want to integrate until,
    and a step size.

    Returns arrays of time and temperature values
    '''
    t = np.arange(t0, t_end+h, h)
    T = np.zeros(len(t))

    T[0] = T0

    for i in range(1,len(t)):
        T[i] = T[i-1] + h * dTdt(T[i-1])

    return t, T


# Parameters
t0 = 0
T0 = 30
t_end = 5

# Get analytical solution
t_analytical = np.arange(t0, t_end+0.1, 0.1)
T_analytical = solve_T(t_analytical, T0)

# Get Euler numerical solution
t_euler_05, T_euler_05 = euler(dTdt, T0, t0, t_end, 0.5)
t_euler_01, T_euler_01 = euler(dTdt, T0, t0, t_end, 0.1)

# Plotting
plt.plot(t_analytical, T_analytical, label='Analytical', linestyle='--')
plt.plot(t_euler_05, T_euler_05, label='Euler method, h = 0.5')
plt.plot(t_euler_01, T_euler_01, label='Euler method, h = 0.1')

plt.title("Analytical and Euler method solutions to Newton's cooling law", fontsize=20)
plt.xlabel('Time [s]', fontsize=20)
plt.ylabel('Temperature [C]', fontsize=20)

plt.legend(fontsize=15)
plt.show()
```

{{< /collapse >}}

It is clear that the step size plays a big role in the accuracy of the solution. We could continue to reduce the step size, but that will quickly increase the time it takes to get a solution beyond a reasonable amount. Maybe we can think of a better way of doing this...

### Runge-Kutta Methods

A major problem with Euler's method is that using the derivative of the point we're at is not very good at estimating the next value of the function (for the kind of step sizes we want to use). The derivative is constantly changing and could be vastly different at the new value. We might get a more accurate step if we use a mix of the derivative at our start point and end point. 

This is the basis for how Runge-Kutta methods work. We find derivatives between the start and end points of each step and use a weighted average of those as the actual derivative for the step. The simplest version of this would be to just use the derivative at the start of the step -- which is exactly the Euler method! The Euler method is a 1st order Runge-Kutta method.

Let's define the method more concretely.

#### Derivation of Runge-Kutta Methods

We will start by examining the initial problem again, being that we want to solve the following general equation for y:

$$
    \dfrac{dy}{dt} = f(y, t)
$$

This could be our cooling equation from before,

$$
    \dfrac{dT}{dt} = -k(T - T_{surr})
$$

or something more complex like a state space defining a spring, mass, damper system:

$$
    \bold{\dot{x}} = 
    \begin{bmatrix} 0 & 1 \\\ \frac{-k}{m} & \frac{-c}{m} \end{bmatrix} \bold{x} + 
    \begin{bmatrix} 0  \\\ \frac{1}{m} \end{bmatrix} u(t)
$$

In both cases, we are given the derivative of the state we want to solve for (eg. temperature) and the derivative depends on the value of the state and time. In the case of the cooling equation, the derivative only depends on the state, **T**, and not time.

Ideally, we could just integrate the derivative to find the value at \\( t + h \\):

$$
    y(t+h) = y(t) + \int_{\tau=t}^{\tau=t+h}{\dfrac{dy(\tau)}{d\tau}} d\tau
$$

However, as we said earlier, this is often hard or impossible. We can instead approximate the integral with a weighted sum.

##### Weighted Sum Approximations

Let's build this up slowly. Take the following function, for example:

$$
    f(t) = t^3 - 2t^2 - t + 3
$$

How would we evaluate

$$
    \int_0^2{f(t)}dt
$$

We can do this analytically, which is like summing up tiny vertical slices of the function to find the total area underneath it. But we could think about this a different way. We can get that same area by finding the average value of the function over the interval, then multiplying by the length of the interval. You can see this in **Figure 4**.

{{< figure src="../img/weighted_sum_approx.png" align=center caption="**Figure 4**: Comparison of areas found by integration and weighted sum approximation." >}}

{{< collapse summary="Figure 4 code" >}}

```python {linenos=inline}
'''
Making sense of weighted sums as integral approximations.

Created by: simmeon
Last Modified: 28/04/24
License: MIT

'''

import matplotlib.pyplot as plt
import numpy as np

plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle')

def f(t):
    return t**3 -2*t**2 - t + 3

# Perform a weighted sum approximation
N = 10  # higher number of samples, N, gives a better approximation
sum = 0
t0 = 0
h = 2
nodes = np.linspace(t0, t0+h, N)

for node in nodes:
    sum += f(node)
weighted_sum = sum / N

print('Weighted sum area: ', weighted_sum * h)

# Plotting
t = np.arange(t0, t0 + h, 0.01)
y = f(t)

# Integration result
plt.subplot(1,2,1)
plt.plot(t,y)
plt.fill_between(t, y, alpha=0.3)
plt.annotate('Area = 2.67', (1,2.5), fontsize=20)
plt.title('Area from integration', fontsize=20)

# Weighted sum result
plt.subplot(1,2,2)
plt.plot(t,y)
plt.fill_between(t, weighted_sum, alpha=0.3)
plt.annotate('Area = {:.2f}'.format(weighted_sum * h), (1,2.5), fontsize=20)
plt.title('Area from weighted sum', fontsize=20)

plt.show()
```

{{< /collapse >}}

To put this idea into an equation, we can say:

$$
    \int_0^2{f(t)}dt \approx \frac{2}{\sum{w_i}} \sum_{i=1}^N{w_i  f(t_i)}
$$

Where \\( N \\) is the number of points we sample from the function to find the average, \\( w_i \\) is the weighting we give to each point, and \\( t_i \\) is some point inside the integration bounds of \\( [0, 2] \\). We can simplify this by giving every point a weighting of 1:

$$
    \int_0^2{f(t)}dt \approx \frac{2}{N} \sum_{i=1}^N{f(t_i)}
$$

The more points we sample, the closer the average will be to the actual average of the function and therefore the actual integral.

We can extend this concept to our derivative from earlier:

$$
    \int_{\tau=t}^{\tau=t+h}{\dfrac{dy(\tau)}{d\tau}} d\tau 
    \approx 
    \frac{h}{\sum{w_i}} \sum_{i=1}^N{w_i y^{\prime}(t+v_i h, y(t+v_i h))}
$$

***What a mess.***

We're still using the same concept of a weighted sum to find the average value of the derivative function, but since the derivative depends on \\( t \\) *and* \\( y \\) it's a bit more complicated. We introduced \\( v_i \\) to help define which values of the function we sample, also called *nodes*. This can range from 0 to 1 to cover the interval from \\( t \\) to \\( t+h \\). Of course, whatever time we evaluate our derivative at must be the same time we use to get the \\( y \\) values for the derivative.

For the sake of simplicity let's say that all our weights will sum up to 1. We are **not**, however, going to assume that all our weights will be the same value as we did previously.

Considering all this, the equation we are trying to solve is now:

$$
    y(t+h) = y(t) + h \sum_{i=1}^N{w_i y^{\prime}(t+v_i h, y(t+v_i h))}
$$

##### The Runge-Kutta Family

You might have noticed there is a problem with the sum we just defined. In particular, we don't know what \\( y(t+v_i h) \\) is. In fact, finding this is basically the point of the whole method. So how do we deal with this?

Simply, we are going to make worse approximations of \\( y(t+v_i h) \\) so that we can make a much better approximation of \\( y(t + h) \\). Again, these \\( y(t+v_i h) \\) values are used to find values of the derivative that we will then average. Let's define what we will do to find these approximations.

We will start by saying that \\( v_1 = 0 \\). This means the first term in the sum will be

$$
    w_1 y^{\prime}(t, y(t))
$$

If this was the only term in our sum (so \\( w_1 = 1 \\) ), then the equation we would be solving would be:

$$
    y(t+h) \approx y(t) + h y^{\prime}(t, y(t))
$$

This should look familiar, it's the Euler method! This is what we mean by the Euler method is a 1st order Runge-Kutta method -- because it uses one term in the weighted sum. Or in other words, it is a linear approximation.

For simplicity, we will write this first sum term as:

$$
    k_1 = y^{\prime}(t, y(t)) h
$$

\\( k_1 \\) is a 1st order estimate of the change in \\( y \\) between \\( y(t) \\) and \\( y(t+h) \\). 

The second term gets trickier as we have to somehow estimate \\( y(t+v_2 h) \\). Conveniently, we have just found an estimate for how \\( y \\) changes: \\( k_1 \\). So we can utilise some fraction of this change to create our estimate for the second weighted sum term where:

$$
    y^{\prime}(t + \alpha_2 h, y(t) + \beta_{2,1} k_1)
$$

We don't know yet how much of \\( k_1 \\) we should add, we will figure that out later. Same with what time we should sample at. 

We have changed notation slightly to help set up higher order Runge-Kutta methods. Instead of \\( v_i \\) we are now using \\( \alpha_i \\) to tell us about what time we are sampling at. And since our \\( y \\) value is no longer defined in simple terms of time, we are going to use \\( \beta_{i,j} \\) to describe how much of previous estimates we will add to the estimate for the new term.

We define

$$
    k_2 = y^{\prime}(t + \alpha_2 h, y(t) + \beta_{2,1} k_1) h
$$

so that the equation we are solving is now:

$$
    y(t+h) \approx y(t) + w_1 k_1 + w_2 k_2
$$

With that, we have defined the 2nd order Runge-Kutta Method! 

We can continue the weighted sum with a third and fourth term, following similar logic of using the previous estimates to inform the new value of \\( y(t + v_i h) \\). This will give us:

$$
    k_3 = y^{\prime}(t + \alpha_3 h, y(t) + \beta_{3,1} k_1 + \beta_{3,2} k_2) h
    \\\
    k_4 = y^{\prime}(t + \alpha_4 h, y(t) + \beta_{4,1} k_1 + \beta_{4,2} k_2 + \beta_{4,3} k_3) h
$$

This gives the 4th order Runge-Kutta method:

$$
    y(t+h) \approx y(t) + w_1 k_1 + w_2 k_2 + w_3 k_3 + w_4 k_4
$$

Now to actually solve these higher order methods, we need to define these coefficients...

##### Defining Coefficients

We need to define all the \\( \alpha_i \\), \\( \beta_{i,j} \\), and \\( w_i \\) coefficients to be able to use these methods. We can do this by comparing the Taylor series expansion of our approximation to the Taylor series expansion of \\( y(t+h) \\) and equating coefficients. Let's do this for the 2nd order method:

$$
    y(t+h) \approx y(t) + w_1 k_1 + w_2 k_2
$$

As this is a 2nd order method, we will need to find the 2nd order expansions of the left and right sides. 

Let's start with the left. The 2nd order Taylor series expansion of \\( y(t+h) \\) about \\( t \\) is:

$$
    y(t+h) \approx y(t) + h \dfrac{dy}{dt} \Big| _{t,y} + \frac{h^2}{2} \dfrac{d^2y}{dt^2} \Big| _{t,y} + O(h^3)
$$

We can write our derivative:

$$
    \dfrac{dy}{dt} = f(t, y)
    \\\
    {}
    \\\
    \dfrac{d^2y}{dt^2} = \dfrac{df(t,y)}{dt} = \dfrac{\partial f}{\partial t} + \dfrac{\partial f}{\partial y} \dfrac{dy}{dt} = 
    \dfrac{\partial f}{\partial t} + f \dfrac{\partial f}{\partial y}
$$

This makes our left hand side (using slightly different notation):

$$
    y_{n+1} \approx y_n + h f(t_n, y_n) + \frac{h^2}{2}  \bigg(\dfrac{\partial f}{\partial t} + f \dfrac{\partial f}{\partial y}\bigg) \bigg|_{t_n, y_n} + O(h^3)
$$

Great! Let's do the right hand side now. We can write the right hand same with the same notation as above:

$$
    y_n + w_1 k_{1,n} + w_2 k_{2,n}
$$

 \\( k_2 \\) is a bit tricky to expand. Our \\( k_2 \\) term is made up mostly of a function with the form \\( f(t + \Delta t, y + \Delta y) \\), which will need to be expanded. In general, the 2nd order expansion looks like:

$$
    f(t + \Delta t, y + \Delta y) = f(t, y) + \Delta t \dfrac{\partial f}{\partial t}\bigg| _{t, y} + 
    \Delta y \dfrac{\partial f}{\partial y} \bigg| _{t, y} + O(h^3)
$$

Applying this to \\( k_2 \\) gives:

$$
    k_{2,n} = h f(t + \alpha_2 h, y_n + \beta_{2,1} k_1) \approx h \bigg( f(t_n, y_n) + \alpha _2 h \dfrac{\partial f}{\partial t} \bigg| _{t_n, y_n} + 
    \beta _{2,1} k_1 \dfrac{\partial f}{\partial y} \bigg| _{t_n, y_n} \bigg)
$$


All together, the right hand side is then

$$
    y_n + w_1 k_{1,n} + w_2 k_{2,n} \approx y_n + w_1 h f(t_n, y_n) + 
    w_2 h \bigg( f(t_n, y_n) + \alpha _2 h \dfrac{\partial f}{\partial t} \bigg| _{t_n, y_n} + 
    \beta _{2,1} k_1 \dfrac{\partial f}{\partial y} \bigg| _{t_n, y_n} \bigg) + O(h^3)
$$

which we can rearrange to be in the same form (substituting in for \\( k_1 \\)) as the left hand side:

$$
    y_n + w_1 k_{1,n} + w_2 k_{2,n} \approx 
    y_n + (w_1 + w_2) h f(t_n, y_n) + \frac{h^2}{2} \bigg(2 w_2 \alpha_2 \dfrac{\partial f}{\partial t} + 
    2 w_2 \beta _{2,1} f \dfrac{\partial f}{\partial y}  \bigg) \bigg| _{t_n, y_n} + O(h^3)
$$

Finally, we have both expansions and can equate the coefficients:

$$
    y(t+h) \approx y(t) + w_1 k_1 + w_2 k_2
    \\\
    {}
    \\\
    \big\downarrow
    \\\
    {}
    \\\
    y_n + h f(t_n, y_n) + \frac{h^2}{2}  \bigg(\dfrac{\partial f}{\partial t} + f \dfrac{\partial f}{\partial y}\bigg) \bigg|_{t_n, y_n} + O(h^3)
    \\\ {} \\\ = \\\ {} \\\
    y_n + (w_1 + w_2) h f(t_n, y_n) + \frac{h^2}{2} \bigg(2 w_2 \alpha_2 \dfrac{\partial f}{\partial t} + 
    2 w_2 \beta _{2,1} f \dfrac{\partial f}{\partial y}  \bigg) \bigg| _{t_n, y_n} + O(h^3)
$$

From this, we can see that our coefficients must satisfy the following equations:

$$
    w_1 + w_2 = 1 \\\ {} \\\
    w_2 \alpha_2= \frac{1}{2} \\\ {} \\\
    w_2 \beta_{2,1} = \frac{1}{2}
$$

With 3 equations and 4 unknows, there are infinitely many solutions. However, the standard choices are:

$$
    \alpha_2 = \beta_{2,1} = 1
    \\\ {} \\\
    w_1 = w_2 = \frac{1}{2}
$$

This brings us *finally* to the complete **2nd order Runge-Kutta Method**:

$$
    k_1 = h y^\prime (t_n, y_n) 
    \\\ {} \\\
    k_2 = h y^\prime (t_n + h, y_n + k_1) 
    \\\ {} \\\
    y_{n+1} = y_n + \frac{1}{2} k_1 + \frac{1}{2} k_2
$$

A similar (messier) method of expansions can be used to for higher order methods. The standard terms for the **4th order Runge-Kutta method** are:

$$
    k_1 = h y^\prime (t_n, y_n) 
    \\\ {} \\\
    k_2 = h y^\prime (t_n + \frac{h}{2}, y_n + \frac{k_1}{2}) 
    \\\ {} \\\
    k_3 = h y^\prime (t_n + \frac{h}{2}, y_n + \frac{k_2}{2}) 
    \\\ {} \\\
    k_4 = h y^\prime (t_n + h, y_n + k_3) 
    \\\ {} \\\
    y_{n+1} = y_n + \frac{1}{6} (k_1 + 2 k_2 + 2 k_3 + k_4)
$$

Keep in mind that there are infinitely many choices of these coefficients and lots of research has gone into figuring out which ones work best. We will stick to these simple standard ones for now.

#### Implementing Runge-Kutta

Now that we have finally derived the Runge-Kutta methods, let's implement them in code. We will do both the 2nd order and 4th order methods and compare their accuracy.

```python {linenos=inline hl_lines=[20,21,23]}
def rk2(dydt, t0, y0, t_end, h):
    """
    Solves a first-order ODE using the 2nd order Runge-Kutta method

    Args:
        dydt: The derivative function to integrate
        t0: Initial value of time
        y0: Initial value of y
        t_end: Final time for integration
        h: Step size

    Returns:
        An array of y values for each time step
    """
    t = np.arange(t0, t_end+h, h)  # Array of time points
    y = np.zeros_like(t)
    y[0] = y0

    for i in range(1, len(t)):
        k1 = h * dydt(t[i - 1], y[i - 1])
        k2 = h * dydt(t[i - 1] + h, y[i - 1] + k1)

        y[i] = y[i - 1] + (k1 + k2) / 2

    return y
```
We can see how, at each time step, we calculate the \\( k_i \\) values and use these to estimate the next value of the function.

```python {linenos=inline hl_lines=[20,21,22,23,25]}
def rk4(dydt, t0, y0, t_end, h):
    """
    Solves a first-order ODE using the 4th order Runge-Kutta method

    Args:
        dydt: The derivative function to integrate
        t0: Initial value of time
        y0: Initial value of y
        t_end: Final time for integration
        h: Step size

    Returns:
        An array of y values for each time step
    """
    t = np.arange(t0, t_end+h, h)
    y = np.zeros_like(t)
    y[0] = y0

    for i in range(1, len(t)):
        k1 = h * dydt(t[i - 1], y[i - 1])
        k2 = h * dydt(t[i - 1] + h / 2, y[i - 1] + k1 / 2)
        k3 = h * dydt(t[i - 1] + h / 2, y[i - 1] + k2 / 2)
        k4 = h * dydt(t[i], y[i - 1] + k3) 

        y[i] = y[i - 1] + (k1 + 2 * k2 + 2 * k3 + k4) / 6
 
    return y
```

After all that derivation, the actual method is remarkably clean and simple to implement. **Figure 5** gives an idea of how these higher order functions perform compared to our original Euler method.

{{< figure src="../img/solver_comparison.png" align=center caption="**Figure 5**: Comparing the accuracy of different order ODE solvers, h = 0.1" >}}

All the solvers are using the same step size here. As we can see, the 4th order method is better than the 2nd order method. They are both much more accurate than the 1st order Euler method.

As a final comparison, let's look at Newton's law of cooling one last time. **Figure 6** shows how our 4th order solver compares to our previous test with the Euler method.

{{< figure src="../img/rk4_cooling_eqn.png" align=center caption="**Figure 6**: Comparing the accuracy of the RK4 and Euler methods on the cooling equation. T<sub>surr</sub> = 20 C, k = 1, h = 0.5" >}}


{{< collapse summary="Figure 5 and 6 code" >}}

```python {linenos=inline}
'''
Implementing and comparing the Runge-Kutta 2nd and 4th order methods.
We can also compare to the Euler method (1st order).

Created by: simmeon
Last Modified: 29/04/24
License: MIT

'''

import matplotlib.pyplot as plt
import numpy as np

plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle')

# Define some functions to test our solvers with
def dydt(t, y):
    return 3 * t**2 + 20 * np.cos(10 * t)

def analytical_solution(t):
    return t**3 + 2 * np.sin(10 * t)  # Analytical solution to the ODE 

# # Define our cooling function
# def solve_T(t, T0):
#     return T_surr + (T0 - T_surr)*np.exp(-k*t)

# # Define the derivative (only depends on the current temp, not time)
# def dTdt(T):
#     return -k * (T - T_surr)

def euler_solver(dydt, t0, y0, t_end, h):
    """
    Solves a first-order ODE using the Euler method

    Args:
        dydt: The differential equation (dy/dt) as a Python function 
        t0: Initial value of time
        y0: Initial value of y
        t_end: Final time for integration
        h: Step size

    Returns:
        An array of y values for each time step
    """
    t = np.arange(t0, t_end+h, h)  
    y = np.zeros_like(t)
    y[0] = y0

    for i in range(1, len(t)):
        y[i] = y[i - 1] + h * dydt(t[i - 1], y[i - 1])

    return y

def rk2(dydt, t0, y0, t_end, h):
    """
    Solves a first-order ODE using the 2nd order Runge-Kutta method

    Args:
        dydt: The derivative function to integrate
        t0: Initial value of time
        y0: Initial value of y
        t_end: Final time for integration
        h: Step size

    Returns:
        An array of y values for each time step
    """
    t = np.arange(t0, t_end+h, h)  # Array of time points
    y = np.zeros_like(t)
    y[0] = y0

    for i in range(1, len(t)):
        k1 = h * dydt(t[i - 1], y[i - 1])
        k2 = h * dydt(t[i - 1] + h, y[i - 1] + k1)
        y[i] = y[i - 1] + (k1 + k2) / 2

    return y

def rk4(dydt, t0, y0, t_end, h):
    """
    Solves a first-order ODE using the 4th order Runge-Kutta method

    Args:
        dydt: The derivative function to integrate
        t0: Initial value of time
        y0: Initial value of y
        t_end: Final time for integration
        h: Step size

    Returns:
        An array of y values for each time step
    """
    t = np.arange(t0, t_end+h, h)
    y = np.zeros_like(t)
    y[0] = y0

    for i in range(1, len(t)):
        k1 = dydt(t[i - 1], y[i - 1])
        k2 = dydt(t[i - 1] + h / 2, y[i - 1] + k1 / 2)
        k3 = dydt(t[i - 1] + h / 2, y[i - 1] + k2 / 2)
        k4 = dydt(t[i], y[i - 1] + k3) 

        y[i] = y[i - 1] + (k1 + 2 * k2 + 2 * k3 + k4) / 6 * h
 
    return y

# Define simulation parameters
t0 = 0
y0 = 0
t_end = 5
h = 0.1

# Solve using both methods
y_euler = euler_solver(dydt, t0, y0, t_end, h)
y_rk2 = rk2(dydt, t0, y0, t_end, h)
y_rk4 = rk4(dydt, t0, y0, t_end, h)

# Get analytical solution
t = np.arange(t0, t_end+h, h)  # Array of time points
y_analytical = analytical_solution(t)

# Calculate errors
error_euler = np.abs(y_euler - y_analytical)
error_rk2 = np.abs(y_rk2 - y_analytical)
error_rk4 = np.abs(y_rk4 - y_analytical)

# Plot the numerical solutions
plt.subplot(1,2,1)
plt.plot(t, y_euler, label='Euler')
plt.plot(t, y_rk2, label='RK2')
plt.plot(t, y_rk4, label='RK4')
plt.plot(t, y_analytical, label='Analytical', linestyle='--') 
plt.xlabel('Time (t)', fontsize=20)
plt.ylabel('y(t)', fontsize=20)
plt.title('Numerical Solutions', fontsize=20)
plt.legend(fontsize=15)

# Plot the errors
plt.subplot(1,2,2)
plt.plot(t, error_euler, label='Euler')
plt.plot(t, error_rk2, label='RK2')
plt.plot(t, error_rk4, label='RK4')
plt.xlabel('Time (t)', fontsize=20)
plt.ylabel('Abs Error', fontsize=20)
plt.title('Absolute Errors of the Numerical Solutions', fontsize=20)
plt.legend(fontsize=15)

plt.show()
```

{{< /collapse >}}

Even with a relatively large step size, the 4th order method is still much better than the Euler method. In fact, we could make the step size even bigger and still have a solution that fell within some tiny tolerance. 

But how can we choose a good value to make this step size? If we want to be as efficient as possible, each step would as large as it can be while staying within the tolerance we want...

## Step Sizing

### Constant Step Size Issues

Take the following function,

$$
    y = \sin{(t^5)} \\\ {} \\\
    \dfrac{dy}{dt} = 5t^4 \cos{(t^5)}
$$

We can use the derivative to solve for \\( y \\) using one of our numerical methods from earlier. **Figure 7** shows us doing this using the Euler method, along with the error in our solution.

{{< figure src="../img/constant_step_size.png" align=center caption="**Figure 7**: Euler method solution and error using a constant step size, h = 0.05." >}}

{{< collapse summary="Figure 7 code" >}}

```python {linenos=inline}
'''
Seeing how a constant step size numerical solver can have wildly varying 
error on diffrent steps.

Created by: simmeon
Last Modified: 18/05/24
License: MIT

'''

import matplotlib.pyplot as plt
import numpy as np

plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle')

# Define function and its derivative
def dydt(t, y):
    return 5*t**4 * np.cos(t**5)

def analytical_solution(t):
    return np.sin(t**5)

# Our Euler numerical solver
def euler_solver(dydt, t0, y0, t_end, h):
    """
    Solves a first-order ODE using the Euler method

    Args:
        dydt: The differential equation (dy/dt) as a Python function 
        t0: Initial value of time
        y0: Initial value of y
        t_end: Final time for integration
        h: Step size

    Returns:
        An array of y values for each time step
    """
    t = np.arange(t0, t_end+h, h)  
    y = np.zeros_like(t)
    y[0] = y0

    for i in range(1, len(t)):
        y[i] = y[i - 1] + h * dydt(t[i - 1], y[i - 1])

    return t, y


# Define simulation parameters
t0 = 0
y0 = 0
t_end = 2

h = 0.05 # we are using a constant step size here

# Numerically integrate
t_euler, y_euler = euler_solver(dydt, t0, y0, t_end, h)

# Get analytical solution
t_analytical = np.arange(t0, t_end+h, 0.001)
y_analytical = analytical_solution(t_analytical)

# Get error
y_analytical_sampled = y_analytical[0:-1:50]
error = np.abs(y_euler - y_analytical_sampled)


# Plotting
plt.subplot(2,1,1)
plt.plot(t_analytical, y_analytical, label='Analytical', linestyle='-')
plt.plot(t_euler, y_euler, label='Euler method')

plt.title("Error when integrating with a constant step size", fontsize=20)
plt.ylabel('y', fontsize=20)
plt.legend(fontsize=15)

plt.subplot(2,1,2)
plt.plot(t_euler, error)
plt.ylabel("absolute error", fontsize=20)
plt.xlabel('t', fontsize=20)

plt.show()
```

{{< /collapse >}}

The method works well initially when the function is changing slowly. However, when the function is changing more rapidly, our Euler method solver error gets large. You can try increasing the time to integrate over and see how the error becomes even bigger.

This is happening because our step size (\\( h = 0.05 \\)) becomes too large to properly capture how the function is changing. We could make our step size smaller, but we don't need that additional resolution for the start of the function, it's already pretty accurate. 

### Using Multiple Step Sizes

Instead of decreasing the step size for the entire integration period, it would be *much* more efficient if we could 
just decrease it where we need to.

In this example, we could try switching to a smaller step size (say, \\( h = 0.01 \\)) after \\( t = 1 \\) when our solution starts getting less accurate. Doing this gives the solution shown in **Figure 8**.


{{< figure src="../img/two_step_sizes.png" align=center caption="**Figure 8**: Euler method solution and error using two step sizes: t <= 1, h = 0.05; t > 1, h = 0.01" >}}

{{< collapse summary="Figure 8 code" >}}

```python {linenos=inline}
'''
Seeing how we can improve our solution and efficiency by using two different 
step sizes at different times.

Created by: simmeon
Last Modified: 18/05/24
License: MIT

'''

import matplotlib.pyplot as plt
import numpy as np

plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle')

# Define function and its derivative
def dydt(t, y):
    return 5*t**4 * np.cos(t**5)

def analytical_solution(t):
    return np.sin(t**5)

# Our Euler numerical solver
def euler_solver(dydt, t0, y0, t_end, h):
    """
    Solves a first-order ODE using the Euler method

    Args:
        dydt: The differential equation (dy/dt) as a Python function 
        t0: Initial value of time
        y0: Initial value of y
        t_end: Final time for integration
        h: Step size

    Returns:
        An array of y values for each time step
    """
    t = np.arange(t0, t_end+h, h)  
    y = np.zeros_like(t)
    y[0] = y0

    for i in range(1, len(t)):
        y[i] = y[i - 1] + h * dydt(t[i - 1], y[i - 1])

    return t, y


# Define simulation parameters
t0 = 0
y0 = 0
t_end1 = 1
t_end2 = 2

h1 = 0.05 # first, bigger step size
h2 = 0.01 # second. smaller step size

# Numerically integrate
t_euler1, y_euler1 = euler_solver(dydt, t0, y0, t_end1, h1)
t_euler2, y_euler2 = euler_solver(dydt, t_end1, y_euler1[-1], t_end2, h2)

y_euler = np.concatenate((y_euler1, y_euler2))
t_euler = np.concatenate((t_euler1, t_euler2))

# Get analytical solution
t_analytical = np.arange(t0, t_end2+h2, 0.001)
y_analytical = analytical_solution(t_analytical)

# Get error
y_analytical_compare = analytical_solution(t_euler)
error = np.abs(y_euler - y_analytical_compare)


# Plotting
plt.subplot(2,1,1)
plt.plot(t_analytical, y_analytical, label='Analytical', linestyle='-')
plt.plot(t_euler, y_euler, label='Euler method')
plt.vlines(t_end1, -1, 1, colors='white', alpha=0.5)

plt.title("Error when integrating with two different step sizes", fontsize=20)
plt.ylabel('y', fontsize=20)
plt.legend(fontsize=15)

plt.subplot(2,1,2)
plt.plot(t_euler, error)
plt.vlines(t_end1, 0, 0.5, colors='white', alpha=0.5)
plt.ylabel("absolute error", fontsize=20)
plt.xlabel('t', fontsize=20)

plt.show()
```

{{< /collapse >}}

This has made our solution around an order of magnitude more accurate. But how much computational complexity have we saved compared to just solving the entire time interval with the smaller step size? We can summarize this in **Table 1**.

**Table 1** 
| Step size | Number of iteration steps |
| --------- | ------------------------: |
| \\( h = 0.05 \\) | 41 |
| \\( t \leq 1, h = 0.05 \\\ t>1, h = 0.01\\) | 122 |
| \\( h = 0.01 \\) | 201 |

So we are saving a significant amount of computation compared to making the whole interval have a smaller step size.

Choosing to make the step size smaller at \\( t = 1 \\) was very arbitrary. We could also change the step size in more places -- maybe making it even bigger initially and even smaller for \\( t > 1.5 \\). We should come up with a smart way of choosing when and how to change the step size.

### Adaptive Step Sizing

Since we want our solution to be accurate, it would be smart to decrease our step size when our error is getting too big. Also, to keep things efficient, we could make our step size bigger when the error is very small.

The issue is we don't have the actual solution, so how can we know what our error is? 

We need to create some sort of "true solution" to compare our estimate against. One way we could do this is to use a higher order method, since we know that these should be more accurate and be closer to the actual solution. For example, if we want to use a 1st order Euler solver, we could also calculate a 2nd order Runge-Kutta solution and use that as the "true solution".

So for every step, we will calculate the next step with two methods: a lower and higher order one. Then, we will use difference in these as our approximation of the error. From that, we can decide whether we should change the step size for the step or if it was ok.

The last thing to decide is how much we should change the step size. For now, a simple approach could be to halve the step size if the error was too big, and double it if it was too small.

More formally, we will:

1. Take a step with the current step size using a 1st and 2nd order method.
2. Compare the solutions to approximate our error.
3. If the \\( error > tol \\), reject the step, halve the step size, and go back to **Step 1**.
4. If the \\( error < tol \\), accept the step and double the step size for the next step.

In code this would look like:

```python {linenos=true}
def euler_solver(dydt, t0, y0, t_end, tol):
    """
    Solves a first-order ODE using the Euler method with adaptive
    step sizing.

    Args:
        dydt: The differential equation (dy/dt) as a Python function 
        t0: Initial value of time
        y0: Initial value of y
        t_end: Final time for integration
        tol: Error tolerance

    Returns:
        Arrays of y and t values for each time step
    """
    h = 0.1 # set some initial step size
    t = [t0]
    y_euler = [y0]
    y_rk2 = [y0]
    
    i = 1
    while t[-1] < t_end:
        # Take a Euler step
        y_euler.append(euler_step(dydt, t[i-1], y_euler[i-1], h))
        t.append(t[i-1] + h)

        # Take a RK2 step, starting from the previous Euler step value
        y_rk2.append(rk2_step(dydt, t[i-1], y_euler[i-1], h))

        isStepGood = False

        error = np.abs(y_rk2[i] - y_euler[i])

        if error < tol:
            # accept step, make step size bigger for next step
            isStepGood = True
            h = h * 2

        while not isStepGood:
            # Take a step with both methods
            y_euler[i] = euler_step(dydt, t[i-1], y_euler[i-1], h)
            t[i] = t[i-1] + h
            y_rk2[i] = rk2_step(dydt, t[i-1], y_euler[i-1], h)

            # Calculate the error between the methods
            error = np.abs(y_rk2[i] - y_euler[i])

            # Check error to accept or reject the step
            if error > tol:
                # reject step, halve step size
                h = h / 2
            else:
                # accept step, make step size bigger for next step
                isStepGood = True
                h = h * 2

        i += 1

    return t, y_euler
```
\
If we use this to solve the same function as before, we get the solution shown in **Figure 9**.

{{< figure src="../img/naive_adaptive_step.png" align=center caption="**Figure 9**: Euler method solution and error with an adaptive step size, tol = 0.01." >}}

{{< collapse summary="Figure 9 code" >}}

```python {linenos=inline}
'''
Implementing an adaptive step size by doubling or halving the step size 
depending on the error.

Created by: simmeon
Last Modified: 18/05/24
License: MIT

'''

import matplotlib.pyplot as plt
import numpy as np

plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle')

# Define function and its derivative
def dydt(t, y):
    return 5*t**4 * np.cos(t**5)

def analytical_solution(t):
    return np.sin(t**5)


def euler_step(dydt, t0, y0, h):
    """
    Takes a single 1st order integration step and returns the y value.

    Args:
        dydt: The differential equation (dy/dt) as a Python function 
        t0: Initial value of time
        y0: Initial value of y
        h: Step size

    Returns:
        The y value after the step.

    """

    y = y0 + h * dydt(t0, y0)

    return y

def rk2_step(dydt, t0, y0, h):
    """
    Takes a single 2nd order integration step and returns the y value.

    Args:
        dydt: The differential equation (dy/dt) as a Python function 
        t0: Initial value of time
        y0: Initial value of y
        h: Step size

    Returns:
        The y value after the step.

    """

    k1 = h * dydt(t0, y0)
    k2 = h * dydt(t0 + h, y0 + k1)
    y = y0 + (k1 + k2) / 2

    return y


# Our adaptive Euler numerical solver
def euler_solver(dydt, t0, y0, t_end, tol):
    """
    Solves a first-order ODE using the Euler method with adaptive
    step sizing.

    Args:
        dydt: The differential equation (dy/dt) as a Python function 
        t0: Initial value of time
        y0: Initial value of y
        t_end: Final time for integration
        tol: Error tolerance

    Returns:
        Arrays of y, t and local error values for each time step
    """
    h = 0.1 # set some initial step size
    t = [t0]
    y_euler = [y0]
    y_rk2 = [y0]
    error = [0]
    
    i = 1
    while t[-1] < t_end:
        # Take a Euler step
        y_euler.append(euler_step(dydt, t[i-1], y_euler[i-1], h))
        t.append(t[i-1] + h)

        # Take a RK2 step, starting from the previous Euler step value
        y_rk2.append(rk2_step(dydt, t[i-1], y_euler[i-1], h))

        isStepGood = False

        error.append(np.abs(y_rk2[i] - y_euler[i]))

        if error[i] < tol:
            # accept step, make step size bigger for next step
            isStepGood = True
            h = h * 2

        while not isStepGood:
            # Take a step with both methods
            y_euler[i] = euler_step(dydt, t[i-1], y_euler[i-1], h)
            t[i] = t[i-1] + h
            y_rk2[i] = rk2_step(dydt, t[i-1], y_euler[i-1], h)

            # Calculate the error between the methods
            error[i] = np.abs(y_rk2[i] - y_euler[i])

            # Check error to accept or reject the step
            if error[i] > tol:
                # reject step, halve step size
                h = h / 2
            else:
                # accept step, make step size bigger for next step
                isStepGood = True
                h = h * 2

        i += 1

    return t, y_euler, error


# Define simulation parameters
t0 = 0
y0 = 0
t_end = 2
tol = 1e-2

# Numerically integrate
t_euler, y_euler, local_error = euler_solver(dydt, t0, y0, t_end, tol)

t_euler = np.array(t_euler)
y_euler = np.array(y_euler)

# Get analytical solution
t_analytical = np.arange(t0, t_end, 0.001)
y_analytical = analytical_solution(t_analytical)

# Get error
y_analytical_compare = analytical_solution(t_euler)
error = np.abs(y_euler - y_analytical_compare)

# Plotting
plt.subplot(3,1,1)
plt.plot(t_analytical, y_analytical, label='Analytical', linestyle='-')
plt.plot(t_euler, y_euler, label='Euler method')
plt.scatter(t_euler, y_euler, s=30, marker='o', facecolors='none', color='C1')

plt.title("Integrating with a adaptive step size, tol = 0.01", fontsize=20)
plt.ylabel('y', fontsize=20)
plt.legend(fontsize=15)
plt.text(0, -0.75, f"Number of steps: {len(t_euler)}", fontsize=15)

plt.subplot(3,1,2)
plt.plot(t_euler, error)
plt.ylabel("absolute error", fontsize=20)
plt.xlabel('t', fontsize=20)

plt.subplot(3,1,3)
plt.plot(t_euler, local_error)
plt.ylabel("local step error", fontsize=20)
plt.xlabel('t', fontsize=20)

plt.show()
```

{{< /collapse >}}

Each dot represents where a step was taken. We can see how the step size is bigger when the function is changing slowly and smaller when the function changes quickly.

The error is interesting. We set a tolerance of \\( 0.01 \\) but the absolute error gets up to nearly \\( 0.1 \\). This is because the tolerance is used to set the allowable error **for each step**, not the overall (or global) error. Another reason is that our error is an estimate but here we are plotting against the actual, analytical error. Howveer, looking at the local error for each step we can see that it never gets above our tolerance.

### Choosing Better Step Size Changes

We used a very simple method of halving the step size if the error was too big or doubling the step size if the error was too small. For the 255 steps we ended up with, the method calculated the next step 746 times. This means that, on average for each step, we changed the step size 2.9 times before finding a value that worked. There is a lot of computation that is wasted there.

Let's try and come up with a better way of choosing the step size to hopefully make our method more efficient. 

***This will depend on what order methods we are using.***

We will start by doing this for the 1st order Euler method. We write this as:

$$
    y_{n+1} = y_{n} + h \dfrac{dy}{dt} \bigg| _{t_n, y_n} + O(h^2)
$$

The error in our estimate is proportional to \\( h^2 \\). If we assume that this error is constant over time, we can say

$$
    error = \Delta = c h^2
$$

where \\( c \\) is some constant value. Then, for some step size, \\( h_1 \\), we would get

$$
    \Delta _1 = c h_1^2 
$$

We could also write an equation for what the step size would have to be to get an error equal to our tolerance. Let's call this error \\( \Delta _0 \\).

$$
    \Delta _0 = c h_0^2
$$

This \\( h_0 \\) is the value that we would want to use for the current step size to. In other words, it will give the largest step that stays within our tolerance.

We can use these two equations together through the constant \\( c \\) to get the relation:

$$
    \frac{\Delta_1}{h_1^2} = \frac{\Delta_0}{h_0^2}
$$

This can then be arranged to solve for what our step size \\( h_0 \\) should be

$$
    h_0 = h_1 \bigg(\frac{\Delta _0}{\Delta _1} \bigg) ^{0.5}
$$

or in more familiar notation

$$
    h_{new} = h_{current} \bigg(\frac{tol}{error} \bigg) ^{0.5}
$$

In theory, using this formula should mean that we get the correct step size in one iteration instead of two or three. However, in practice, our error is only an approximation and so we should put in a safety factor to make the new step size slightly smaller that the theoretical value:

$$
    h_{new} = 0.9 h_{current} \bigg(\frac{tol}{error} \bigg) ^{0.5}
$$

Let's use this instead of our doubling and halving approach and see how many calculations we use. Note that formula has the correct behaviour for errors that are both larger and smaller than the tolerance. We can see the result in **Figure 10**.

{{< figure src="../img/better_adaptive_step.png" align=center caption="**Figure 10**: Euler method solution and error with a better adaptive step size, tol = 0.01." >}}

{{< collapse summary="Figure 10 code" >}}

```python {linenos=inline}
'''
Implementing an adaptive step size with better step sizing.

Created by: simmeon
Last Modified: 19/05/24
License: MIT

'''

import matplotlib.pyplot as plt
import numpy as np

plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle')

# Define function and its derivative
def dydt(t, y):
    return 5*t**4 * np.cos(t**5)

def analytical_solution(t):
    return np.sin(t**5)


def euler_step(dydt, t0, y0, h):
    """
    Takes a single 1st order integration step and returns the y value.

    Args:
        dydt: The differential equation (dy/dt) as a Python function 
        t0: Initial value of time
        y0: Initial value of y
        h: Step size

    Returns:
        The y value after the step.

    """

    y = y0 + h * dydt(t0, y0)

    return y

def rk2_step(dydt, t0, y0, h):
    """
    Takes a single 2nd order integration step and returns the y value.

    Args:
        dydt: The differential equation (dy/dt) as a Python function 
        t0: Initial value of time
        y0: Initial value of y
        h: Step size

    Returns:
        The y value after the step.

    """

    k1 = h * dydt(t0, y0)
    k2 = h * dydt(t0 + h, y0 + k1)
    y = y0 + (k1 + k2) / 2

    return y


# Our adaptive Euler numerical solver
def euler_solver(dydt, t0, y0, t_end, tol):
    """
    Solves a first-order ODE using the Euler method with adaptive
    step sizing.

    Args:
        dydt: The differential equation (dy/dt) as a Python function 
        t0: Initial value of time
        y0: Initial value of y
        t_end: Final time for integration
        tol: Error tolerance

    Returns:
        Arrays of y, t and local error values for each time step
    """
    h = 0.1 # set some initial step size
    t = [t0]
    y_euler = [y0]
    y_rk2 = [y0]
    error = [0]
    
    i = 1
    count = 0

    while t[-1] < t_end:
        count += 1
        # Take a Euler step
        y_euler.append(euler_step(dydt, t[i-1], y_euler[i-1], h))
        t.append(t[i-1] + h)

        # Take a RK2 step, starting from the previous Euler step value
        y_rk2.append(rk2_step(dydt, t[i-1], y_euler[i-1], h))

        isStepGood = False

        error.append(np.abs(y_rk2[i] - y_euler[i]))

        if error[i] < tol:
            # accept step, make step size bigger for next step
            isStepGood = True
            h = 0.9 * h * (tol / error[i]) ** 0.5

        while not isStepGood:
            count += 1
            # Take a step with both methods
            y_euler[i] = euler_step(dydt, t[i-1], y_euler[i-1], h)
            t[i] = t[i-1] + h
            y_rk2[i] = rk2_step(dydt, t[i-1], y_euler[i-1], h)

            # Calculate the error between the methods
            error[i] = np.abs(y_rk2[i] - y_euler[i])

            # Check error to accept or reject the step
            if error[i] > tol:
                # reject step, halve step size
                h = 0.9 * h * (tol / error[i]) ** 0.5
            else:
                # accept step, make step size bigger for next step
                isStepGood = True
                h = 0.9 * h * (tol / error[i]) ** 0.5

        i += 1
    print("Count: ", count)

    return t, y_euler, error


# Define simulation parameters
t0 = 0
y0 = 0
t_end = 2
tol = 1e-2

# Numerically integrate
t_euler, y_euler, local_error = euler_solver(dydt, t0, y0, t_end, tol)

t_euler = np.array(t_euler)
y_euler = np.array(y_euler)

# Get analytical solution
t_analytical = np.arange(t0, t_end, 0.001)
y_analytical = analytical_solution(t_analytical)

# Get error
y_analytical_compare = analytical_solution(t_euler)
error = np.abs(y_euler - y_analytical_compare)

# Plotting
plt.subplot(3,1,1)
plt.plot(t_analytical, y_analytical, label='Analytical', linestyle='-')
plt.plot(t_euler, y_euler, label='Euler method')
plt.scatter(t_euler, y_euler, s=30, marker='o', facecolors='none', color='C1')

plt.title("Integrating with a adaptive step size, tol = 0.01", fontsize=20)
plt.ylabel('y', fontsize=20)
plt.legend(fontsize=15)
plt.text(0, -0.75, f"Number of steps: {len(t_euler)}", fontsize=15)

plt.subplot(3,1,2)
plt.plot(t_euler, error)
plt.ylabel("absolute error", fontsize=20)

plt.subplot(3,1,3)
plt.plot(t_euler, local_error)
plt.ylabel("local step error", fontsize=20)
plt.xlabel('t', fontsize=20)

plt.show()
```

{{< /collapse >}}

This change used 216 steps with a total of 323 step calulations, meaning we changed the step size 1.5 times per step. This is much closer to the minimum of 1 change per step. It also ended up requiring less steps than previously, being even more efficient.

This step change formula we created can be applied to higher order methods, for example if we were using a 4th order method with 5th order errors we would use:

$$
    h_{new} = 0.9 h_{current} \bigg(\frac{tol}{error} \bigg) ^{\frac{1}{5}}
$$


## A Full RK45 Method

Finally, we have all the parts to create a working 4th order Runge-Kutta numerical solver with adaptive step sizing based on 5th order errors.

We will start by writing a function that takes a single 5th order step. We are going to use the 5th order step to approximate our "true solution" and compare it to the 4th order step to find our error. The function will then return the 4th order step and the error. One reason this method is so good is because calculating the 5th step reuses a lot of the RK4 step calculations, making it very efficient.

To do this we will be using the Dormand-Prince coefficients. These are much messier than the standard coefficients we used before. However, they are very good at minimising the error in the 5th order approximation, which is exactly what we want -- we want that 5th order approximation to be as close to the real solution as possible.

The intermediate approximations are calculated as follows:

$$
    \begin{aligned}
        &k_1 = hf(t_n, y_n)
        \\\ {} \\\
        &k_2 = hf \Big(t_n + \frac{1}{5}h, y_n + \frac{1}{5} k_1 \Big)
        \\\ {} \\\
        &k_3 = hf \Big(t_n + \frac{3}{10}h, y_n + \frac{3}{40} k_1 + \frac{9}{40} k_2 \Big)
        \\\ {} \\\
        &k_4 = hf \Big(t_n + \frac{4}{5}h, y_n + \frac{44}{45} k_1 - \frac{56}{15} k_2 + \frac{32}{9} k_3 \Big)
        \\\ {} \\\
        &k_5 = hf \Big(t_n + \frac{8}{9}h, y_n + \frac{19372}{6561} k_1 - \frac{25360}{2187} k_2 + \frac{64448}{6561} k_3 - \frac{212}{729} k_4 \Big)
        \\\ {} \\\
        &k_6 = hf \Big(t_n + h, y_n + \frac{9017}{3168} k_1 - \frac{355}{33} k_2 - \frac{46732}{5247} k_3 + \frac{49}{176} k_4 - \frac{5103}{18656} k_5 \Big)
        \\\ {} \\\
        &k_7 = hf \Big(t_n + h, y_n + \frac{35}{848} k_1 + \frac{500}{1113} k_3 + \frac{125}{192} k_4 - \frac{2187}{6784} k_5 + \frac{11}{84} k_6 \Big)
    \end{aligned}
$$

Then, the next 4th order step is calculated as

$$
    y_{n+1} = y_n + \frac{35}{384} k_1 + \frac{500}{1113} k_3 + \frac{125}{192} k_4 - \frac{2187}{6784} k_5 + \frac{11}{84} k_6
$$

Note that these are the same coefficients that we use for \\( k_7 \\), which is useful. We can rewrite \\( k_7 \\) as

$$
    k_7 = hf \Big( t_n + h, y_{n+1} \Big)
$$

This value is then the same as \\( k_1 \\) will be in the next step (except for \\( h \\) changing). This means that, except for the first step, we only have to calculate 6 derivatives per step.

Then, the 5th order step, \\( z_{n+1} \\), is calculated as

$$
    z_{n+1} = y_n + \frac{5179}{57600} k_1 + \frac{7571}{16695} k_3 + \frac{393}{640} k_4 - \frac{92097}{339200} k_5 + \frac{187}{2100} k_6 + \frac{1}{40} k_7
$$

This gives the error

$$
    error = \Big| z_{n+1} - y_{n+1} \Big| = \Big| -\frac{71}{57600} k_1 + \frac{71}{16695} k_3 - \frac{71}{1920} k_4 + \frac{17253}{339200} k_5 - \frac{22}{525} k_6 + \frac{1}{40} k_7 \Big|
$$

and from earlier we know that our step size change will be

$$
    h_{new} = 0.9 h_{current} \bigg(\frac{tol}{error} \bigg) ^{0.2}
$$

Let's write this step function now, along with the function that adaptively steps to return the solution.

```python {linenos=true}
# ----- Dormand-Prince coefficients ----- #

# Alpha in notation, coefficients describe what percentage of the full
# step we evaluate each derivative at
A = np.array([0, 1/5, 3/10, 4/5, 8/9, 1, 1])

# Beta in notation, coefficients describe how much of each previous change in y
# we add to the current estimate
B = np.array([
        [0, 0, 0, 0, 0, 0],
        [1/5, 0, 0, 0, 0, 0],
        [3/40, 9/40, 0, 0, 0, 0],
        [44/45, -56/15, 32/9, 0, 0, 0],
        [19372/6561, -25360/2187, 64448/6561, -212/729, 0, 0],
        [9017/3168, -355/33, 46732/5247, 49/176, -5103/18656, 0],
        [35/384, 0, 500/1113, 125/192, -2187/6784, 11/84]
    ])

# Weighting for each derivative in the approximation, w in notation
W = np.array([35/384, 0, 500/1113, 125/192, -2187/6784, 11/84, 0])
#W = np.array([5179/57600, 0, 7571/16695, 393/640, -92097/339200, 187/2100, 1/40])

# Coefficients for error calculation
E = np.array([-71/57600, 0, 71/16695, -71/1920, 17253/339200, -22/525, 1/40])

# ------------------------------------------------------------------ #

# Define a function to take a single RK45 step
# To improve efficiency, k7 can be reused as k1 in the following step
def rk45_step(dydt, t0, y0, h):
    """
    Takes a single 5th order integration step and returns the 
    4th order step along with the 5th order error.

    Args:
        dydt: The differential equation (dy/dt) as a Python function 
        t0: Initial value of time
        y0: Initial value of y
        h: Step size

    Returns:
        The the 4th order y value along with the 5th order error.

    """

    k1 = h * dydt(t0, y0)
    k2 = h * dydt(t0 + A[1] * h, y0 + B[1][0]*k1)
    k3 = h * dydt(t0 + A[2] * h, y0 + B[2][0]*k1 + B[2][1]*k2)
    k4 = h * dydt(t0 + A[3] * h, y0 + B[3][0]*k1 + B[3][1]*k2 + B[3][2]*k3)
    k5 = h * dydt(t0 + A[4] * h, y0 + B[4][0]*k1 + B[4][1]*k2 + B[4][2]*k3 + B[4][3]*k4)
    k6 = h * dydt(t0 + A[5] * h, y0 + B[5][0]*k1 + B[5][1]*k2 + B[5][2]*k3 + B[5][3]*k4 + B[5][4]*k5)

    y = y0 + W[0]*k1 + W[1]*k2 + W[2]*k3 + W[3]*k4 + W[4]*k5 + W[5]*k6

    k7 = h * dydt(t0 + A[6] * h, y)

    error = np.abs( E[0]*k1 + E[1]*k2 + E[2]*k3 + E[3]*k4 + E[4]*k5 + E[5]*k6 + E[6]*k7 )

    return y, error

# Define the full RK45 solver function
def rk45_solver(dydt, t0, y0, t_end, tol):
    """
    Solves a first-order ODE using the RK45 method.

    Args:
        dydt: The differential equation (dy/dt) as a Python function 
        t0: Initial value of time
        y0: Initial value of y
        t_end: Final time for integration
        tol: Error tolerance

    Returns:
        Arrays of y, t and local error values for each time step
    """
    h = 1e-3 # set some initial step size
    t = [t0]
    y = [y0]
    error = [0]
    
    i = 1
    while t[-1] < t_end:
        # Take a step
        t.append(t[i-1] + h)
        y_step, error_step = rk45_step(dydt, t[i-1], y[i-1], h)
        y.append(y_step)
        error.append(error_step)

        # Update step size after step
        h = 0.9 * h * (tol / error[i]) ** 0.2

        isStepGood = False

        if error[i] < tol:
            # accept step
            isStepGood = True

        while not isStepGood:
            # If there was too much error...

            # Take a step
            t[i] = t[i-1] + h # update our time value with the new step size
            y_step, error_step = rk45_step(dydt, t[i-1], y[i-1], h)
            y[i] = y_step
            error[i] = error_step
            
            # Update step size after step
            h = 0.9 * h * (tol / error[i]) ** 0.2

            # Check error to accept or reject the step
            if error[i] < tol:
                isStepGood = True

        i += 1

    return t, y, error
```
\
Using this step function along with updating the step size gives the result in **Figure 11**.

{{< figure src="../img/rk45.png" align=center caption="**Figure 11**: RK45 Solver with error." >}}

{{< collapse summary="Figure 11 code (***full solver code + graphing***)" >}}

```python {linenos=inline}
'''
Implementing a complete RK45 algorithm.

Created by: simmeon
Last Modified: 19/05/24
License: MIT

'''

import matplotlib.pyplot as plt
import numpy as np

plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle')

# Define function (for analytical solution) and its derivative
def dydt(t, y):
    return 5*t**4 * np.cos(t**5)

def analytical_solution(t):
    return np.sin(t**5)


# ----- Dormand-Prince coefficients ----- #

# Alpha in notation, coefficients describe what percentage of the full
# step we evaluate each derivative at
A = np.array([0, 1/5, 3/10, 4/5, 8/9, 1, 1])

# Beta in notation, coefficients describe how much of each previous change in y
# we add to the current estimate
B = np.array([
        [0, 0, 0, 0, 0, 0],
        [1/5, 0, 0, 0, 0, 0],
        [3/40, 9/40, 0, 0, 0, 0],
        [44/45, -56/15, 32/9, 0, 0, 0],
        [19372/6561, -25360/2187, 64448/6561, -212/729, 0, 0],
        [9017/3168, -355/33, 46732/5247, 49/176, -5103/18656, 0],
        [35/384, 0, 500/1113, 125/192, -2187/6784, 11/84]
    ])

# Weighting for each derivative in the approximation, w in notation
W = np.array([35/384, 0, 500/1113, 125/192, -2187/6784, 11/84, 0])
#W = np.array([5179/57600, 0, 7571/16695, 393/640, -92097/339200, 187/2100, 1/40])

# Coefficients for error calculation
E = np.array([-71/57600, 0, 71/16695, -71/1920, 17253/339200, -22/525, 1/40])

# ------------------------------------------------------------------ #

# Define a function to take a single RK45 step
# To improve efficiency, k7 can be reused as k1 in the following step
def rk45_step(dydt, t0, y0, h):
    """
    Takes a single 5th order integration step and returns the 
    4th order step along with the 5th order error.

    Args:
        dydt: The differential equation (dy/dt) as a Python function 
        t0: Initial value of time
        y0: Initial value of y
        h: Step size

    Returns:
        The the 4th order y value along with the 5th order error.

    """

    k1 = h * dydt(t0, y0)
    k2 = h * dydt(t0 + A[1] * h, y0 + B[1][0]*k1)
    k3 = h * dydt(t0 + A[2] * h, y0 + B[2][0]*k1 + B[2][1]*k2)
    k4 = h * dydt(t0 + A[3] * h, y0 + B[3][0]*k1 + B[3][1]*k2 + B[3][2]*k3)
    k5 = h * dydt(t0 + A[4] * h, y0 + B[4][0]*k1 + B[4][1]*k2 + B[4][2]*k3 + B[4][3]*k4)
    k6 = h * dydt(t0 + A[5] * h, y0 + B[5][0]*k1 + B[5][1]*k2 + B[5][2]*k3 + B[5][3]*k4 + B[5][4]*k5)

    y = y0 + W[0]*k1 + W[1]*k2 + W[2]*k3 + W[3]*k4 + W[4]*k5 + W[5]*k6

    k7 = h * dydt(t0 + A[6] * h, y)

    error = np.abs( E[0]*k1 + E[1]*k2 + E[2]*k3 + E[3]*k4 + E[4]*k5 + E[5]*k6 + E[6]*k7 )

    return y, error

# Define the full RK45 solver function
def rk45_solver(dydt, t0, y0, t_end, tol):
    """
    Solves a first-order ODE using the RK45 method.

    Args:
        dydt: The differential equation (dy/dt) as a Python function 
        t0: Initial value of time
        y0: Initial value of y
        t_end: Final time for integration
        tol: Error tolerance

    Returns:
        Arrays of y, t and local error values for each time step
    """
    h = 1e-3 # set some initial step size
    t = [t0]
    y = [y0]
    error = [0]
    
    i = 1
    while t[-1] < t_end:
        # Take a step
        t.append(t[i-1] + h)
        y_step, error_step = rk45_step(dydt, t[i-1], y[i-1], h)
        y.append(y_step)
        error.append(error_step)

        # Update step size after step
        h = 0.9 * h * (tol / error[i]) ** 0.2

        isStepGood = False

        if error[i] < tol:
            # accept step
            isStepGood = True

        while not isStepGood:
            # If there was too much error...

            # Take a step
            t[i] = t[i-1] + h # update our time value with the new step size
            y_step, error_step = rk45_step(dydt, t[i-1], y[i-1], h)
            y[i] = y_step
            error[i] = error_step
            
            # Update step size after step
            h = 0.9 * h * (tol / error[i]) ** 0.2

            # Check error to accept or reject the step
            if error[i] < tol:
                isStepGood = True

        i += 1

    return t, y, error


# Define simulation parameters
t0 = 0
y0 = 0
t_end = 2
tol = 1e-6

# Numerically integrate
t_rk45, y_rk45, local_error = rk45_solver(dydt, t0, y0, t_end, tol)

t_rk45 = np.array(t_rk45)
y_rk45 = np.array(y_rk45)

# Get analytical solution
t_analytical = np.arange(t0, t_end, 0.001)
y_analytical = analytical_solution(t_analytical)

# Get error
y_analytical_compare = analytical_solution(t_rk45)
error = np.abs(y_rk45 - y_analytical_compare)

# Plotting
plt.subplot(3,1,1)
plt.plot(t_analytical, y_analytical, label='Analytical', linestyle='-')
plt.plot(t_rk45, y_rk45, label='RK45 method')
#plt.scatter(t_rk45, y_rk45, s=30, marker='o', facecolors='none', color='C1')

plt.title(f"RK45 Solver, tol = {tol}", fontsize=20)
plt.ylabel('y', fontsize=20)
plt.legend(fontsize=15)
plt.text(0, -0.75, f"Number of steps: {len(t_rk45)}", fontsize=15)

plt.subplot(3,1,2)
plt.plot(t_rk45, error)
plt.ylabel("absolute error", fontsize=20)

plt.subplot(3,1,3)
plt.plot(t_rk45, local_error)
plt.ylabel("local step error", fontsize=20)
plt.xlabel('t', fontsize=20)

plt.show()
```

{{< /collapse >}}

As we can see, the method is incredibly efficient and accurate. In comparison, our 1st order adaptive method used over 200 steps for a tolerance of only 1e-03. This combination of efficiency and accuracy makes the RK45 method an incredibly good general solver.

## A Final Application

So far we have used mostly trivial or contrived examples to demonstrate why the RK45 solver is good. To finish off, we will use a real-world example where the adaptability, efficiency, and accuracy of the RK45 solver is very useful.

We will be using the solver to propagate a satellite orbit around the earth. We will skip some details of deriving the relevant physics, but the important equation is:

$$
    F = \frac{G m_{earth} m_{sat}}{r^2}
$$

Let's define the gravitational parameter, \\( \mu \\)

$$
    \mu = G(m_{earth} + m_{sat})
$$

By assuming an inertial reference frame centred on the Earth, we get

$$
    \ddot{\vec{r}} = - \frac{G(m_{earth} + m_{sat})}{r^3}\vec{r}
    \\\ {} \\\
    \ddot{\vec{r}} = - \frac{\mu}{r^3}\vec{r}
$$

where \\( \vec{r} \\) is the position vector of the satellite we want to solve for.

This is currently a second order ODE. We can use a state space to transform it into being first order:

$$
    \bold{x} = \begin{bmatrix}
                        r_x \\\
                        r_y \\\
                        \dot{r}_x \\\
                        \dot{r}_y
    \end{bmatrix}
    \\\ {} \\\ {} \\\
    \bold{\dot{x}} = \begin{bmatrix}
                            \dot{r}_x \\\ {} \\\
                            \dot{r}_y \\\ {} \\\
                            \ddot{r}_x \\\ {} \\\
                            \ddot{r}_y \\\
    \end{bmatrix} = \begin{bmatrix}
                            \dot{r}_x \\\ {} \\\
                            \dot{r}_y \\\ {} \\\
                            -\Large \frac{\mu r_x}{|r|^3} \\\ {} \\\
                            -\Large \frac{\mu r_y}{|r|^3} \\\
                    \end{bmatrix}
    \\\ {} \\\ {} \\\
    \bold{\dot{x}} = \begin{bmatrix}
                            0 & 0 & 1 & 0 \\\
                            0 & 0 & 0 & 1 \\\
                            -\Large \frac{\mu}{|r|^3} & 0 & 0 & 0 \\\
                            0 & -\Large \frac{\mu}{|r|^3} & 0 & 0
                        \end{bmatrix} \bold{x}
$$

This is now a system of first order ODEs where we are solving for \\( \bold{x} \\). With some small code changes to allow for vector inputs, **Figure 12** shows the resulting satellite position and velocity for some initial state. To deal with multiple ODE systems with arrays of solutions instead of just a single value, we use the maximum error as our test for whether the step was good or not.

{{< figure src="../img/orbit.png" align=center caption="**Figure 12**: Propagating an orbit with RK45." >}}

{{< collapse summary="Figure 12 code" >}}

```python {linenos=inline}
'''
An application of the RK45 ODE solver. We will solve for the position
of a satellite around Earth given an initial position and velocity.

Created by: simmeon
Last Modified: 19/05/24
License: MIT

'''

import matplotlib.pyplot as plt
import numpy as np

plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle')

# Define derivative
def dxdt(t, x):
    r_mag = np.sqrt(x[0]**2 + x[1]**2)
    mu = 398600.4415;   # [km^3 / (kg*s^2)]

    return np.array([x[2], x[3], -mu * x[0] / (r_mag**3), -mu * x[1] / (r_mag**3)])



# ----- Dormand-Prince coefficients ----- #

# Alpha in notation, coefficients describe what percentage of the full
# step we evaluate each derivative at
A = np.array([0, 1/5, 3/10, 4/5, 8/9, 1, 1])

# Beta in notation, coefficients describe how much of each previous change in y
# we add to the current estimate
B = np.array([
        [0, 0, 0, 0, 0, 0],
        [1/5, 0, 0, 0, 0, 0],
        [3/40, 9/40, 0, 0, 0, 0],
        [44/45, -56/15, 32/9, 0, 0, 0],
        [19372/6561, -25360/2187, 64448/6561, -212/729, 0, 0],
        [9017/3168, -355/33, 46732/5247, 49/176, -5103/18656, 0],
        [35/384, 0, 500/1113, 125/192, -2187/6784, 11/84]
    ])

# Weighting for each derivative in the approximation, w in notation
W = np.array([35/384, 0, 500/1113, 125/192, -2187/6784, 11/84, 0])
#W = np.array([5179/57600, 0, 7571/16695, 393/640, -92097/339200, 187/2100, 1/40])

# Coefficients for error calculation
E = np.array([-71/57600, 0, 71/16695, -71/1920, 17253/339200, -22/525, 1/40])

# ------------------------------------------------------------------ #

# Define a function to take a single RK45 step
# To improve efficiency, k7 can be reused as k1 in the following step
def rk45_step(dydt, t0, y0, h):
    """
    Takes a single 5th order integration step and returns the 
    4th order step along with the 5th order error.

    Args:
        dydt: The differential equation (dy/dt) as a Python function 
        t0: Initial value of time
        y0: Initial value of y
        h: Step size

    Returns:
        The the 4th order y value along with the 5th order error.

    """

    k1 = h * dydt(t0, y0)
    k2 = h * dydt(t0 + A[1] * h, y0 + B[1][0]*k1)
    k3 = h * dydt(t0 + A[2] * h, y0 + B[2][0]*k1 + B[2][1]*k2)
    k4 = h * dydt(t0 + A[3] * h, y0 + B[3][0]*k1 + B[3][1]*k2 + B[3][2]*k3)
    k5 = h * dydt(t0 + A[4] * h, y0 + B[4][0]*k1 + B[4][1]*k2 + B[4][2]*k3 + B[4][3]*k4)
    k6 = h * dydt(t0 + A[5] * h, y0 + B[5][0]*k1 + B[5][1]*k2 + B[5][2]*k3 + B[5][3]*k4 + B[5][4]*k5)

    y = y0 + W[0]*k1 + W[1]*k2 + W[2]*k3 + W[3]*k4 + W[4]*k5 + W[5]*k6

    k7 = h * dydt(t0 + A[6] * h, y)

    error = np.abs( E[0]*k1 + E[1]*k2 + E[2]*k3 + E[3]*k4 + E[4]*k5 + E[5]*k6 + E[6]*k7 )

    return y, error

# Define the full RK45 solver function
def rk45_solver(dydt, t0, y0, t_end, tol):
    """
    Solves a first-order ODE using the RK45 method.

    Args:
        dydt: The differential equation (dy/dt) as a Python function 
        t0: Initial value of time
        y0: Initial value of y
        t_end: Final time for integration
        tol: Error tolerance

    Returns:
        Arrays of y, t and local error values for each time step
    """
    h = 1e-3 # set some initial step size
    t = [t0]
    y = np.array([y0])
    error = [0]
    
    i = 1
    while t[-1] < t_end:
        # Take a step
        t.append(t[i-1] + h)
        y_step, error_step = rk45_step(dydt, t[i-1], y[i-1], h)
        y = np.append(y, np.array([y_step]), axis=0)
        error.append(error_step)

        # Update step size after step
        h = 0.9 * h * (tol / max(error[i])) ** 0.2

        isStepGood = False

        if max(error[i]) < tol:
            # accept step
            isStepGood = True

        while not isStepGood:
            # If there was too much error...

            # Take a step
            t[i] = t[i-1] + h # update our time value with the new step size
            y_step, error_step = rk45_step(dydt, t[i-1], y[i-1], h)
            y[i] = y_step
            error[i] = error_step
            
            # Update step size after step
            h = 0.9 * h * (tol / max(error[i])) ** 0.2

            # Check error to accept or reject the step
            if max(error[i]) < tol:
                isStepGood = True

        i += 1

    return t, y, error


# Define orbit parameters
mu = 398600.4415
rp = 6678
e = 0.9
a = rp/(1-e)
T = 2*np.pi*np.sqrt(a**3/mu)

# Define simulation parameters
t0 = 0
x0 = np.array([rp, 0, 0, np.sqrt(2*mu/rp - mu/a)])
t_end = T
tol = 1e-12

# Numerically integrate
t_rk45, y_rk45, local_error = rk45_solver(dxdt, t0, x0, t_end, tol)

y_rk45 = np.array(y_rk45)

# Plotting
plt.subplot(2,1,1)
plt.plot(y_rk45[:, 0], y_rk45[:, 1])
plt.axis('equal')
plt.scatter(0, 0, s=50) # dot for Earth
plt.title("Satellite Orbit Around Earth", fontsize=20)
plt.ylabel('y [km]', fontsize=20)
plt.xlabel('x [km]', fontsize=20)

plt.subplot(2,1,2)
plt.plot(t_rk45, np.sqrt(y_rk45[:, 2]**2 + y_rk45[:, 3]**2), color='C1')
plt.title("Satellite Velocity", fontsize=20)
plt.ylabel('velocity [km/s]', fontsize=20)
plt.xlabel('t [s]', fontsize=20)

plt.show()
```

{{< /collapse >}}


The adaptive step size is able to deal with velocity changing by orders of magnitude. With a constant step size, we would be wasting a huge amount of computation on the far side of the orbit. The accuracy of the RK45 method also allows us to use very small tolerances (1e-12) and still have the computation time be relatively quick.

## Conclusion

This guide should serve as a single comprehensive reference for what the RK45 method is, how it is derived, and how and why we use it to solve engineering problems. There is still room to improve on the ideas and code that have been shown here (the code lacks a lot of polish and dealing with edge cases for example). 

However, for those like me who wanted to understand where this method comes from and why it is so good, I hope this has been a useful reference. The goal was to make these quite dense mathematical concepts as clear as possible for other engineers like me.

## References

[1] [Roson J. S., "The Runge-Kutta Equations by Quadrature Methods", 1967, *NASA*.](https://ntrs.nasa.gov/api/citations/19680000653/downloads/19680000653.pdf)

[2] ["Runge-Kutta Methods", *10.001: Numerical Solution of Ordinary Differential Equations*](https://web.mit.edu/10.001/Web/Course_Notes/Differential_Equations_Notes/node5.html)

[3] [Explanation and proof of the 4th order Runge-Kutta method](https://math.stackexchange.com/questions/528856/explanation-and-proof-of-the-4th-order-runge-kutta-method)

[4] [Numerical Recipes in C: The Art of Scientific Computing](https://people.computing.clemson.edu/~dhouse/courses/817/papers/adaptive-h-c16-2.pdf)

[5] [S. Brorson, "Numerically Solving Ordinary Differential Equations"](https://math.libretexts.org/Bookshelves/Differential_Equations/Numerically_Solving_Ordinary_Differential_Equations_(Brorson)/01%3A_Chapters/1.05%3A_Adaptive_stepping)

[6] [Adaptive Runge-Kutta Methods | Lecture 54 | Numerical Methods for Engineers](https://www.youtube.com/watch?v=6bCBXvsD7gw)

[7] [Dormand, J. R. and P. J. Prince, A family of embedded Runge-Kutta formulae, J. Comp. Appl. Math., Vol. 6, 1980, pp. 1926.](https://core.ac.uk/download/pdf/81989096.pdf)

[8] [Dormand-Prince Method](https://numerary.readthedocs.io/en/latest/dormand-prince-method.html)