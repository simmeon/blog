---
title: "An Engineer's Guide to the Runge-Kutta (RK45) Method"
date: '2024-04-28T07:08:00+12:00'
# weight: 1
# aliases: ["/first"]
tags: ["runge kutta", "ode", "numerical integration", "rk45", "python"]
author: "simmeon"
showToc: true
TocOpen: false
draft: false
hidemeta: false
comments: false
description: "Understanding how to apply the RK45 method to engineering problems."
canonicalURL: "https://simmeon.github.io/blog/post/rk45.md"
disableHLJS: true # to disable highlightjs
disableShare: false
disableHLJS: false
hideSummary: false
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
ShowRssButtonInSectionTermList: true
UseHugoToc: false
cover:
    image: "" # image path/url
    alt: "<alt text>" # alt text
    caption: "<text>" # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: true # only hide on current single page
editPost:
    URL: "https://github.com/simmeon/blog/content/posts/rk45"
    Text: "Suggest Changes" # edit text
    appendFilePath: true # to append file path to Edit link
---

{{< math.inline >}}
{{ if or .Page.Params.math .Site.Params.math }}

<!-- KaTeX -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
{{ end }}
{{</ math.inline >}}



## The Initial Value Problem

In engineering, we often encounter systems that evolve over time, such as circuits, mechanical systems, or chemical reactions. These systems are best described using differential equations.
For example, Newton's law of cooling states:

$$
    \dfrac{dT}{dt} = -k(T - T_{surr})
$$


where **T** is the temperature of some point, **k** is a proportionality constant, and **T<sub>surr<sub>** is the temperature surrounding the point of interest.

To solve this equation is to find a solution for the temperature, **T**, over time. In this case, this is fairly straightforward and we can come up with an analytical solution of the form:

$$
    T(t) = T_{surr} + (T(0) - T_{surr})e^{-kt}
$$

Notice that the solution ***depends on the initial temperature***, as **Figure 1** shows. We need a point of reference to define the solution that is relevant to our system.

{{< figure src="../img/heat-transfer.png" align=center caption="**Figure 1**: Temperature of a point over time with different initial temperatures. T<sub>surr</sub> = 20 C, k = 1." >}}

{{< collapse summary="Figure 1 code" >}}
 
```python {linenos=inline}
'''
Plotting the temperature of a point with different starting temperatures.

Created by: simmeon
Last Modified: 28/04/24
License: MIT

'''

import matplotlib.pyplot as plt
import numpy as np

# Define system parameters
T_surr = 20     # surrounding temperature of 20 C
k = 1           # proportionality constant of 1 for simplicity

# Define our cooling function
def solve_T(t, T0):
    return T_surr + (T0 - T_surr)*np.exp(-k*t)

# Create an array of times from 0-5 seconds
t = np.arange(0, 5, 0.1)

# Solve and plot the solutions for different initial values of T
T0_choices = [30, 25, 20, 15, 10]

plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle')

for T0 in T0_choices:
    T = solve_T(t, T0)
    plt.plot(t, T, label='T0 = {} C'.format(T0))

plt.title('Temperature over time', fontsize=20)
plt.xlabel('Time [s]', fontsize=20)
plt.ylabel('Temperature [C]', fontsize=20)
plt.xlim(0, 5)
plt.ylim(10, 30)
plt.grid(alpha=0.7)
plt.legend()
plt.show()
```
{{< /collapse >}}

The initial value problem deals with solving these ordinary differential equations where we have an initial state of the system, eg. the initial temperature is 25 C. However, often these systems are hard or impossible to solve analytically. To solve them, we use numerical methods to approximate the solution. Let's look at how we might be able to do that...

## Numerical Methods

Let's take the previous cooling equation,

$$
    \dfrac{dT}{dt} = -k(T - T_{surr})
$$

 and assume we can't find an analytical solution. Let's consider the information we *do* have that could help us approximate the solution. We have an expression for the derivative that we can calculate at any time, ***t***, assuming we know the current temperature at that time. We know the temperature at some initial time (***t*** = 0 in this case), so we can calculate the derivative at that time. But how does this help us find the temperature at other times?

### The Euler Method

 From calculus we know that we get the derivative of a function by taking two points on the function and approximating the derivative as if it were a striaght line. As the distance, ***h***, between the two points gets closer to 0, the approximation gets better. Formally,

 $$
    f^{\prime}(x) = \lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h}
 $$

 We can rearrange this and assume a finite step size, ***h***, to get

 $$
    f(x+h) \approx f(x) + h f^{\prime}(x)
 $$

 which gives us an expression to approximate the next step of a function using only the current known function value and the function derivative. We can see what this looks like with different step sizes in **Figure 2**.

 {{< figure src="../img/the_derivative.png" align=center caption="**Figure 2**: Estimating the value of y = x<sup>2</sup> with different step sizes." >}}

{{< collapse summary="Figure 2 code" >}}

```python {linenos=inline}
'''
Visualising how the derivative can be used to estimate the value of a function.

Created by: simmeon
Last Modified: 28/04/24
License: MIT

'''

import matplotlib.pyplot as plt
import numpy as np

plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle')

# Define some function we want to find the value of
def func(t):
    return t**2

# Define the derivative line of the function
def dydt(t, t0, y0):
    m = 2 * t0
    return m * (t-t0) + y0

# Create an array of time values to plot the function
t_func = np.arange(0.5, 2, 0.1)

# Estimation parameters
t0 = 1.0                    # known initial value
y0 = func(t0)               # known initial value
h = [1, 0.5, 0.2]           # step size options

# Plotting
# Find and plot function values
y_func = func(t_func)
plt.plot(t_func, y_func)
plt.plot(t0, y0, 'x', color='C0')

# Plot the derivative line for each step size
colours = ['C1', 'C2', 'C3']
for i in range(len(h)):
    t_dydt = np.arange(t0, t0 + h[i], 0.1)
    y_dydt = dydt(t_dydt, t0, y0)
    plt.plot(t_dydt, y_dydt, color=colours[i], label='Step = {:.1f}'.format(h[i]))
    plt.plot(t_dydt[-1], y_dydt[-1], 'o', color=colours[i])

# Show plot
plt.title("Estimating a function by using the derivative", fontsize=20)
plt.legend(loc='lower right', fontsize=10)
plt.show()
```

{{< /collapse >}}

This is Euler's method for solving the initial value problem. We might also write it as:

$$
    f_{k+1} \approx f_{k} + h f^{\prime}(x_{k})
$$

for some index, ***k***.

We can iterate through time with this method, using the newly found *f<sub>k+1</sub>* as our new *f<sub>k</sub>* and so on. In code, that would look like the following: 

```python {linenos=inline, hl_lines=[14]}
def euler(dydt, y0, t0, t_end, h=0.1):
    '''
    Takes the derivative function, an initial condition, 
    the time we want to integrate until, and a step size.

    Returns arrays of time and y values.
    '''
    t = np.arange(t0, t_end+h, h)
    y = np.zeros(len(t))

    y[0] = y0

    for i in range(1,len(t)):
        y[i] = y[i-1] + h * dydt(y[i-1])

    return t, y
```
The highlighted line shows the Euler method itself where the next value of the function is estimated.

Let's try do that with Newton's cooling law and see how the result compares to the analytical solution. **Figure 3** shows what that would look like.

{{< figure src="../img/euler_method.png" align=center caption="**Figure 3**: Analytical and Euler method solutions to Newton's cooling law. T<sub>surr</sub> = 20 C, k = 1." >}}

{{< collapse summary="Figure 3 code" >}}

```python {linenos=inline}
'''
Performing Euler integration to solve the cooling equation. We will compare with the analytical solution.

Created by: simmeon
Last Modified: 28/04/24
License: MIT

'''

import matplotlib.pyplot as plt
import numpy as np

plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle')

# Define system
T_surr = 20     # surrounding temperature of 20 C
k = 1           # proportionality constant of 1 for simplicity

# Define our cooling function
def solve_T(t, T0):
    return T_surr + (T0 - T_surr)*np.exp(-k*t)

# Define the derivative (only depends on the current temp, not time)
def dTdt(T):
    return -k * (T - T_surr)

# Define our Euler integration function
def euler(dTdt, T0, t0, t_end, h=0.1):
    '''
    Takes the derivative function, an initial condition, the time we want to integrate until,
    and a step size.

    Returns arrays of time and temperature values
    '''
    t = np.arange(t0, t_end+h, h)
    T = np.zeros(len(t))

    T[0] = T0

    for i in range(1,len(t)):
        T[i] = T[i-1] + h * dTdt(T[i-1])

    return t, T


# Parameters
t0 = 0
T0 = 30
t_end = 5

# Get analytical solution
t_analytical = np.arange(t0, t_end+0.1, 0.1)
T_analytical = solve_T(t_analytical, T0)

# Get Euler numerical solution
t_euler_05, T_euler_05 = euler(dTdt, T0, t0, t_end, 0.5)
t_euler_01, T_euler_01 = euler(dTdt, T0, t0, t_end, 0.1)

# Plotting
plt.plot(t_analytical, T_analytical, label='Analytical', linestyle='--')
plt.plot(t_euler_05, T_euler_05, label='Euler method, h = 0.5')
plt.plot(t_euler_01, T_euler_01, label='Euler method, h = 0.1')

plt.title("Analytical and Euler method solutions to Newton's cooling law", fontsize=20)
plt.xlabel('Time [s]', fontsize=20)
plt.ylabel('Temperature [C]', fontsize=20)

plt.legend(fontsize=15)
plt.show()
```

{{< /collapse >}}

It is clear that the step size plays a big role in the accuracy of the solution. We could continue to reduce the step size, but that will quickly increase the time it takes to get a solution beyond a reasonable amount. Maybe we can think of a better way of doing this...

### Runge-Kutta Methods

A major problem with Euler's method is that using the derivative of the point we're at is not very good at estimating the next value of the function (for the kind of step sizes we want to use). The derivative is constantly changing and could be vastly different at the new value. We might get a more accurate step if we use a mix of the derivative at our start point and end point. 

This is the basis for how Runge-Kutta methods work. We find derivatives between the start and end points of each step and use a weighted average of those as the actual derivative for the step. The simplest version of this would be to just use the derivative at the start of the step -- which is exactly the Euler method! The Euler method is a 1st order Runge-Kutta method.

Let's define the method more concretely.

#### Derivation of Runge-Kutta Methods

We will start by examining the initial problem again, being that we want to solve the following general equation for y:

$$
    \dfrac{dy}{dt} = f(y, t)
$$

This could be our cooling equation from before,

$$
    \dfrac{dT}{dt} = -k(T - T_{surr})
$$

or something more complex like a state space defining a spring, mass, damper system:

$$
    \bold{\dot{x}} = 
    \begin{bmatrix} 0 & 1 \\\ \frac{-k}{m} & \frac{-c}{m} \end{bmatrix} \bold{x} + 
    \begin{bmatrix} 0  \\\ \frac{1}{m} \end{bmatrix} u(t)
$$

In both cases, we are given the derivative of the state we want to solve for (eg. temperature) and the derivative depends on the value of the state and time. In the case of the cooling equation, the derivative only depends on the state, **T**, and not time.

Ideally, we could just integrate the derivative to find the value at \\( t + h \\):

$$
    y(t+h) = y(t) + \int_{\tau=t}^{\tau=t+h}{\dfrac{dy(\tau)}{d\tau}} d\tau
$$

However, as we said earlier, this is often hard or impossible. We can instead approximate the integral with a weighted sum.

##### Weighted Sum Approximations

Let's build this up slowly. Take the following function, for example:

$$
    f(t) = t^3 - 2t^2 - t + 3
$$

How would we evaluate

$$
    \int_0^2{f(t)}dt
$$

We can do this analytically, which is like summing up tiny vertical slices of the function to find the total area underneath it. But we could think about this a different way. We can get that same area by finding the average value of the function over the interval, then multiplying by the length of the interval. You can see this in **Figure 4**.

{{< figure src="../img/weighted_sum_approx.png" align=center caption="**Figure 4**: Comparison of areas found by integration and weighted sum approximation." >}}

{{< collapse summary="Figure 4 code" >}}

```python {linenos=inline}
'''
Making sense of weighted sums as integral approximations.

Created by: simmeon
Last Modified: 28/04/24
License: MIT

'''

import matplotlib.pyplot as plt
import numpy as np

plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle')

def f(t):
    return t**3 -2*t**2 - t + 3

# Perform a weighted sum approximation
N = 10  # higher number of samples, N, gives a better approximation
sum = 0
t0 = 0
h = 2
nodes = np.linspace(t0, t0+h, N)

for node in nodes:
    sum += f(node)
weighted_sum = sum / N

print('Weighted sum area: ', weighted_sum * h)

# Plotting
t = np.arange(t0, t0 + h, 0.01)
y = f(t)

# Integration result
plt.subplot(1,2,1)
plt.plot(t,y)
plt.fill_between(t, y, alpha=0.3)
plt.annotate('Area = 2.67', (1,2.5), fontsize=20)
plt.title('Area from integration', fontsize=20)

# Weighted sum result
plt.subplot(1,2,2)
plt.plot(t,y)
plt.fill_between(t, weighted_sum, alpha=0.3)
plt.annotate('Area = {:.2f}'.format(weighted_sum * h), (1,2.5), fontsize=20)
plt.title('Area from weighted sum', fontsize=20)

plt.show()
```

{{< /collapse >}}

To put this idea into an equation, we can say:

$$
    \int_0^2{f(t)}dt \approx \frac{2}{\sum{w_i}} \sum_{i=1}^N{w_i  f(t_i)}
$$

Where \\( N \\) is the number of points we sample from the function to find the average, \\( w_i \\) is the weighting we give to each point, and \\( t_i \\) is some point inside the integration bounds of \\( [0, 2] \\). We can simplify this by giving every point a weighting of 1:

$$
    \int_0^2{f(t)}dt \approx \frac{2}{N} \sum_{i=1}^N{f(t_i)}
$$

The more points we sample, the closer the average will be to the actual average of the function and therefore the actual integral.

We can extend this concept to our derivative from earlier:

$$
    \int_{\tau=t}^{\tau=t+h}{\dfrac{dy(\tau)}{d\tau}} d\tau 
    \approx 
    \frac{h}{\sum{w_i}} \sum_{i=1}^N{w_i y^{\prime}(t+v_i h, y(t+v_i h))}
$$

***What a mess.***

We're still using the same concept of a weighted sum to find the average value of the derivative function, but since the derivative depends on \\( t \\) *and* \\( y \\) it's a bit more complicated. We introduced \\( v_i \\) to help define which values of the function we sample, also called *nodes*. This can range from 0 to 1 to cover the interval from \\( t \\) to \\( t+h \\). Of course, whatever time we evaluate our derivative at must be the same time we use to get the \\( y \\) values for the derivative.

For the sake of simplicity let's say that all our weights will sum up to 1. We are **not**, however, going to assume that all our weights will be the same value as we did previously.

Considering all this, the equation we are trying to solve is now:

$$
    y(t+h) = y(t) + h \sum_{i=1}^N{w_i y^{\prime}(t+v_i h, y(t+v_i h))}
$$

##### The Runge-Kutta Family

You might have noticed there is a problem with the sum we just defined. In particular, we don't know what \\( y(t+v_i h) \\) is. In fact, finding this is basically the point of the whole method. So how do we deal with this?

Simply, we are going to make worse approximations of \\( y(t+v_i h) \\) so that we can make a much better approximation of \\( y(t + h) \\). Again, these \\( y(t+v_i h) \\) values are used to find values of the derivative that we will then average. Let's define what we will do to find these approximations.

We will start by saying that \\( v_1 = 0 \\). This means the first term in the sum will be

$$
    w_1 y^{\prime}(t, y(t))
$$

If this was the only term in our sum (so \\( w_1 = 1 \\) ), then the equation we would be solving would be:

$$
    y(t+h) \approx y(t) + h y^{\prime}(t, y(t))
$$

This should look familiar, it's the Euler method! This is what we mean by the Euler method is a 1st order Runge-Kutta method -- because it uses one term in the weighted sum. Or in other words, it is a linear approximation.

For simplicity, we will write this first sum term as:

$$
    k_1 = y^{\prime}(t, y(t)) h
$$

\\( k_1 \\) is a 1st order estimate of the change in \\( y \\) between \\( y(t) \\) and \\( y(t+h) \\). 

The second term gets trickier as we have to somehow estimate \\( y(t+v_2 h) \\). Conveniently, we have just found an estimate for how \\( y \\) changes: \\( k_1 \\). So we can utilise some fraction of this change to create our estimate for the second weighted sum term where:

$$
    y^{\prime}(t + \alpha_2 h, y(t) + \beta_{2,1} k_1)
$$

We don't know yet how much of \\( k_1 \\) we should add, we will figure that out later. Same with what time we should sample at. 

We have changed notation slightly to help set up higher order Runge-Kutta methods. Instead of \\( v_i \\) we are now using \\( \alpha_i \\) to tell us about what time we are sampling at. And since our \\( y \\) value is no longer defined in simple terms of time, we are going to use \\( \beta_{i,j} \\) to describe how much of previous estimates we will add to the estimate for the new term.

We define

$$
    k_2 = y^{\prime}(t + \alpha_2 h, y(t) + \beta_{2,1} k_1) h
$$

so that the equation we are solving is now:

$$
    y(t+h) \approx y(t) + w_1 k_1 + w_2 k_2
$$

With that, we have defined the 2nd order Runge-Kutta Method! 

We can continue the weighted sum with a third and fourth term, following similar logic of using the previous estimates to inform the new value of \\( y(t + v_i h) \\). This will give us:

$$
    k_3 = y^{\prime}(t + \alpha_3 h, y(t) + \beta_{3,1} k_1 + \beta_{3,2} k_2) h
    \\\
    k_4 = y^{\prime}(t + \alpha_4 h, y(t) + \beta_{4,1} k_1 + \beta_{4,2} k_2 + \beta_{4,3} k_3) h
$$

This gives the 4th order Runge-Kutta method:

$$
    y(t+h) \approx y(t) + w_1 k_1 + w_2 k_2 + w_3 k_3 + w_4 k_4
$$

Now to actually solve these higher order methods, we need to define these coefficients...

##### Defining Coefficients

We need to define all the \\( \alpha_i \\), \\( \beta_{i,j} \\), and \\( w_i \\) coefficients to be able to use these methods. We can do this by comparing the Taylor series expansion of our approximation to the Taylor series expansion of \\( y(t+h) \\) and equating coefficients. Let's do this for the 2nd order method:

$$
    y(t+h) \approx y(t) + w_1 k_1 + w_2 k_2
$$

As this is a 2nd order method, we will need to find the 2nd order expansions of the left and right sides. 

Let's start with the left. The 2nd order Taylor series expansion of \\( y(t+h) \\) about \\( t \\) is:

$$
    y(t+h) \approx y(t) + h \dfrac{dy}{dt} \Big| _{t,y} + \frac{h^2}{2} \dfrac{d^2y}{dt^2} \Big| _{t,y} + O(h^3)
$$

We can write our derivative:

$$
    \dfrac{dy}{dt} = f(t, y)
    \\\
    {}
    \\\
    \dfrac{d^2y}{dt^2} = \dfrac{df(t,y)}{dt} = \dfrac{\partial f}{\partial t} + \dfrac{\partial f}{\partial y} \dfrac{dy}{dt} = 
    \dfrac{\partial f}{\partial t} + f \dfrac{\partial f}{\partial y}
$$

This makes our left hand side (using slightly different notation):

$$
    y_{n+1} \approx y_n + h f(t_n, y_n) + \frac{h^2}{2}  \bigg(\dfrac{\partial f}{\partial t} + f \dfrac{\partial f}{\partial y}\bigg) \bigg|_{t_n, y_n} + O(h^3)
$$

Great! Let's do the right hand side now. We can write the right hand same with the same notation as above:

$$
    y_n + w_1 k_{1,n} + w_2 k_{2,n}
$$

 \\( k_2 \\) is a bit tricky to expand. Our \\( k_2 \\) term is made up mostly of a function with the form \\( f(t + \Delta t, y + \Delta y) \\), which will need to be expanded. In general, the 2nd order expansion looks like:

$$
    f(t + \Delta t, y + \Delta y) = f(t, y) + \Delta t \dfrac{\partial f}{\partial t}\bigg| _{t, y} + 
    \Delta y \dfrac{\partial f}{\partial y} \bigg| _{t, y} + O(h^3)
$$

Applying this to \\( k_2 \\) gives:

$$
    k_{2,n} = h f(t + \alpha_2 h, y_n + \beta_{2,1} k_1) \approx h \bigg( f(t_n, y_n) + \alpha _2 h \dfrac{\partial f}{\partial t} \bigg| _{t_n, y_n} + 
    \beta _{2,1} k_1 \dfrac{\partial f}{\partial y} \bigg| _{t_n, y_n} \bigg)
$$


All together, the right hand side is then

$$
    y_n + w_1 k_{1,n} + w_2 k_{2,n} \approx y_n + w_1 h f(t_n, y_n) + 
    w_2 h \bigg( f(t_n, y_n) + \alpha _2 h \dfrac{\partial f}{\partial t} \bigg| _{t_n, y_n} + 
    \beta _{2,1} k_1 \dfrac{\partial f}{\partial y} \bigg| _{t_n, y_n} \bigg) + O(h^3)
$$

which we can rearrange to be in the same form (substituting in for \\( k_1 \\)) as the left hand side:

$$
    y_n + w_1 k_{1,n} + w_2 k_{2,n} \approx 
    y_n + (w_1 + w_2) h f(t_n, y_n) + \frac{h^2}{2} \bigg(2 w_2 \alpha_2 \dfrac{\partial f}{\partial t} + 
    2 w_2 \beta _{2,1} f \dfrac{\partial f}{\partial y}  \bigg) \bigg| _{t_n, y_n} + O(h^3)
$$

Finally, we have both expansions and can equate the coefficients:

$$
    y(t+h) \approx y(t) + w_1 k_1 + w_2 k_2
    \\\
    {}
    \\\
    \big\downarrow
    \\\
    {}
    \\\
    y_n + h f(t_n, y_n) + \frac{h^2}{2}  \bigg(\dfrac{\partial f}{\partial t} + f \dfrac{\partial f}{\partial y}\bigg) \bigg|_{t_n, y_n} + O(h^3)
    \\\ {} \\\ = \\\ {} \\\
    y_n + (w_1 + w_2) h f(t_n, y_n) + \frac{h^2}{2} \bigg(2 w_2 \alpha_2 \dfrac{\partial f}{\partial t} + 
    2 w_2 \beta _{2,1} f \dfrac{\partial f}{\partial y}  \bigg) \bigg| _{t_n, y_n} + O(h^3)
$$

From this, we can see that our coefficients must satisfy the following equations:

$$
    w_1 + w_2 = 1 \\\ {} \\\
    w_2 \alpha_2= \frac{1}{2} \\\ {} \\\
    w_2 \beta_{2,1} = \frac{1}{2}
$$

With 3 equations and 4 unknows, there are infinitely many solutions. However, the standard choices are:

$$
    \alpha_2 = \beta_{2,1} = 1
    \\\ {} \\\
    w_1 = w_2 = \frac{1}{2}
$$

This brings us *finally* to the complete **2nd order Runge-Kutta Method**:

$$
    k_1 = h y^\prime (t_n, y_n) 
    \\\ {} \\\
    k_2 = h y^\prime (t_n + h, y_n + k_1) 
    \\\ {} \\\
    y_{n+1} = y_n + \frac{1}{2} k_1 + \frac{1}{2} k_2
$$

A similar (messier) method of expansions can be used to for higher order methods. The standard terms for the **4th order Runge-Kutta method** are:

$$
    k_1 = h y^\prime (t_n, y_n) 
    \\\ {} \\\
    k_2 = h y^\prime (t_n + \frac{h}{2}, y_n + \frac{k_1}{2}) 
    \\\ {} \\\
    k_3 = h y^\prime (t_n + \frac{h}{2}, y_n + \frac{k_2}{2}) 
    \\\ {} \\\
    k_4 = h y^\prime (t_n + h, y_n + k_3) 
    \\\ {} \\\
    y_{n+1} = y_n + \frac{1}{6} (k_1 + 2 k_2 + 2 k_3 + k_4)
$$

Keep in mind that there are infinitely many choices of these coefficients and lots of research has gone into figuring out which ones work best. We will stick to these simple standard ones for now.

#### Implementing Runge-Kutta

Now that we have finally derived the Runge-Kutta methods, let's implement them in code. We will do both the 2nd order and 4th order methods and compare their accuracy.

```python {linenos=inline hl_lines=[20,21,23]}
def rk2(dydt, t0, y0, t_end, h):
    """
    Solves a first-order ODE using the 2nd order Runge-Kutta method

    Args:
        dydt: The derivative function to integrate
        t0: Initial value of time
        y0: Initial value of y
        t_end: Final time for integration
        h: Step size

    Returns:
        An array of y values for each time step
    """
    t = np.arange(t0, t_end+h, h)  # Array of time points
    y = np.zeros_like(t)
    y[0] = y0

    for i in range(1, len(t)):
        k1 = h * dydt(t[i - 1], y[i - 1])
        k2 = h * dydt(t[i - 1] + h, y[i - 1] + k1)

        y[i] = y[i - 1] + (k1 + k2) / 2

    return y
```
We can see how, at each time step, we calculate the \\( k_i \\) values and use these to estimate the next value of the function.

```python {linenos=inline hl_lines=[20,21,22,23,25]}
def rk4(dydt, t0, y0, t_end, h):
    """
    Solves a first-order ODE using the 4th order Runge-Kutta method

    Args:
        dydt: The derivative function to integrate
        t0: Initial value of time
        y0: Initial value of y
        t_end: Final time for integration
        h: Step size

    Returns:
        An array of y values for each time step
    """
    t = np.arange(t0, t_end+h, h)
    y = np.zeros_like(t)
    y[0] = y0

    for i in range(1, len(t)):
        k1 = h * dydt(t[i - 1], y[i - 1])
        k2 = h * dydt(t[i - 1] + h / 2, y[i - 1] + k1 / 2)
        k3 = h * dydt(t[i - 1] + h / 2, y[i - 1] + k2 / 2)
        k4 = h * dydt(t[i], y[i - 1] + k3) 

        y[i] = y[i - 1] + (k1 + 2 * k2 + 2 * k3 + k4) / 6
 
    return y
```

After all that derivation, the actual method is remarkably clean and simple to implement. **Figure 5** gives an idea of how these higher order functions perform compared to our original Euler method.

{{< figure src="../img/solver_comparison.png" align=center caption="**Figure 5**: Comparing the accuracy of different order ODE solvers, h = 0.1" >}}

All the solvers are using the same step size here. As we can see, the 4th order method is better than the 2nd order method. They are both much more accurate than the 1st order Euler method.

As a final comparison, let's look at Newton's law of cooling one last time. **Figure 6** shows how our 4th order solver compares to our previous test with the Euler method.

{{< figure src="../img/rk4_cooling_eqn.png" align=center caption="**Figure 6**: Comparing the accuracy of the RK4 and Euler methods on the cooling equation. T<sub>surr</sub> = 20 C, k = 1, h = 0.5" >}}


{{< collapse summary="Figure 5 and 6 code" >}}

```python {linenos=inline}
'''
Implementing and comparing the Runge-Kutta 2nd and 4th order methods.
We can also compare to the Euler method (1st order).

Created by: simmeon
Last Modified: 29/04/24
License: MIT

'''

import matplotlib.pyplot as plt
import numpy as np

plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle')

# Define some functions to test our solvers with
def dydt(t, y):
    return 3 * t**2 + 20 * np.cos(10 * t)

def analytical_solution(t):
    return t**3 + 2 * np.sin(10 * t)  # Analytical solution to the ODE 

# # Define our cooling function
# def solve_T(t, T0):
#     return T_surr + (T0 - T_surr)*np.exp(-k*t)

# # Define the derivative (only depends on the current temp, not time)
# def dTdt(T):
#     return -k * (T - T_surr)

def euler_solver(dydt, t0, y0, t_end, h):
    """
    Solves a first-order ODE using the Euler method

    Args:
        dydt: The differential equation (dy/dt) as a Python function 
        t0: Initial value of time
        y0: Initial value of y
        t_end: Final time for integration
        h: Step size

    Returns:
        An array of y values for each time step
    """
    t = np.arange(t0, t_end+h, h)  
    y = np.zeros_like(t)
    y[0] = y0

    for i in range(1, len(t)):
        y[i] = y[i - 1] + h * dydt(t[i - 1], y[i - 1])

    return y

def rk2(dydt, t0, y0, t_end, h):
    """
    Solves a first-order ODE using the 2nd order Runge-Kutta method

    Args:
        dydt: The derivative function to integrate
        t0: Initial value of time
        y0: Initial value of y
        t_end: Final time for integration
        h: Step size

    Returns:
        An array of y values for each time step
    """
    t = np.arange(t0, t_end+h, h)  # Array of time points
    y = np.zeros_like(t)
    y[0] = y0

    for i in range(1, len(t)):
        k1 = h * dydt(t[i - 1], y[i - 1])
        k2 = h * dydt(t[i - 1] + h, y[i - 1] + k1)
        y[i] = y[i - 1] + (k1 + k2) / 2

    return y

def rk4(dydt, t0, y0, t_end, h):
    """
    Solves a first-order ODE using the 4th order Runge-Kutta method

    Args:
        dydt: The derivative function to integrate
        t0: Initial value of time
        y0: Initial value of y
        t_end: Final time for integration
        h: Step size

    Returns:
        An array of y values for each time step
    """
    t = np.arange(t0, t_end+h, h)
    y = np.zeros_like(t)
    y[0] = y0

    for i in range(1, len(t)):
        k1 = dydt(t[i - 1], y[i - 1])
        k2 = dydt(t[i - 1] + h / 2, y[i - 1] + k1 / 2)
        k3 = dydt(t[i - 1] + h / 2, y[i - 1] + k2 / 2)
        k4 = dydt(t[i], y[i - 1] + k3) 

        y[i] = y[i - 1] + (k1 + 2 * k2 + 2 * k3 + k4) / 6 * h
 
    return y

# Define simulation parameters
t0 = 0
y0 = 0
t_end = 5
h = 0.1

# Solve using both methods
y_euler = euler_solver(dydt, t0, y0, t_end, h)
y_rk2 = rk2(dydt, t0, y0, t_end, h)
y_rk4 = rk4(dydt, t0, y0, t_end, h)

# Get analytical solution
t = np.arange(t0, t_end+h, h)  # Array of time points
y_analytical = analytical_solution(t)

# Calculate errors
error_euler = np.abs(y_euler - y_analytical)
error_rk2 = np.abs(y_rk2 - y_analytical)
error_rk4 = np.abs(y_rk4 - y_analytical)

# Plot the numerical solutions
plt.subplot(1,2,1)
plt.plot(t, y_euler, label='Euler')
plt.plot(t, y_rk2, label='RK2')
plt.plot(t, y_rk4, label='RK4')
plt.plot(t, y_analytical, label='Analytical', linestyle='--') 
plt.xlabel('Time (t)', fontsize=20)
plt.ylabel('y(t)', fontsize=20)
plt.title('Numerical Solutions', fontsize=20)
plt.legend(fontsize=15)

# Plot the errors
plt.subplot(1,2,2)
plt.plot(t, error_euler, label='Euler')
plt.plot(t, error_rk2, label='RK2')
plt.plot(t, error_rk4, label='RK4')
plt.xlabel('Time (t)', fontsize=20)
plt.ylabel('Abs Error', fontsize=20)
plt.title('Absolute Errors of the Numerical Solutions', fontsize=20)
plt.legend(fontsize=15)

plt.show()
```

{{< /collapse >}}

Even with a relatively large step size, the 4th order method is still much better than the Euler method. In fact, we could make the step size even bigger and still have a solution that fell within some tiny tolerance. 

But how can we choose a good value to make this step size? And better yet, could we even possibly change the step size during our integration to make sure our result stayed within some tolerance?

## Adaptive Step Sizing

...to be continued...






## References

[1] [The Runge-Kutta Equations by Quadrature Methods. Roson J. S., 1967, *NASA*.](https://ntrs.nasa.gov/api/citations/19680000653/downloads/19680000653.pdf)

[2] [Runge-Kutta Methods. *10.001: Numerical Solution of Ordinary Differential Equations, MIT*](https://web.mit.edu/10.001/Web/Course_Notes/Differential_Equations_Notes/node5.html)

[3] [Explanation and proof of the 4th order Runge-Kutta method](https://math.stackexchange.com/questions/528856/explanation-and-proof-of-the-4th-order-runge-kutta-method)